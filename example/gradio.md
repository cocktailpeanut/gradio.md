<!-- Source: guides/02_building-interfaces/00_the-interface-class.md -->
# The `Interface` class

As mentioned in the [Quickstart](/main/guides/quickstart), the `gr.Interface` class is a high-level abstraction in Gradio that allows you to quickly create a demo for any Python function simply by specifying the input types and the output types. Revisiting our first demo:

$code_hello_world_4


We see that the `Interface` class is initialized with three required parameters:

- `fn`: the function to wrap a user interface (UI) around
- `inputs`: which Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.
- `outputs`: which Gradio component(s) to use for the output. The number of components should match the number of return values from your function.

In this Guide, we'll dive into `gr.Interface` and the various ways it can be customized, but before we do that, let's get a better understanding of Gradio components.

## Gradio Components

Gradio includes more than 30 pre-built components (as well as many [community-built _custom components_](https://www.gradio.app/custom-components/gallery)) that can be used as inputs or outputs in your demo. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.LinePlot` component displays line plots, and so on. 

## Components Attributes

We used the default versions of the `gr.Textbox` and `gr.Slider`, but what if you want to change how the UI components look or behave?

Let's say you want to customize the slider to have values from 1 to 10, with a default of 2. And you wanted to customize the output text field â€” you want it to be larger and have a label.

If you use the actual classes for `gr.Textbox` and `gr.Slider` instead of the string shortcuts, you have access to much more customizability through component attributes.

$code_hello_world_2
$demo_hello_world_2

## Multiple Input and Output Components

Suppose you had a more complex function, with multiple outputs as well. In the example below, we define a function that takes a string, boolean, and number, and returns a string and number. 

$code_hello_world_3
$demo_hello_world_3

Just as each component in the `inputs` list corresponds to one of the parameters of the function, in order, each component in the `outputs` list corresponds to one of the values returned by the function, in order.

## An Image Example

Gradio supports many types of components, such as `Image`, `DataFrame`, `Video`, or `Label`. Let's try an image-to-image function to get a feel for these!

$code_sepia_filter
$demo_sepia_filter

When using the `Image` component as input, your function will receive a NumPy array with the shape `(height, width, 3)`, where the last dimension represents the RGB values. We'll return an image as well in the form of a NumPy array. 

Gradio handles the preprocessing and postprocessing to convert images to NumPy arrays and vice versa. You can also control the preprocessing performed with the `type=` keyword argument. For example, if you wanted your function to take a file path to an image instead of a NumPy array, the input `Image` component could be written as:

```python
gr.Image(type="filepath")
```

You can read more about the built-in Gradio components and how to customize them in the [Gradio docs](https://gradio.app/docs).

## Example Inputs

You can provide example data that a user can easily load into `Interface`. This can be helpful to demonstrate the types of inputs the model expects, as well as to provide a way to explore your dataset in conjunction with your model. To load example data, you can provide a **nested list** to the `examples=` keyword argument of the Interface constructor. Each sublist within the outer list represents a data sample, and each element within the sublist represents an input for each input component. The format of example data for each component is specified in the [Docs](https://gradio.app/docs#components).

$code_calculator
$demo_calculator

You can load a large dataset into the examples to browse and interact with the dataset through Gradio. The examples will be automatically paginated (you can configure this through the `examples_per_page` argument of `Interface`).

Continue learning about examples in the [More On Examples](https://gradio.app/guides/more-on-examples) guide.

## Descriptive Content

In the previous example, you may have noticed the `title=` and `description=` keyword arguments in the `Interface` constructor that helps users understand your app.

There are three arguments in the `Interface` constructor to specify where this content should go:

- `title`: which accepts text and can display it at the very top of interface, and also becomes the page title.
- `description`: which accepts text, markdown or HTML and places it right under the title.
- `article`: which also accepts text, markdown or HTML and places it below the interface.

![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)

Another useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`. You can also add the `info=` keyword argument to form elements like `Textbox` or `Radio` to provide further information on their usage.

```python
gr.Number(label='Age', info='In years, must be greater than 0')
```

## Additional Inputs within an Accordion

If your prediction function takes many inputs, you may want to hide some of them within a collapsed accordion to avoid cluttering the UI. The `Interface` class takes an `additional_inputs` argument which is similar to `inputs` but any input components included here are not visible by default. The user must click on the accordion to show these components. The additional inputs are passed into the prediction function, in order, after the standard inputs.

You can customize the appearance of the accordion by using the optional `additional_inputs_accordion` argument, which accepts a string (in which case, it becomes the label of the accordion), or an instance of the `gr.Accordion()` class (e.g. this lets you control whether the accordion is open or closed by default).

Here's an example:

$code_interface_with_additional_inputs
$demo_interface_with_additional_inputs

---

<!-- Source: guides/01_getting-started/01_quickstart.md -->
# Quickstart

Gradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/gif-version.gif" style="padding-bottom: 10px">

It just takes a few lines of Python to create your own demo, so let's get started ðŸ’«


## Installation

**Prerequisite**: Gradio requires [Python 3.10 or higher](https://www.python.org/downloads/).


We recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:

```bash
pip install --upgrade gradio
```


Tip: It is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems <a href="https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment">are provided here</a>. 

## Building Your First Demo

You can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:


$code_hello_world_4


Tip: We shorten the imported name from <code>gradio</code> to <code>gr</code>. This is a widely adopted convention for better readability of code. 

Now, run your code. If you've written the Python code in a file named `app.py`, then you would run `python app.py` from the terminal.

The demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.

$demo_hello_world_4

Type your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.

Tip: When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. You can also enable <strong>vibe mode</strong> by using the <code>--vibe</code> flag, e.g. <code>gradio --vibe app.py</code>, which provides an in-browser chat that can be used to write or edit your Gradio app using natural language. Learn more in the <a href="https://www.gradio.app/guides/developing-faster-with-reload-mode">Hot Reloading Guide</a>.


**Understanding the `Interface` Class**

You'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. 

The `Interface` class has three core arguments:

- `fn`: the function to wrap a user interface (UI) around
- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.
- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.

The `fn` argument is very flexible -- you can pass *any* Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.

The `inputs` and `outputs` arguments take one or more Gradio components. As we'll see, Gradio includes more than [30 built-in components](https://www.gradio.app/docs/gradio/introduction) (such as the `gr.Textbox()`, `gr.Image()`, and `gr.HTML()` components) that are designed for machine learning applications. 

Tip: For the `inputs` and `outputs` arguments, you can pass in the name of these components as a string (`"textbox"`) or an instance of the class (`gr.Textbox()`).

If your function accepts more than one argument, as is the case above, pass a list of input components to `inputs`, with each input component corresponding to one of the arguments of the function, in order. The same holds true if your function returns more than one value: simply pass in a list of components to `outputs`. This flexibility makes the `Interface` class a very powerful way to create demos.

We'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).

## Sharing Your Demo

What good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let's revisit our example demo,  but change the last line as follows:

```python
import gradio as gr

def greet(name):
    return "Hello " + name + "!"

demo = gr.Interface(fn=greet, inputs="textbox", outputs="textbox")
    
demo.launch(share=True)  # Share your demo with just 1 extra parameter ðŸš€
```

When you run this code, a public URL will be generated for your demo in a matter of seconds, something like:

ðŸ‘‰ &nbsp; `https://a23dsf231adb.gradio.live`

Now, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.

To learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).


## An Overview of Gradio

So far, we've been discussing the `Interface` class, which is a high-level class that lets you build demos quickly with Gradio. But what else does Gradio include?

### Custom Demos with `gr.Blocks`

Gradio offers a low-level approach for designing web apps with more customizable layouts and data flows with the `gr.Blocks` class. Blocks supports things like controlling where components appear on the page, handling multiple data flows and more complex interactions (e.g. outputs can serve as inputs to other functions), and updating properties/visibility of components based on user interaction â€” still all in Python. 

You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).

### Chatbots with `gr.ChatInterface`

Gradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).

### The Gradio Python & JavaScript Ecosystem

That's the gist of the core `gradio` Python library, but Gradio is actually so much more! It's an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:

* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.
* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.
* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications â€” for free!

## What's Next?

Keep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [let's dive deeper into the Interface class](https://www.gradio.app/guides/the-interface-class).

Or, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).


## Gradio Sketch

You can also build Gradio applications without writing any code. Simply type `gradio sketch` into your terminal to open up an editor that lets you define and modify Gradio components, adjust their layouts, add events, all through a web editor. Or [use this hosted version of Gradio Sketch, running on Hugging Face Spaces](https://huggingface.co/spaces/aliabid94/Sketch).

---

<!-- Source: guides/02_building-interfaces/01_more-on-examples.md -->
# More on Examples

In the [previous Guide](/main/guides/the-interface-class), we discussed how to provide example inputs for your demo to make it easier for users to try it out. Here, we dive into more details.

## Providing Examples

Adding examples to an Interface is as easy as providing a list of lists to the `examples`
keyword argument.
Each sublist is a data sample, where each element corresponds to an input of the prediction function.
The inputs must be ordered in the same order as the prediction function expects them.

If your interface only has one input component, then you can provide your examples as a regular list instead of a list of lists.

### Loading Examples from a Directory

You can also specify a path to a directory containing your examples. If your Interface takes only a single file-type input, e.g. an image classifier, you can simply pass a directory filepath to the `examples=` argument, and the `Interface` will load the images in the directory as examples.
In the case of multiple inputs, this directory must
contain a log.csv file with the example values.
In the context of the calculator demo, we can set `examples='/demo/calculator/examples'` and in that directory we include the following `log.csv` file:

```csv
num,operation,num2
5,"add",3
4,"divide",2
5,"multiply",3
```

This can be helpful when browsing flagged data. Simply point to the flagged directory and the `Interface` will load the examples from the flagged data.

### Providing Partial Examples

Sometimes your app has many input components, but you would only like to provide examples for a subset of them. In order to exclude some inputs from the examples, pass `None` for all data samples corresponding to those particular components.

## Caching examples

You may wish to provide some cached examples of your model for users to quickly try out, in case your model takes a while to run normally.
If `cache_examples=True`, your Gradio app will run all of the examples and save the outputs when you call the `launch()` method. This data will be saved in a directory called `gradio_cached_examples` in your working directory by default. You can also set this directory with the `GRADIO_EXAMPLES_CACHE` environment variable, which can be either an absolute path or a relative path to your working directory.

Whenever a user clicks on an example, the output will automatically be populated in the app now, using data from this cached directory instead of actually running the function. This is useful so users can quickly try out your model without adding any load!

Alternatively, you can set `cache_examples="lazy"`. This means that each particular example will only get cached after it is first used (by any user) in the Gradio app. This is helpful if your prediction function is long-running and you do not want to wait a long time for your Gradio app to start.

Keep in mind once the cache is generated, it will not be updated automatically in future launches. If the examples or function logic change, delete the cache folder to clear the cache and rebuild it with another `launch()`.

---

<!-- Source: guides/03_building-with-blocks/01_blocks-and-event-listeners.md -->
# Blocks and Event Listeners

We briefly described the Blocks class in the [Quickstart](/main/guides/quickstart#custom-demos-with-gr-blocks) as a way to build custom demos. Let's dive deeper. 


## Blocks Structure

Take a look at the demo below.

$code_hello_blocks
$demo_hello_blocks

- First, note the `with gr.Blocks() as demo:` clause. The Blocks app code will be contained within this clause.
- Next come the Components. These are the same Components used in `Interface`. However, instead of being passed to some constructor, Components are automatically added to the Blocks as they are created within the `with` clause.
- Finally, the `click()` event listener. Event listeners define the data flow within the app. In the example above, the listener ties the two Textboxes together. The Textbox `name` acts as the input and Textbox `output` acts as the output to the `greet` method. This dataflow is triggered when the Button `greet_btn` is clicked. Like an Interface, an event listener can take multiple inputs or outputs.

You can also attach event listeners using decorators - skip the `fn` argument and assign `inputs` and `outputs` directly:

$code_hello_blocks_decorator

## Event Listeners and Interactivity

In the example above, you'll notice that you are able to edit Textbox `name`, but not Textbox `output`. This is because any Component that acts as an input to an event listener is made interactive. However, since Textbox `output` acts only as an output, Gradio determines that it should not be made interactive. You can override the default behavior and directly configure the interactivity of a Component with the boolean `interactive` keyword argument, e.g. `gr.Textbox(interactive=True)`.

```python
output = gr.Textbox(label="Output", interactive=True)
```

_Note_: What happens if a Gradio component is neither an input nor an output? If a component is constructed with a default value, then it is presumed to be displaying content and is rendered non-interactive. Otherwise, it is rendered interactive. Again, this behavior can be overridden by specifying a value for the `interactive` argument.

## Types of Event Listeners

Take a look at the demo below:

$code_blocks_hello
$demo_blocks_hello

Instead of being triggered by a click, the `welcome` function is triggered by typing in the Textbox `inp`. This is due to the `change()` event listener. Different Components support different event listeners. For example, the `Video` Component supports a `play()` event listener, triggered when a user presses play. See the [Docs](http://gradio.app/docs#components) for the event listeners for each Component.

## Multiple Data Flows

A Blocks app is not limited to a single data flow the way Interfaces are. Take a look at the demo below:

$code_reversible_flow
$demo_reversible_flow

Note that `num1` can act as input to `num2`, and also vice-versa! As your apps get more complex, you will have many data flows connecting various Components.

Here's an example of a "multi-step" demo, where the output of one model (a speech-to-text model) gets fed into the next model (a sentiment classifier).

$code_blocks_speech_text_sentiment
$demo_blocks_speech_text_sentiment

## Function Input List vs Dict

The event listeners you've seen so far have a single input component. If you'd like to have multiple input components pass data to the function, you have two options on how the function can accept input component values:

1. as a list of arguments, or
2. as a single dictionary of values, keyed by the component

Let's see an example of each:
$code_calculator_list_and_dict

Both `add()` and `sub()` take `a` and `b` as inputs. However, the syntax is different between these listeners.

1. To the `add_btn` listener, we pass the inputs as a list. The function `add()` takes each of these inputs as arguments. The value of `a` maps to the argument `num1`, and the value of `b` maps to the argument `num2`.
2. To the `sub_btn` listener, we pass the inputs as a set (note the curly brackets!). The function `sub()` takes a single dictionary argument `data`, where the keys are the input components, and the values are the values of those components.

It is a matter of preference which syntax you prefer! For functions with many input components, option 2 may be easier to manage.

$demo_calculator_list_and_dict

## Function Return List vs Dict

Similarly, you may return values for multiple output components either as:

1. a list of values, or
2. a dictionary keyed by the component

Let's first see an example of (1), where we set the values of two output components by returning two values:

```python
with gr.Blocks() as demo:
    food_box = gr.Number(value=10, label="Food Count")
    status_box = gr.Textbox()

    def eat(food):
        if food > 0:
            return food - 1, "full"
        else:
            return 0, "hungry"

    gr.Button("Eat").click(
        fn=eat,
        inputs=food_box,
        outputs=[food_box, status_box]
    )
```

Above, each return statement returns two values corresponding to `food_box` and `status_box`, respectively.

**Note:** if your event listener has a single output component, you should **not** return it as a single-item list. This will not work, since Gradio does not know whether to interpret that outer list as part of your return value. You should instead just return that value directly.

Now, let's see option (2). Instead of returning a list of values corresponding to each output component in order, you can also return a dictionary, with the key corresponding to the output component and the value as the new value. This also allows you to skip updating some output components.

```python
with gr.Blocks() as demo:
    food_box = gr.Number(value=10, label="Food Count")
    status_box = gr.Textbox()

    def eat(food):
        if food > 0:
            return {food_box: food - 1, status_box: "full"}
        else:
            return {status_box: "hungry"}

    gr.Button("Eat").click(
        fn=eat,
        inputs=food_box,
        outputs=[food_box, status_box]
    )
```

Notice how when there is no food, we only update the `status_box` element. We skipped updating the `food_box` component.

Dictionary returns are helpful when an event listener affects many components on return, or conditionally affects outputs and not others.

Keep in mind that with dictionary returns, we still need to specify the possible outputs in the event listener.

## Updating Component Configurations

The return value of an event listener function is usually the updated value of the corresponding output Component. Sometimes we want to update the configuration of the Component as well, such as the visibility. In this case, we return a new Component, setting the properties we want to change.

$code_blocks_essay_simple
$demo_blocks_essay_simple

See how we can configure the Textbox itself through a new `gr.Textbox()` method. The `value=` argument can still be used to update the value along with Component configuration. Any arguments we do not set will preserve their previous values.

## Not Changing a Component's Value

In some cases, you may want to leave a component's value unchanged. Gradio includes a special function, `gr.skip()`, which can be returned from your function. Returning this function will keep the output component (or components') values as is. Let us illustrate with an example:

$code_skip
$demo_skip

Note the difference between returning `None` (which generally resets a component's value to an empty state) versus returning `gr.skip()`, which leaves the component value unchanged.

Tip: if you have multiple output components, and you want to leave all of their values unchanged, you can just return a single `gr.skip()` instead of returning a tuple of skips, one for each element.

## Running Events Consecutively

You can also run events consecutively by using the `then` method of an event listener. This will run an event after the previous event has finished running. This is useful for running events that update components in multiple steps.

For example, in the chatbot example below, we first update the chatbot with the user message immediately, and then update the chatbot with the computer response after a simulated delay.

$code_chatbot_consecutive
$demo_chatbot_consecutive

The `.then()` method of an event listener executes the subsequent event regardless of whether the previous event raised any errors. If you'd like to only run subsequent events if the previous event executed successfully, use the `.success()` method, which takes the same arguments as `.then()`. Conversely, if you'd like to only run subsequent events if the previous event failed (i.e., raised an error), use the `.failure()` method. This is particularly useful for error handling workflows, such as displaying error messages or restoring previous states when an operation fails.

## Binding Multiple Triggers to a Function

Often times, you may want to bind multiple triggers to the same function. For example, you may want to allow a user to click a submit button, or press enter to submit a form. You can do this using the `gr.on` method and passing a list of triggers to the `trigger`.

$code_on_listener_basic
$demo_on_listener_basic

You can use decorator syntax as well:

$code_on_listener_decorator

You can use `gr.on` to create "live" events by binding to the `change` event of components that implement it. If you do not specify any triggers, the function will automatically bind to all `change` event of all input components that include a `change` event (for example `gr.Textbox` has a `change` event whereas `gr.Button` does not).

$code_on_listener_live
$demo_on_listener_live

You can follow `gr.on` with `.then`, just like any regular event listener. This handy method should save you from having to write a lot of repetitive code!

## Binding a Component Value Directly to a Function of Other Components

If you want to set a Component's value to always be a function of the value of other Components, you can use the following shorthand:

```python
with gr.Blocks() as demo:
  num1 = gr.Number()
  num2 = gr.Number()
  product = gr.Number(lambda a, b: a * b, inputs=[num1, num2])
```

This functionally the same as:
```python
with gr.Blocks() as demo:
  num1 = gr.Number()
  num2 = gr.Number()
  product = gr.Number()

  gr.on(
    [num1.change, num2.change, demo.load], 
    lambda a, b: a * b, 
    inputs=[num1, num2], 
    outputs=product
  )
```

---

<!-- Source: guides/04_additional-features/01_queuing.md -->
# Queuing

Every Gradio app comes with a built-in queuing system that can scale to thousands of concurrent users. Because many of your event listeners may involve heavy processing, Gradio automatically creates a queue to handle every event listener in the backend. Every event listener in your app automatically has a queue to process incoming events.

## Configuring the Queue

By default, each event listener has its own queue, which handles one request at a time. This can be configured via two arguments:

- `concurrency_limit`: This sets the maximum number of concurrent executions for an event listener. By default, the limit is 1 unless configured otherwise in `Blocks.queue()`. You can also set it to `None` for no limit (i.e., an unlimited number of concurrent executions). For example:

```python
import gradio as gr

with gr.Blocks() as demo:
    prompt = gr.Textbox()
    image = gr.Image()
    generate_btn = gr.Button("Generate Image")
    generate_btn.click(image_gen, prompt, image, concurrency_limit=5)
```

In the code above, up to 5 requests can be processed simultaneously for this event listener. Additional requests will be queued until a slot becomes available.

If you want to manage multiple event listeners using a shared queue, you can use the `concurrency_id` argument:

- `concurrency_id`: This allows event listeners to share a queue by assigning them the same ID. For example, if your setup has only 2 GPUs but multiple functions require GPU access, you can create a shared queue for all those functions. Here's how that might look:

```python
import gradio as gr

with gr.Blocks() as demo:
    prompt = gr.Textbox()
    image = gr.Image()
    generate_btn_1 = gr.Button("Generate Image via model 1")
    generate_btn_2 = gr.Button("Generate Image via model 2")
    generate_btn_3 = gr.Button("Generate Image via model 3")
    generate_btn_1.click(image_gen_1, prompt, image, concurrency_limit=2, concurrency_id="gpu_queue")
    generate_btn_2.click(image_gen_2, prompt, image, concurrency_id="gpu_queue")
    generate_btn_3.click(image_gen_3, prompt, image, concurrency_id="gpu_queue")
```

In this example, all three event listeners share a queue identified by `"gpu_queue"`. The queue can handle up to 2 concurrent requests at a time, as defined by the `concurrency_limit`.

### Notes

- To ensure unlimited concurrency for an event listener, set `concurrency_limit=None`.  This is useful if your function is calling e.g. an external API which handles the rate limiting of requests itself.
- The default concurrency limit for all queues can be set globally using the `default_concurrency_limit` parameter in `Blocks.queue()`. 

These configurations make it easy to manage the queuing behavior of your Gradio app.

---

<!-- Source: guides/05_chatbots/01_creating-a-chatbot-fast.md -->
# How to Create a Chatbot with Gradio

Tags: LLM, CHATBOT, NLP

## Introduction

Chatbots are a popular application of large language models (LLMs). Using Gradio, you can easily build a chat application and share that with your users, or try it yourself using an intuitive UI.

This tutorial uses `gr.ChatInterface()`, which is a high-level abstraction that allows you to create your chatbot UI fast, often with a _few lines of Python_. It can be easily adapted to support multimodal chatbots, or chatbots that require further customization.

**Prerequisites**: please make sure you are using the latest version of Gradio:

```bash
$ pip install --upgrade gradio
```

## Note for OpenAI-API compatible endpoints

If you have a chat server serving an OpenAI-API compatible endpoint (such as Ollama), you can spin up a ChatInterface in a single line of Python. First, also run `pip install openai`. Then, with your own URL, model, and optional token:

```python
import gradio as gr

gr.load_chat("http://localhost:11434/v1/", model="llama3.2", token="***").launch()
```

Read about `gr.load_chat` in [the docs](https://www.gradio.app/docs/gradio/load_chat). If you have your own model, keep reading to see how to create an application around any chat model in Python!

## Defining a chat function

To create a chat application with `gr.ChatInterface()`, the first thing you should do is define your **chat function**. In the simplest case, your chat function should accept two arguments: `message` and `history` (the arguments can be named anything, but must be in this order).

- `message`: a `str` representing the user's most recent message.
- `history`: a list of openai-style dictionaries with `role` and `content` keys, representing the previous conversation history. May also include additional keys representing message metadata.

The `history` would look like this:

```python
[
    {"role": "user", "content": [{"type": "text", "text": "What is the capital of France?"}]},
    {"role": "assistant", "content": [{"type": "text", "text": "Paris"}]}
]
```

while the next `message` would be:

```py
"And what is its largest city?"
```

Your chat function simply needs to return: 

* a `str` value, which is the chatbot's response based on the chat `history` and most recent `message`, for example, in this case:

```
Paris is also the largest city.
```

Let's take a look at a few example chat functions:

**Example: a chatbot that randomly responds with yes or no**

Let's write a chat function that responds `Yes` or `No` randomly.

Here's our chat function:

```python
import random

def random_response(message, history):
    return random.choice(["Yes", "No"])
```

Now, we can plug this into `gr.ChatInterface()` and call the `.launch()` method to create the web interface:

```python
import gradio as gr

gr.ChatInterface(
    fn=random_response, 
).launch()
```

That's it! Here's our running demo, try it out:

$demo_chatinterface_random_response

**Example: a chatbot that alternates between agreeing and disagreeing**

Of course, the previous example was very simplistic, it didn't take user input or the previous history into account! Here's another simple example showing how to incorporate a user's input as well as the history.

```python
import gradio as gr

def alternatingly_agree(message, history):
    if len([h for h in history if h['role'] == "assistant"]) % 2 == 0:
        return f"Yes, I do think that: {message}"
    else:
        return "I don't think so"

gr.ChatInterface(
    fn=alternatingly_agree, 
).launch()
```

We'll look at more realistic examples of chat functions in our next Guide, which shows [examples of using `gr.ChatInterface` with popular LLMs](../guides/chatinterface-examples). 

## Streaming chatbots

In your chat function, you can use `yield` to generate a sequence of partial responses, each replacing the previous ones. This way, you'll end up with a streaming chatbot. It's that simple!

```python
import time
import gradio as gr

def slow_echo(message, history):
    for i in range(len(message)):
        time.sleep(0.3)
        yield "You typed: " + message[: i+1]

gr.ChatInterface(
    fn=slow_echo, 
).launch()
```

While the response is streaming, the "Submit" button turns into a "Stop" button that can be used to stop the generator function.

Tip: Even though you are yielding the latest message at each iteration, Gradio only sends the "diff" of each message from the server to the frontend, which reduces latency and data consumption over your network.

## Customizing the Chat UI

If you're familiar with Gradio's `gr.Interface` class, the `gr.ChatInterface` includes many of the same arguments that you can use to customize the look and feel of your Chatbot. For example, you can:

- add a title and description above your chatbot using `title` and `description` arguments.
- add a theme or custom css using `theme` and `css` arguments respectively in the `launch()` method.
- add `examples` and even enable `cache_examples`, which make your Chatbot easier for users to try it out.
- customize the chatbot (e.g. to change the height or add a placeholder) or textbox (e.g. to add a max number of characters or add a placeholder).

**Adding examples**

You can add preset examples to your `gr.ChatInterface` with the `examples` parameter, which takes a list of string examples. Any examples will appear as "buttons" within the Chatbot before any messages are sent. If you'd like to include images or other files as part of your examples, you can do so by using this dictionary format for each example instead of a string: `{"text": "What's in this image?", "files": ["cheetah.jpg"]}`. Each file will be a separate message that is added to your Chatbot history.

You can change the displayed text for each example by using the `example_labels` argument. You can add icons to each example as well using the `example_icons` argument. Both of these arguments take a list of strings, which should be the same length as the `examples` list.

If you'd like to cache the examples so that they are pre-computed and the results appear instantly, set `cache_examples=True`.

**Customizing the chatbot or textbox component**

If you want to customize the `gr.Chatbot` or `gr.Textbox` that compose the `ChatInterface`, then you can pass in your own chatbot or textbox components. Here's an example of how we to apply the parameters we've discussed in this section:

```python
import gradio as gr

def yes_man(message, history):
    if message.endswith("?"):
        return "Yes"
    else:
        return "Ask me anything!"

gr.ChatInterface(
    yes_man,
    chatbot=gr.Chatbot(height=300),
    textbox=gr.Textbox(placeholder="Ask me a yes or no question", container=False, scale=7),
    title="Yes Man",
    description="Ask Yes Man any question",
    examples=["Hello", "Am I cool?", "Are tomatoes vegetables?"],
    cache_examples=True,
).launch(theme="ocean")
```

Here's another example that adds a "placeholder" for your chat interface, which appears before the user has started chatting. The `placeholder` argument of `gr.Chatbot` accepts Markdown or HTML:

```python
gr.ChatInterface(
    yes_man,
    chatbot=gr.Chatbot(placeholder="<strong>Your Personal Yes-Man</strong><br>Ask Me Anything"),
...
```

The placeholder appears vertically and horizontally centered in the chatbot.

## Multimodal Chat Interface

You may want to add multimodal capabilities to your chat interface. For example, you may want users to be able to upload images or files to your chatbot and ask questions about them. You can make your chatbot "multimodal" by passing in a single parameter (`multimodal=True`) to the `gr.ChatInterface` class.

When `multimodal=True`, the signature of your chat function changes slightly: the first parameter of your function (what we referred to as `message` above) should accept a dictionary consisting of the submitted text and uploaded files that looks like this: 

```py
{
    "text": "user input", 
    "files": [
        "updated_file_1_path.ext",
        "updated_file_2_path.ext", 
        ...
    ]
}
```

This second parameter of your chat function, `history`, will be in the same openai-style dictionary format as before. However, if the history contains uploaded files, the `content` key will be a dictionary with a "type" key whose value is "file" and the file will be represented as a dictionary. All the files will be grouped in message in the history. So after uploading two files and asking a question, your history might look like this:

```python
[
    {"role": "user", "content": [{"type": "file", "file": {"path": "cat1.png"}},
                                 {"type": "file", "file": {"path": "cat1.png"}},
                                 {"type": "text", "text": "What's the difference between these two images?"}]}
]
```

The return type of your chat function does *not change* when setting `multimodal=True` (i.e. in the simplest case, you should still return a string value). We discuss more complex cases, e.g. returning files [below](#returning-complex-responses).

If you are customizing a multimodal chat interface, you should pass in an instance of `gr.MultimodalTextbox` to the `textbox` parameter. You can customize the `MultimodalTextbox` further by passing in the `sources` parameter, which is a list of sources to enable. Here's an example that illustrates how to set up and customize and multimodal chat interface:
 

```python
import gradio as gr

def count_images(message, history):
    num_images = len(message["files"])
    total_images = 0
    for message in history:
        for content in message["content"]:
            if content["type"] == "file":
                total_images += 1
    return f"You just uploaded {num_images} images, total uploaded: {total_images+num_images}"

demo = gr.ChatInterface(
    fn=count_images, 
    examples=[
        {"text": "No files", "files": []}
    ], 
    multimodal=True,
    textbox=gr.MultimodalTextbox(file_count="multiple", file_types=["image"], sources=["upload", "microphone"])
)

demo.launch()
```

## Additional Inputs

You may want to add additional inputs to your chat function and expose them to your users through the chat UI. For example, you could add a textbox for a system prompt, or a slider that sets the number of tokens in the chatbot's response. The `gr.ChatInterface` class supports an `additional_inputs` parameter which can be used to add additional input components.

The `additional_inputs` parameters accepts a component or a list of components. You can pass the component instances directly, or use their string shortcuts (e.g. `"textbox"` instead of `gr.Textbox()`). If you pass in component instances, and they have _not_ already been rendered, then the components will appear underneath the chatbot within a `gr.Accordion()`. 

Here's a complete example:

$code_chatinterface_system_prompt

If the components you pass into the `additional_inputs` have already been rendered in a parent `gr.Blocks()`, then they will _not_ be re-rendered in the accordion. This provides flexibility in deciding where to lay out the input components. In the example below, we position the `gr.Textbox()` on top of the Chatbot UI, while keeping the slider underneath.

```python
import gradio as gr
import time

def echo(message, history, system_prompt, tokens):
    response = f"System prompt: {system_prompt}\n Message: {message}."
    for i in range(min(len(response), int(tokens))):
        time.sleep(0.05)
        yield response[: i+1]

with gr.Blocks() as demo:
    system_prompt = gr.Textbox("You are helpful AI.", label="System Prompt")
    slider = gr.Slider(10, 100, render=False)

    gr.ChatInterface(
        echo, additional_inputs=[system_prompt, slider],
    )

demo.launch()
```

**Examples with additional inputs**

You can also add example values for your additional inputs. Pass in a list of lists to the `examples` parameter, where each inner list represents one sample, and each inner list should be `1 + len(additional_inputs)` long. The first element in the inner list should be the example value for the chat message, and each subsequent element should be an example value for one of the additional inputs, in order. When additional inputs are provided, examples are rendered in a table underneath the chat interface.

If you need to create something even more custom, then its best to construct the chatbot UI using the low-level `gr.Blocks()` API. We have [a dedicated guide for that here](/guides/creating-a-custom-chatbot-with-blocks).

## Additional Outputs

In the same way that you can accept additional inputs into your chat function, you can also return additional outputs. Simply pass in a list of components to the `additional_outputs` parameter in `gr.ChatInterface` and return additional values for each component from your chat function. Here's an example that extracts code and outputs it into a separate `gr.Code` component:

$code_chatinterface_artifacts

**Note:** unlike the case of additional inputs, the components passed in `additional_outputs` must be already defined in your `gr.Blocks` context -- they are not rendered automatically. If you need to render them after your `gr.ChatInterface`, you can set `render=False` when they are first defined and then `.render()` them in the appropriate section of your `gr.Blocks()` as we do in the example above.

## Returning Complex Responses

We mentioned earlier that in the simplest case, your chat function should return a `str` response, which will be rendered as Markdown in the chatbot. However, you can also return more complex responses as we discuss below:


**Returning files or Gradio components**

Currently, the following Gradio components can be displayed inside the chat interface:
* `gr.Image`
* `gr.Plot`
* `gr.Audio`
* `gr.HTML`
* `gr.Video`
* `gr.Gallery`
* `gr.File`

Simply return one of these components from your function to use it with `gr.ChatInterface`. Here's an example that returns an audio file:

```py
import gradio as gr

def music(message, history):
    if message.strip():
        return gr.Audio("https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav")
    else:
        return "Please provide the name of an artist"

gr.ChatInterface(
    music,
    textbox=gr.Textbox(placeholder="Which artist's music do you want to listen to?", scale=7),
).launch()
```

Similarly, you could return image files with `gr.Image`, video files with `gr.Video`, or arbitrary files with the `gr.File` component.

**Returning Multiple Messages**

You can return multiple assistant messages from your chat function simply by returning a `list` of messages, each of which is a valid chat type. This lets you, for example, send a message along with files, as in the following example:

$code_chatinterface_echo_multimodal


**Displaying intermediate thoughts or tool usage**

The `gr.ChatInterface` class supports displaying intermediate thoughts or tool usage direct in the chatbot.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/nested-thought.png)

 To do this, you will need to return a `gr.ChatMessage` object from your chat function. Here is the schema of the `gr.ChatMessage` data class as well as two internal typed dictionaries:
 
 ```py
MessageContent = Union[str, FileDataDict, FileData, Component]

@dataclass
class ChatMessage:
    content: MessageContent | list[MessageContent]
    metadata: MetadataDict = None
    options: list[OptionDict] = None

class MetadataDict(TypedDict):
    title: NotRequired[str]
    id: NotRequired[int | str]
    parent_id: NotRequired[int | str]
    log: NotRequired[str]
    duration: NotRequired[float]
    status: NotRequired[Literal["pending", "done"]]

class OptionDict(TypedDict):
    label: NotRequired[str]
    value: str
 ```
 
As you can see, the `gr.ChatMessage` dataclass is similar to the openai-style message format, e.g. it has a "content" key that refers to the chat message content. But it also includes a "metadata" key whose value is a dictionary. If this dictionary includes a "title" key, the resulting message is displayed as an intermediate thought with the title being displayed on top of the thought. Here's an example showing the usage:

$code_chatinterface_thoughts

You can even show nested thoughts, which is useful for agent demos in which one tool may call other tools. To display nested thoughts, include "id" and "parent_id" keys in the "metadata" dictionary. Read our [dedicated guide on displaying intermediate thoughts and tool usage](/guides/agents-and-tool-usage) for more realistic examples.

**Providing preset responses**

When returning an assistant message, you may want to provide preset options that a user can choose in response. To do this, again, you will again return a `gr.ChatMessage` instance from your chat function. This time, make sure to set the `options` key specifying the preset responses.

As shown in the schema for `gr.ChatMessage` above, the value corresponding to the `options` key should be a list of dictionaries, each with a `value` (a string that is the value that should be sent to the chat function when this response is clicked) and an optional `label` (if provided, is the text displayed as the preset response instead of the `value`). 

This example illustrates how to use preset responses:

$code_chatinterface_options

## Modifying the Chatbot Value Directly

You may wish to modify the value of the chatbot with your own events, other than those prebuilt in the `gr.ChatInterface`. For example, you could create a dropdown that prefills the chat history with certain conversations or add a separate button to clear the conversation history. The `gr.ChatInterface` supports these events, but you need to use the `gr.ChatInterface.chatbot_value` as the input or output component in such events. In this example, we use a `gr.Radio` component to prefill the the chatbot with certain conversations:

$code_chatinterface_prefill

## Using Your Chatbot via API

Once you've built your Gradio chat interface and are hosting it on [Hugging Face Spaces](https://hf.space) or somewhere else, then you can query it with a simple API. The API route will be the name of the function you pass to the ChatInterface. So if `gr.ChatInterface(respond)`, then the API route is `/respond`. The endpoint just expects the user's message and will return the response, internally keeping track of the message history.

![](https://github.com/gradio-app/gradio/assets/1778297/7b10d6db-6476-4e2e-bebd-ecda802c3b8f)

To use the endpoint, you should use either the [Gradio Python Client](/guides/getting-started-with-the-python-client) or the [Gradio JS client](/guides/getting-started-with-the-js-client). Or, you can deploy your Chat Interface to other platforms, such as a:

* Slack bot [[tutorial]](../guides/creating-a-slack-bot-from-a-gradio-app)
* Website widget [[tutorial]](../guides/creating-a-website-widget-from-a-gradio-chatbot)

## Chat History

You can enable persistent chat history for your ChatInterface, allowing users to maintain multiple conversations and easily switch between them. When enabled, conversations are stored locally and privately in the user's browser using local storage. So if you deploy a ChatInterface e.g. on [Hugging Face Spaces](https://hf.space), each user will have their own separate chat history that won't interfere with other users' conversations. This means multiple users can interact with the same ChatInterface simultaneously while maintaining their own private conversation histories.

To enable this feature, simply set `gr.ChatInterface(save_history=True)` (as shown in the example in the next section). Users will then see their previous conversations in a side panel and can continue any previous chat or start a new one.

## Collecting User Feedback

To gather feedback on your chat model, set `gr.ChatInterface(flagging_mode="manual")` and users will be able to thumbs-up or thumbs-down assistant responses. Each flagged response, along with the entire chat history, will get saved in a CSV file in the app working directory (this can be configured via the `flagging_dir` parameter). 

You can also change the feedback options via `flagging_options` parameter. The default options are "Like" and "Dislike", which appear as the thumbs-up and thumbs-down icons. Any other options appear under a dedicated flag icon. This example shows a ChatInterface that has both chat history (mentioned in the previous section) and user feedback enabled:

$code_chatinterface_streaming_echo

Note that in this example, we set several flagging options: "Like", "Spam", "Inappropriate", "Other". Because the case-sensitive string "Like" is one of the flagging options, the user will see a thumbs-up icon next to each assistant message. The three other flagging options will appear in a dropdown under the flag icon.

## What's Next?

Now that you've learned about the `gr.ChatInterface` class and how it can be used to create chatbot UIs quickly, we recommend reading one of the following:

* [Our next Guide](../guides/chatinterface-examples) shows examples of how to use `gr.ChatInterface` with popular LLM libraries.
* If you'd like to build very custom chat applications from scratch, you can build them using the low-level Blocks API, as [discussed in this Guide](../guides/creating-a-custom-chatbot-with-blocks).
* Once you've deployed your Gradio Chat Interface, its easy to use in other applications because of the built-in API. Here's a tutorial on [how to deploy a Gradio chat interface as a Discord bot](../guides/creating-a-discord-bot-from-a-gradio-app).

---

<!-- Source: guides/06_data-science-and-plots/01_creating-plots.md -->
# Creating Plots

Gradio is a great way to create extremely customizable dashboards. Gradio comes with three native Plot components: `gr.LinePlot`, `gr.ScatterPlot` and `gr.BarPlot`. All these plots have the same API. Let's take a look how to set them up.

## Creating a Plot with a pd.Dataframe

Plots accept a pandas Dataframe as their value. The plot also takes `x` and `y` which represent the names of the columns that represent the x and y axes respectively. Here's a simple example:

$code_plot_guide_line
$demo_plot_guide_line

All plots have the same API, so you could swap this out with a `gr.ScatterPlot`:

$code_plot_guide_scatter
$demo_plot_guide_scatter

The y axis column in the dataframe should have a numeric type, but the x axis column can be anything from strings, numbers, categories, or datetimes.

$code_plot_guide_scatter_nominal
$demo_plot_guide_scatter_nominal

## Breaking out Series by Color

You can break out your plot into series using the `color` argument.

$code_plot_guide_series_nominal
$demo_plot_guide_series_nominal

If you wish to assign series specific colors, use the `color_map` arg, e.g. `gr.ScatterPlot(..., color_map={'white': '#FF9988', 'asian': '#88EEAA', 'black': '#333388'})`

The color column can be numeric type as well.

$code_plot_guide_series_quantitative
$demo_plot_guide_series_quantitative

## Aggregating Values

You can aggregate values into groups using the `x_bin` and `y_aggregate` arguments. If your x-axis is numeric, providing an `x_bin` will create a histogram-style binning:

$code_plot_guide_aggregate_quantitative
$demo_plot_guide_aggregate_quantitative

If your x-axis is a string type instead, they will act as the category bins automatically:

$code_plot_guide_aggregate_nominal
$demo_plot_guide_aggregate_nominal

## Selecting Regions

You can use the `.select` listener to select regions of a plot. Click and drag on the plot below to select part of the plot.

$code_plot_guide_selection
$demo_plot_guide_selection

You can combine this and the `.double_click` listener to create some zoom in/out effects by changing `x_lim` which sets the bounds of the x-axis:

$code_plot_guide_zoom
$demo_plot_guide_zoom

If you had multiple plots with the same x column, your event listeners could target the x limits of all other plots so that the x-axes stay in sync.

$code_plot_guide_zoom_sync
$demo_plot_guide_zoom_sync

## Making an Interactive Dashboard

Take a look how you can have an interactive dashboard where the plots are functions of other Components.

$code_plot_guide_interactive
$demo_plot_guide_interactive

It's that simple to filter and control the data presented in your visualization!

---

<!-- Source: guides/07_streaming/01_streaming-ai-generated-audio.md -->
# Streaming AI Generated Audio

Tags: AUDIO, STREAMING

In this guide, we'll build a novel AI application to showcase Gradio's audio output streaming. We're going to a build a talking [Magic 8 Ball](https://en.wikipedia.org/wiki/Magic_8_Ball) ðŸŽ±

A Magic 8 Ball is a toy that answers any question after you shake it. Our application will do the same but it will also speak its response!

We won't cover all the implementation details in this blog post but the code is freely available on [Hugging Face Spaces](https://huggingface.co/spaces/gradio/magic-8-ball).

## The Overview

Just like the classic Magic 8 Ball, a user should ask it a question orally and then wait for a response. Under the hood, we'll use Whisper to transcribe the audio and then use an LLM to generate a magic-8-ball-style answer. Finally, we'll use Parler TTS to read the response aloud.

## The UI

First let's define the UI and put placeholders for all the python logic.

```python
import gradio as gr

with gr.Blocks() as block:
    gr.HTML(
        f"""
        <h1 style='text-align: center;'> Magic 8 Ball ðŸŽ± </h1>
        <h3 style='text-align: center;'> Ask a question and receive wisdom </h3>
        <p style='text-align: center;'> Powered by <a href="https://github.com/huggingface/parler-tts"> Parler-TTS</a>
        """
    )
    with gr.Group():
        with gr.Row():
            audio_out = gr.Audio(label="Spoken Answer", streaming=True, autoplay=True)
            answer = gr.Textbox(label="Answer")
            state = gr.State()
        with gr.Row():
            audio_in = gr.Audio(label="Speak your question", sources="microphone", type="filepath")

    audio_in.stop_recording(generate_response, audio_in, [state, answer, audio_out])\
        .then(fn=read_response, inputs=state, outputs=[answer, audio_out])

block.launch()
```

We're placing the output Audio and Textbox components and the input Audio component in separate rows. In order to stream the audio from the server, we'll set `streaming=True` in the output Audio component. We'll also set `autoplay=True` so that the audio plays as soon as it's ready.
We'll be using the Audio input component's `stop_recording` event to trigger our application's logic when a user stops recording from their microphone.

We're separating the logic into two parts. First, `generate_response` will take the recorded audio, transcribe it and generate a response with an LLM. We're going to store the response in a `gr.State` variable that then gets passed to the `read_response` function that generates the audio.

We're doing this in two parts because only `read_response` will require a GPU. Our app will run on Hugging Faces [ZeroGPU](https://huggingface.co/zero-gpu-explorers) which has time-based quotas. Since generating the response can be done with Hugging Face's Inference API, we shouldn't include that code in our GPU function as it will needlessly use our GPU quota.

## The Logic

As mentioned above, we'll use [Hugging Face's Inference API](https://huggingface.co/docs/huggingface_hub/guides/inference) to transcribe the audio and generate a response from an LLM. After instantiating the client, I use the `automatic_speech_recognition` method (this automatically uses Whisper running on Hugging Face's Inference Servers) to transcribe the audio. Then I pass the question to an LLM (Mistal-7B-Instruct) to generate a response. We are prompting the LLM to act like a magic 8 ball with the system message.

Our `generate_response` function will also send empty updates to the output textbox and audio components (returning `None`). 
This is because I want the Gradio progress tracker to be displayed over the components but I don't want to display the answer until the audio is ready.


```python
from huggingface_hub import InferenceClient

client = InferenceClient(token=os.getenv("HF_TOKEN"))

def generate_response(audio):
    gr.Info("Transcribing Audio", duration=5)
    question = client.automatic_speech_recognition(audio).text

    messages = [{"role": "system", "content": ("You are a magic 8 ball."
                                              "Someone will present to you a situation or question and your job "
                                              "is to answer with a cryptic adage or proverb such as "
                                              "'curiosity killed the cat' or 'The early bird gets the worm'."
                                              "Keep your answers short and do not include the phrase 'Magic 8 Ball' in your response. If the question does not make sense or is off-topic, say 'Foolish questions get foolish answers.'"
                                              "For example, 'Magic 8 Ball, should I get a dog?', 'A dog is ready for you but are you ready for the dog?'")},
                {"role": "user", "content": f"Magic 8 Ball please answer this question -  {question}"}]
    
    response = client.chat_completion(messages, max_tokens=64, seed=random.randint(1, 5000),
                                      model="mistralai/Mistral-7B-Instruct-v0.3")

    response = response.choices[0].message.content.replace("Magic 8 Ball", "").replace(":", "")
    return response, None, None
```


Now that we have our text response, we'll read it aloud with Parler TTS. The `read_response` function will be a python generator that yields the next chunk of audio as it's ready.


We'll be using the [Mini v0.1](https://huggingface.co/parler-tts/parler_tts_mini_v0.1) for the feature extraction but the [Jenny fine tuned version](https://huggingface.co/parler-tts/parler-tts-mini-jenny-30H) for the voice. This is so that the voice is consistent across generations.


Streaming audio with transformers requires a custom Streamer class. You can see the implementation [here](https://huggingface.co/spaces/gradio/magic-8-ball/blob/main/streamer.py). Additionally, we'll convert the output to bytes so that it can be streamed faster from the backend. 


```python
from streamer import ParlerTTSStreamer
from transformers import AutoTokenizer, AutoFeatureExtractor, set_seed
import numpy as np
import spaces
import torch
from threading import Thread


device = "cuda:0" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
torch_dtype = torch.float16 if device != "cpu" else torch.float32

repo_id = "parler-tts/parler_tts_mini_v0.1"

jenny_repo_id = "ylacombe/parler-tts-mini-jenny-30H"

model = ParlerTTSForConditionalGeneration.from_pretrained(
    jenny_repo_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True
).to(device)

tokenizer = AutoTokenizer.from_pretrained(repo_id)
feature_extractor = AutoFeatureExtractor.from_pretrained(repo_id)

sampling_rate = model.audio_encoder.config.sampling_rate
frame_rate = model.audio_encoder.config.frame_rate

@spaces.GPU
def read_response(answer):

    play_steps_in_s = 2.0
    play_steps = int(frame_rate * play_steps_in_s)

    description = "Jenny speaks at an average pace with a calm delivery in a very confined sounding environment with clear audio quality."
    description_tokens = tokenizer(description, return_tensors="pt").to(device)

    streamer = ParlerTTSStreamer(model, device=device, play_steps=play_steps)
    prompt = tokenizer(answer, return_tensors="pt").to(device)

    generation_kwargs = dict(
        input_ids=description_tokens.input_ids,
        prompt_input_ids=prompt.input_ids,
        streamer=streamer,
        do_sample=True,
        temperature=1.0,
        min_new_tokens=10,
    )

    set_seed(42)
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    for new_audio in streamer:
        print(f"Sample of length: {round(new_audio.shape[0] / sampling_rate, 2)} seconds")
        yield answer, numpy_to_mp3(new_audio, sampling_rate=sampling_rate)
```

## Conclusion

You can see our final application [here](https://huggingface.co/spaces/gradio/magic-8-ball)!

---

<!-- Source: guides/08_custom-components/01_custom-components-in-five-minutes.md -->
# Custom Components in 5 minutes

Gradio includes the ability for developers to create their own custom components and use them in Gradio apps. You can publish your components as Python packages so that other users can use them as well.

Users will be able to use all of Gradio's existing functions, such as `gr.Blocks`, `gr.Interface`, API usage, themes, etc. with Custom Components. This guide will cover how to get started making custom components.

## Installation

You will need to have:

* Python 3.10+ (<a href="https://www.python.org/downloads/" target="_blank">install here</a>)
* pip 21.3+ (`python -m pip install --upgrade pip`)
* Node.js 20+ (<a href="https://nodejs.dev/en/download/package-manager/" target="_blank">install here</a>)
* npm 9+ (<a href="https://docs.npmjs.com/downloading-and-installing-node-js-and-npm/" target="_blank">install here</a>)
* Gradio 5+ (`pip install --upgrade gradio`)

## The Workflow

The Custom Components workflow consists of 4 steps: create, dev, build, and publish.

1. create: creates a template for you to start developing a custom component.
2. dev: launches a development server with a sample app & hot reloading allowing you to easily develop your custom component
3. build: builds a python package containing to your custom component's Python and JavaScript code -- this makes things official!
4. publish: uploads your package to [PyPi](https://pypi.org/) and/or a sample app to [HuggingFace Spaces](https://hf.co/spaces).

Each of these steps is done via the Custom Component CLI. You can invoke it with `gradio cc` or `gradio component`

Tip: Run `gradio cc --help` to get a help menu of all available commands. There are some commands that are not covered in this guide. You can also append `--help` to any command name to bring up a help page for that command, e.g. `gradio cc create --help`.

## 1. create

Bootstrap a new template by running the following in any working directory:

```bash
gradio cc create MyComponent --template SimpleTextbox
```

Instead of `MyComponent`, give your component any name.

Instead of `SimpleTextbox`, you can use any Gradio component as a template. `SimpleTextbox` is actually a special component that a stripped-down version of the `Textbox` component that makes it particularly useful when creating your first custom component.
Some other components that are good if you are starting out: `SimpleDropdown`, `SimpleImage`, or `File`.

Tip: Run `gradio cc show` to get a list of available component templates.

The `create` command will:

1. Create a directory with your component's name in lowercase with the following structure:
```directory
- backend/ <- The python code for your custom component
- frontend/ <- The javascript code for your custom component
- demo/ <- A sample app using your custom component. Modify this to develop your component!
- pyproject.toml <- Used to build the package and specify package metadata.
```

2. Install the component in development mode

Each of the directories will have the code you need to get started developing!

## 2. dev

Once you have created your new component, you can start a development server by `entering the directory` and running

```bash
gradio cc dev
```

You'll see several lines that are printed to the console.
The most important one is the one that says:

> Frontend Server (Go here): http://localhost:7861/

The port number might be different for you.
Click on that link to launch the demo app in hot reload mode.
Now, you can start making changes to the backend and frontend you'll see the results reflected live in the sample app!
We'll go through a real example in a later guide.

Tip: You don't have to run dev mode from your custom component directory. The first argument to `dev` mode is the path to the directory. By default it uses the current directory.

## 3. build

Once you are satisfied with your custom component's implementation, you can `build` it to use it outside of the development server.

From your component directory, run:

```bash
gradio cc build
```

This will create a `tar.gz` and `.whl` file in a `dist/` subdirectory.
If you or anyone installs that `.whl` file (`pip install <path-to-whl>`) they will be able to use your custom component in any gradio app!

The `build` command will also generate documentation for your custom component. This takes the form of an interactive space and a static `README.md`. You can disable this by passing `--no-generate-docs`. You can read more about the documentation generator in [the dedicated guide](https://gradio.app/guides/documenting-custom-components).

## 4. publish

Right now, your package is only available on a `.whl` file on your computer.
You can share that file with the world with the `publish` command!

Simply run the following command from your component directory:

```bash
gradio cc publish
```

This will guide you through the following process:

1. Upload your distribution files to PyPi. This makes it easier to upload the demo to Hugging Face spaces. Otherwise your package must be at a publicly available url. If you decide to upload to PyPi, you will need a PyPI username and password. You can get one [here](https://pypi.org/account/register/).
2. Upload a demo of your component to hugging face spaces. This is also optional.


Here is an example of what publishing looks like:

<video autoplay muted loop>
  <source src="https://gradio-builds.s3.amazonaws.com/assets/text_with_attachments_publish.mov" type="video/mp4" />
</video>


## Conclusion

Now that you know the high-level workflow of creating custom components, you can go in depth in the next guides!
After reading the guides, check out this [collection](https://huggingface.co/collections/gradio/custom-components-65497a761c5192d981710b12) of custom components on the HuggingFace Hub so you can learn from other's code.

Tip: If you want to start off from someone else's custom component see this [guide](./frequently-asked-questions#do-i-always-need-to-start-my-component-from-scratch).

---

<!-- Source: guides/09_gradio-clients-and-lite/01_getting-started-with-the-python-client.md -->
# Getting Started with the Gradio Python client

Tags: CLIENT, API, SPACES

The Gradio Python client makes it very easy to use any Gradio app as an API. As an example, consider this [Hugging Face Space that transcribes audio files](https://huggingface.co/spaces/abidlabs/whisper) that are recorded from the microphone.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)

Using the `gradio_client` library, we can easily use the Gradio as an API to transcribe audio files programmatically.

Here's the entire code to do it:

```python
from gradio_client import Client, handle_file

client = Client("abidlabs/whisper")

client.predict(
    audio=handle_file("audio_sample.wav")
)

>> "This is a test of the whisper speech recognition model."
```

The Gradio client works with any hosted Gradio app! Although the Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), your app can be hosted anywhere, such as your own server.

**Prerequisites**: To use the Gradio client, you do _not_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components.

## Installation

If you already have a recent version of `gradio`, then the `gradio_client` is included as a dependency. But note that this documentation reflects the latest version of the `gradio_client`, so upgrade if you're not sure!

The lightweight `gradio_client` package can be installed from pip (or pip3) and is tested to work with **Python versions 3.10 or higher**:

```bash
$ pip install --upgrade gradio_client
```

## Connecting to a Gradio App on Hugging Face Spaces

Start by connecting instantiating a `Client` object and connecting it to a Gradio app that is running on Hugging Face Spaces.

```python
from gradio_client import Client

client = Client("abidlabs/en2fr")  # a Space that translates from English to French
```

You can also connect to private Spaces by passing in your HF token with the `token` parameter. You can get your HF token here: https://huggingface.co/settings/tokens

```python
from gradio_client import Client

client = Client("abidlabs/my-private-space", token="...")
```


## Duplicating a Space for private use

While you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space,
and then use it to make as many requests as you'd like!

The `gradio_client` includes a class method: `Client.duplicate()` to make this process simple (you'll need to pass in your [Hugging Face token](https://huggingface.co/settings/tokens) or be logged in using the Hugging Face CLI):

```python
import os
from gradio_client import Client, handle_file

HF_TOKEN = os.environ.get("HF_TOKEN")

client = Client.duplicate("abidlabs/whisper", token=HF_TOKEN)
client.predict(handle_file("audio_sample.wav"))

>> "This is a test of the whisper speech recognition model."
```

If you have previously duplicated a Space, re-running `duplicate()` will _not_ create a new Space. Instead, the Client will attach to the previously-created Space. So it is safe to re-run the `Client.duplicate()` method multiple times.

**Note:** if the original Space uses GPUs, your private Space will as well, and your Hugging Face account will get billed based on the price of the GPU. To minimize charges, your Space will automatically go to sleep after 1 hour of inactivity. You can also set the hardware using the `hardware` parameter of `duplicate()`.

## Connecting a general Gradio app

If your app is running somewhere else, just provide the full URL instead, including the "http://" or "https://". Here's an example of making predictions to a Gradio app that is running on a share URL:

```python
from gradio_client import Client

client = Client("https://bec81a83-5b5c-471e.gradio.live")
```

## Connecting to a Gradio app with auth

If the Gradio application you are connecting to [requires a username and password](/guides/sharing-your-app#authentication), then provide them as a tuple to the `auth` argument of the `Client` class:

```python
from gradio_client import Client

Client(
  space_name,
  auth=[username, password]
)
```


## Inspecting the API endpoints

Once you have connected to a Gradio app, you can view the APIs that are available to you by calling the `Client.view_api()` method. For the Whisper Space, we see the following:

```bash
Client.predict() Usage Info
---------------------------
Named API endpoints: 1

 - predict(audio, api_name="/predict") -> output
    Parameters:
     - [Audio] audio: filepath (required)  
    Returns:
     - [Textbox] output: str 
```

We see  that we have 1 API endpoint in this space, and shows us how to use the API endpoint to make a prediction: we should call the `.predict()` method (which we will explore below), providing a parameter `input_audio` of type `str`, which is a `filepath or URL`.

We should also provide the `api_name='/predict'` argument to the `predict()` method. Although this isn't necessary if a Gradio app has only 1 named endpoint, it does allow us to call different endpoints in a single app if they are available.

## The "View API" Page

As an alternative to running the `.view_api()` method, you can click on the "Use via API" link in the footer of the Gradio app, which shows us the same information, along with example usage. 

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api.png)

The View API page also includes an "API Recorder" that lets you interact with the Gradio UI normally and converts your interactions into the corresponding code to run with the Python Client.

## Making a prediction

The simplest way to make a prediction is simply to call the `.predict()` function with the appropriate arguments:

```python
from gradio_client import Client

client = Client("abidlabs/en2fr")
client.predict("Hello", api_name='/predict')

>> Bonjour
```

If there are multiple parameters, then you should pass them as separate arguments to `.predict()`, like this:

```python
from gradio_client import Client

client = Client("gradio/calculator")
client.predict(4, "add", 5)

>> 9.0
```

It is recommended to provide key-word arguments instead of positional arguments:


```python
from gradio_client import Client

client = Client("gradio/calculator")
client.predict(num1=4, operation="add", num2=5)

>> 9.0
```

This allows you to take advantage of default arguments. For example, this Space includes the default value for the Slider component so you do not need to provide it when accessing it with the client.

```python
from gradio_client import Client

client = Client("abidlabs/image_generator")
client.predict(text="an astronaut riding a camel")
```

The default value is the initial value of the corresponding Gradio component. If the component does not have an initial value, but if the corresponding argument in the predict function has a default value of `None`, then that parameter is also optional in the client. Of course, if you'd like to override it, you can include it as well:

```python
from gradio_client import Client

client = Client("abidlabs/image_generator")
client.predict(text="an astronaut riding a camel", steps=25)
```

For providing files or URLs as inputs, you should pass in the filepath or URL to the file enclosed within `gradio_client.handle_file()`. This takes care of uploading the file to the Gradio server and ensures that the file is preprocessed correctly:

```python
from gradio_client import Client, handle_file

client = Client("abidlabs/whisper")
client.predict(
    audio=handle_file("https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3")
)

>> "My thought I have nobody by a beauty and will as you poured. Mr. Rochester is serve in that so don't find simpus, and devoted abode, to at might in a râ€”"
```

## Running jobs asynchronously

One should note that `.predict()` is a _blocking_ operation as it waits for the operation to complete before returning the prediction.

In many cases, you may be better off letting the job run in the background until you need the results of the prediction. You can do this by creating a `Job` instance using the `.submit()` method, and then later calling `.result()` on the job to get the result. For example:

```python
from gradio_client import Client

client = Client(space="abidlabs/en2fr")
job = client.submit("Hello", api_name="/predict")  # This is not blocking

# Do something else

job.result()  # This is blocking

>> Bonjour
```

## Adding callbacks

Alternatively, one can add one or more callbacks to perform actions after the job has completed running, like this:

```python
from gradio_client import Client

def print_result(x):
    print("The translated result is: {x}")

client = Client(space="abidlabs/en2fr")

job = client.submit("Hello", api_name="/predict", result_callbacks=[print_result])

# Do something else

>> The translated result is: Bonjour

```

## Status

The `Job` object also allows you to get the status of the running job by calling the `.status()` method. This returns a `StatusUpdate` object with the following attributes: `code` (the status code, one of a set of defined strings representing the status. See the `utils.Status` class), `rank` (the current position of this job in the queue), `queue_size` (the total queue size), `eta` (estimated time this job will complete), `success` (a boolean representing whether the job completed successfully), and `time` (the time that the status was generated).

```py
from gradio_client import Client

client = Client(src="gradio/calculator")
job = client.submit(5, "add", 4, api_name="/predict")
job.status()

>> <Status.STARTING: 'STARTING'>
```

_Note_: The `Job` class also has a `.done()` instance method which returns a boolean indicating whether the job has completed.

## Cancelling Jobs

The `Job` class also has a `.cancel()` instance method that cancels jobs that have been queued but not started. For example, if you run:

```py
client = Client("abidlabs/whisper")
job1 = client.submit(handle_file("audio_sample1.wav"))
job2 = client.submit(handle_file("audio_sample2.wav"))
job1.cancel()  # will return False, assuming the job has started
job2.cancel()  # will return True, indicating that the job has been canceled
```

If the first job has started processing, then it will not be canceled. If the second job
has not yet started, it will be successfully canceled and removed from the queue.

## Generator Endpoints

Some Gradio API endpoints do not return a single value, rather they return a series of values. You can get the series of values that have been returned at any time from such a generator endpoint by running `job.outputs()`:

```py
from gradio_client import Client

client = Client(src="gradio/count_generator")
job = client.submit(3, api_name="/count")
while not job.done():
    time.sleep(0.1)
job.outputs()

>> ['0', '1', '2']
```

Note that running `job.result()` on a generator endpoint only gives you the _first_ value returned by the endpoint.

The `Job` object is also iterable, which means you can use it to display the results of a generator function as they are returned from the endpoint. Here's the equivalent example using the `Job` as a generator:

```py
from gradio_client import Client

client = Client(src="gradio/count_generator")
job = client.submit(3, api_name="/count")

for o in job:
    print(o)

>> 0
>> 1
>> 2
```

You can also cancel jobs that that have iterative outputs, in which case the job will finish as soon as the current iteration finishes running.

```py
from gradio_client import Client
import time

client = Client("abidlabs/test-yield")
job = client.submit("abcdef")
time.sleep(3)
job.cancel()  # job cancels after 2 iterations
```

## Demos with Session State

Gradio demos can include [session state](https://www.gradio.app/guides/state-in-blocks), which provides a way for demos to persist information from user interactions within a page session.

For example, consider the following demo, which maintains a list of words that a user has submitted in a `gr.State` component. When a user submits a new word, it is added to the state, and the number of previous occurrences of that word is displayed:

```python
import gradio as gr

def count(word, list_of_words):
    return list_of_words.count(word), list_of_words + [word]

with gr.Blocks() as demo:
    words = gr.State([])
    textbox = gr.Textbox()
    number = gr.Number()
    textbox.submit(count, inputs=[textbox, words], outputs=[number, words])
    
demo.launch()
```

If you were to connect this this Gradio app using the Python Client, you would notice that the API information only shows a single input and output:

```csv
Client.predict() Usage Info
---------------------------
Named API endpoints: 1

 - predict(word, api_name="/count") -> value_31
    Parameters:
     - [Textbox] word: str (required)  
    Returns:
     - [Number] value_31: float 
```

That is because the Python client handles state automatically for you -- as you make a series of requests, the returned state from one request is stored internally and automatically supplied for the subsequent request. If you'd like to reset the state, you can do that by calling `Client.reset_session()`.

---

<!-- Source: guides/10_mcp/01_building-mcp-server-with-gradio.md -->
# Building an MCP Server with Gradio

Tags: MCP, TOOL, LLM, SERVER

In this guide, we will describe how to launch your Gradio app so that it functions as an MCP Server.

Punchline: it's as simple as setting `mcp_server=True` in `.launch()`. 

### Prerequisites

If not already installed, please install Gradio with the MCP extra:

```bash
pip install "gradio[mcp]"
```

This will install the necessary dependencies, including the `mcp` package. Also, you will need an LLM application that supports tool calling using the MCP protocol, such as Claude Desktop, Cursor, or Cline (these are known as "MCP Clients").

## What is an MCP Server?

An MCP (Model Control Protocol) server is a standardized way to expose tools so that they can be used by  LLMs. A tool can provide an LLM functionality that it does not have natively, such as the ability to generate images or calculate the prime factors of a number. 

## Example: Counting Letters in a Word

LLMs are famously not great at counting the number of letters in a word (e.g. the number of "r"-s in "strawberry"). But what if we equip them with a tool to help? Let's start by writing a simple Gradio app that counts the number of letters in a word or phrase:

$code_letter_counter

Notice that we have: (1) included a detailed docstring for our function, and (2) set `mcp_server=True` in `.launch()`. This is all that's needed for your Gradio app to serve as an MCP server! Now, when you run this app, it will:

1. Start the regular Gradio web interface
2. Start the MCP server
3. Print the MCP server URL in the console

The MCP server will be accessible at:
```
http://your-server:port/gradio_api/mcp/
```

Gradio automatically converts the `letter_counter` function into an MCP tool that can be used by LLMs. The docstring of the function and the type hints of arguments will be used to generate the description of the tool and its parameters. The name of the function will be used as the name of your tool. Any initial values you provide to your input components (e.g. "strawberry" and "r" in the `gr.Textbox` components above) will be used as the default values if your LLM doesn't specify a value for that particular input parameter.

Now, all you need to do is add this URL endpoint to your MCP Client (e.g. Claude Desktop, Cursor, or Cline), which typically means pasting this config in the settings:

```
{
  "mcpServers": {
    "gradio": {
      "url": "http://your-server:port/gradio_api/mcp/"
    }
  }
}
```

(By the way, you can find the exact config to copy-paste by going to the "View API" link in the footer of your Gradio app, and then clicking on "MCP").

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api-mcp.png)

## Key features of the Gradio <> MCP Integration

1. **Tool Conversion**: Each API endpoint in your Gradio app is automatically converted into an MCP tool with a corresponding name, description, and input schema. To view the tools and schemas, visit http://your-server:port/gradio_api/mcp/schema or go to the "View API" link in the footer of your Gradio app, and then click on "MCP".


2. **Environment variable support**. There are two ways to enable the MCP server functionality:

*  Using the `mcp_server` parameter, as shown above:
   ```python
   demo.launch(mcp_server=True)
   ```

* Using environment variables:
   ```bash
   export GRADIO_MCP_SERVER=True
   ```

3. **File Handling**: The Gradio MCP server automatically handles file data conversions, including:
   - Processing image files and returning them in the correct format
   - Managing temporary file storage

    By default, the Gradio MCP server accepts input images and files as full URLs ("http://..." or "https:/..."). For convenience, an additional STDIO-based MCP server is also generated, which can be used to upload files to any remote Gradio app and which returns a URL that can be used for subsequent tool calls.

4. **Hosted MCP Servers on ó € ðŸ¤— Spaces**: You can publish your Gradio application for free on Hugging Face Spaces, which will allow you to have a free hosted MCP server. Here's an example of such a Space: https://huggingface.co/spaces/abidlabs/mcp-tools. Notice that you can add this config to your MCP Client to start using the tools from this Space immediately:

```
{
  "mcpServers": {
    "gradio": {
      "url": "https://abidlabs-mcp-tools.hf.space/gradio_api/mcp/"
    }
  }
}
```

<video src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/mcp_guide1.mp4" style="width:100%" controls preload> </video>


## Converting an Existing Space

If there's an existing Space that you'd like to use an MCP server, you'll need to do three things:

1. First, [duplicate the Space](https://huggingface.co/docs/hub/en/spaces-more-ways-to-create#duplicating-a-space) if it is not your own Space. This will allow you to make changes to the app. If the Space requires a GPU, set the hardware of the duplicated Space to be same as the original Space. You can make it either a public Space or a private Space, since it is possible to use either as an MCP server, as described below.
2. Then, add docstrings to the functions that you'd like the LLM to be able to call as a tool. The docstring should be in the same format as the example code above.
3. Finally, add `mcp_server=True` in `.launch()`.

That's it!

## Private Spaces

You can use either a public Space or a private Space as an MCP server. If you'd like to use a private Space as an MCP server (or a ZeroGPU Space with your own quota), then you will need to provide your [Hugging Face token](https://huggingface.co/settings/token) when you make your request. To do this, simply add it as a header in your config like this:

```
{
  "mcpServers": {
    "gradio": {
      "url": "https://abidlabs-mcp-tools.hf.space/gradio_api/mcp/",
      "headers": {
        "Authorization": "Bearer <YOUR-HUGGING-FACE-TOKEN>"
      }
    }
  }
}
```

## Authentication and Credentials

You may wish to authenticate users more precisely or let them provide other kinds of credentials or tokens in order to provide a custom experience for different users. 

Gradio allows you to access the underlying `starlette.Request` that has made the tool call, which means that you can access headers, originating IP address, or any other information that is part of the network request. To do this, simply add a parameter in your function of the type `gr.Request`, and Gradio will automatically inject the request object as the parameter.

Here's an example:

```py
import gradio as gr

def echo_headers(x, request: gr.Request):
    return str(dict(request.headers))

gr.Interface(echo_headers, "textbox", "textbox").launch(mcp_server=True)
```

This MCP server will simply ignore the user's input and echo back all of the headers from a user's request. One can build more complex apps using the same idea. See the [docs on `gr.Request`](https://www.gradio.app/main/docs/gradio/request) for more information (note that only the core Starlette attributes of the `gr.Request` object will be present, attributes such as Gradio's `.session_hash` will not be present).

### Using the gr.Header class

A common pattern in MCP server development is to use authentication headers to call services on behalf of your users. Instead of using a `gr.Request` object like in the example above, you can use a `gr.Header` argument. Gradio will automatically extract that header from the incoming request (if it exists) and pass it to your function.

In the example below, the `X-API-Token` header is extracted from the incoming request and passed in as the `x_api_token` argument to `make_api_request_on_behalf_of_user`.

The benefit of using `gr.Header` is that the MCP connection docs will automatically display the headers you need to supply when connecting to the server! See the image below:

```python
import gradio as gr

def make_api_request_on_behalf_of_user(prompt: str, x_api_token: gr.Header):
    """Make a request to everyone's favorite API.
    Args:
        prompt: The prompt to send to the API.
    Returns:
        The response from the API.
    Raises:
        AssertionError: If the API token is not valid.
    """
    return "Hello from the API" if not x_api_token else "Hello from the API with token!"


demo = gr.Interface(
    make_api_request_on_behalf_of_user,
    [
        gr.Textbox(label="Prompt"),
    ],
    gr.Textbox(label="Response"),
)

demo.launch(mcp_server=True)
```

![MCP Header Connection Page](https://github.com/user-attachments/assets/e264eedf-a91a-476b-880d-5be0d5934134)

### Sending Progress Updates

The Gradio MCP server automatically sends progress updates to your MCP Client based on the queue in the Gradio application. If you'd like to send custom progress updates, you can do so using the same mechanism as you would use to display progress updates in the UI of your Gradio app: by using the `gr.Progress` class!

Here's an example of how to do this:

$code_mcp_progress

[Here are the docs](https://www.gradio.app/docs/gradio/progress) for the `gr.Progress` class, which can also automatically track `tqdm` calls.

Note: by default, progress notifications are enabled for all MCP tools, even if the corresponding Gradio functions do not include a `gr.Progress`. However, this can add some overhead to the MCP tool (typically ~500ms). To disable progress notification, you can set `queue=False` in your Gradio event handler to skip the overhead related to subscribing to the queue's progress updates.


## Modifying Tool Descriptions

Gradio automatically sets the tool name based on the name of your function, and the description from the docstring of your function. But you may want to change how the description appears to your LLM. You can do this by using the `api_description` parameter in `Interface`, `ChatInterface`, or any event listener. This parameter takes three different kinds of values:

* `None` (default): the tool description is automatically created from the docstring of the function (or its parent's docstring if it does not have a docstring but inherits from a method that does.)
* `False`: no tool description appears to the LLM.
* `str`: an arbitrary string to use as the tool description.

In addition to modifying the tool descriptions, you can also toggle which tools appear to the LLM. You can do this by setting the `show_api` parameter, which is by default `True`. Setting it to `False` hides the endpoint from the API docs and from the MCP server. If you expose multiple tools, users of your app will also be able to toggle which tools they'd like to add to their MCP server by checking boxes in the "view MCP or API" panel.

Here's an example that shows the `api_description` and `show_api` parameters in actions:

$code_mcp_tools



## MCP Resources and Prompts

In addition to tools (which execute functions generally and are the default for any function exposed through the Gradio MCP integration), MCP supports two other important primitives: **resources** (for exposing data) and **prompts** (for defining reusable templates). Gradio provides decorators to easily create MCP servers with all three capabilities.


### Creating MCP Resources

Use the `@gr.mcp.resource` decorator on any function to expose data through your Gradio app. Resources can be static (always available at a fixed URI) or templated (with parameters in the URI).

$code_mcp_resources_and_prompts

In this example:
- The `get_greeting` function is exposed as a resource with a URI template `greeting://{name}`
- When an MCP client requests `greeting://Alice`, it receives "Hello, Alice!"
- Resources can also return images and other types of files or binary data. In order to return non-text data, you should specify the `mime_type` parameter in `@gr.mcp.resource()` and return a Base64 string from your function.

### Creating MCP Prompts  

Prompts help standardize how users interact with your tools. They're especially useful for complex workflows that require specific formatting or multiple steps.

The `greet_user` function in the example above is decorated with `@gr.mcp.prompt()`, which:
- Makes it available as a prompt template in MCP clients
- Accepts parameters (`name` and `style`) to customize the output
- Returns a structured prompt that guides the LLM's behavior


## Adding MCP-Only Functions

So far, all of our MCP tools, resources, or prompts have corresponded to event listeners in the UI. This works well for functions that directly update the UI, but may not work if you wish to expose a "pure logic" function that should return raw data (e.g. a JSON object) without directly causing a UI update.

In order to expose such an MCP tool, you can create a pure Gradio API endpoint using `gr.api` (see [full docs here](https://www.gradio.app/main/docs/gradio/api)). Here's an example of creating an MCP tool that slices a list:

$code_mcp_tool_only

Note that if you use this approach, your function signature must be fully typed, including the return value, as these signature are used to determine the typing information for the MCP tool.

## Gradio with FastMCP

In some cases, you may decide not to use Gradio's built-in integration and instead manually create an FastMCP Server that calls a Gradio app. This approach is useful when you want to:

- Store state / identify users between calls instead of treating every tool call completely independently
- Start the Gradio app MCP server when a tool is called (if you are running multiple Gradio apps locally and want to save memory / GPU)

This is very doable thanks to the [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) and the [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)'s `FastMCP` class. Here's an example of creating a custom MCP server that connects to various Gradio apps hosted on [HuggingFace Spaces](https://huggingface.co/spaces) using the `stdio` protocol:

```python
from mcp.server.fastmcp import FastMCP
from gradio_client import Client
import sys
import io
import json 

mcp = FastMCP("gradio-spaces")

clients = {}

def get_client(space_id: str) -> Client:
    """Get or create a Gradio client for the specified space."""
    if space_id not in clients:
        clients[space_id] = Client(space_id)
    return clients[space_id]


@mcp.tool()
async def generate_image(prompt: str, space_id: str = "ysharma/SanaSprint") -> str:
    """Generate an image using Flux.
    
    Args:
        prompt: Text prompt describing the image to generate
        space_id: HuggingFace Space ID to use 
    """
    client = get_client(space_id)
    result = client.predict(
            prompt=prompt,
            model_size="1.6B",
            seed=0,
            randomize_seed=True,
            width=1024,
            height=1024,
            guidance_scale=4.5,
            num_inference_steps=2,
            api_name="/infer"
    )
    return result


@mcp.tool()
async def run_dia_tts(prompt: str, space_id: str = "ysharma/Dia-1.6B") -> str:
    """Text-to-Speech Synthesis.
    
    Args:
        prompt: Text prompt describing the conversation between speakers S1, S2
        space_id: HuggingFace Space ID to use 
    """
    client = get_client(space_id)
    result = client.predict(
            text_input=f"""{prompt}""",
            audio_prompt_input=None, 
            max_new_tokens=3072,
            cfg_scale=3,
            temperature=1.3,
            top_p=0.95,
            cfg_filter_top_k=30,
            speed_factor=0.94,
            api_name="/generate_audio"
    )
    return result


if __name__ == "__main__":
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    
    mcp.run(transport='stdio')
```

This server exposes two tools:
1. `run_dia_tts` - Generates a conversation for the given transcript in the form of `[S1]first-sentence. [S2]second-sentence. [S1]...`
2. `generate_image` - Generates images using a fast text-to-image model

To use this MCP Server with Claude Desktop (as MCP Client):

1. Save the code to a file (e.g., `gradio_mcp_server.py`)
2. Install the required dependencies: `pip install mcp gradio-client`
3. Configure Claude Desktop to use your server by editing the configuration file at `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS) or `%APPDATA%\Claude\claude_desktop_config.json` (Windows):

```json
{
    "mcpServers": {
        "gradio-spaces": {
            "command": "python",
            "args": [
                "/absolute/path/to/gradio_mcp_server.py"
            ]
        }
    }
}
```

4. Restart Claude Desktop

Now, when you ask Claude about generating an image or transcribing audio, it can use your Gradio-powered tools to accomplish these tasks.


## Troubleshooting your MCP Servers

The MCP protocol is still in its infancy and you might see issues connecting to an MCP Server that you've built. We generally recommend using the [MCP Inspector Tool](https://github.com/modelcontextprotocol/inspector) to try connecting and debugging your MCP Server.

Here are some things that may help:

**1. Ensure that you've provided type hints and valid docstrings for your functions**

As mentioned earlier, Gradio reads the docstrings for your functions and the type hints of input arguments to generate the description of the tool and parameters. A valid function and docstring looks like this (note the "Args:" block with indented parameter names underneath):

```py
def image_orientation(image: Image.Image) -> str:
    """
    Returns whether image is portrait or landscape.

    Args:
        image (Image.Image): The image to check.
    """
    return "Portrait" if image.height > image.width else "Landscape"
```

Note: You can preview the schema that is created for your MCP server by visiting the `http://your-server:port/gradio_api/mcp/schema` URL.

**2. Try accepting input arguments as `str`**

Some MCP Clients do not recognize parameters that are numeric or other complex types, but all of the MCP Clients that we've tested accept `str` input parameters. When in doubt, change your input parameter to be a `str` and then cast to a specific type in the function, as in this example:

```py
def prime_factors(n: str):
    """
    Compute the prime factorization of a positive integer.

    Args:
        n (str): The integer to factorize. Must be greater than 1.
    """
    n_int = int(n)
    if n_int <= 1:
        raise ValueError("Input must be an integer greater than 1.")

    factors = []
    while n_int % 2 == 0:
        factors.append(2)
        n_int //= 2

    divisor = 3
    while divisor * divisor <= n_int:
        while n_int % divisor == 0:
            factors.append(divisor)
            n_int //= divisor
        divisor += 2

    if n_int > 1:
        factors.append(n_int)

    return factors
```

**3. Ensure that your MCP Client Supports Streamable HTTP**

Some MCP Clients do not yet support streamable HTTP-based MCP Servers. In those cases, you can use a tool such as [mcp-remote](https://github.com/geelen/mcp-remote). First install [Node.js](https://nodejs.org/en/download/). Then, add the following to your own MCP Client config:

```
{
  "mcpServers": {
    "gradio": {
      "command": "npx",
      "args": [
        "mcp-remote",
        "http://your-server:port/gradio_api/mcp/"
      ]
    }
  }
}
```

**4. Restart your MCP Client and MCP Server**

Some MCP Clients require you to restart them every time you update the MCP configuration. Other times, if the connection between the MCP Client and servers breaks, you might need to restart the MCP server. If all else fails, try restarting both your MCP Client and MCP Servers!

---

<!-- Source: guides/11_other-tutorials/01_using-hugging-face-integrations.md -->
# Using Hugging Face Integrations

Related spaces: https://huggingface.co/spaces/gradio/en2es
Tags: HUB, SPACES, EMBED

Contributed by <a href="https://huggingface.co/osanseviero">Omar Sanseviero</a> ðŸ¦™

## Introduction

The Hugging Face Hub is a central platform that has hundreds of thousands of [models](https://huggingface.co/models), [datasets](https://huggingface.co/datasets) and [demos](https://huggingface.co/spaces) (also known as Spaces). 

Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference Endpoints

Hugging Face has a service called [Serverless Inference Endpoints](https://huggingface.co/docs/api-inference/index), which allows you to send HTTP requests to models on the Hub. The API includes a generous free tier, and you can switch to [dedicated Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated) when you want to use it in production. Gradio integrates directly with Serverless Inference Endpoints so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:

```python
import gradio as gr

demo = gr.load("Helsinki-NLP/opus-mt-en-es", src="models")

demo.launch()
```

For any Hugging Face model supported in Inference Endpoints, Gradio automatically infers the expected input and output and make the underlying server calls, so you don't have to worry about defining the prediction function. 

Notice that we just put specify the model name and state that the `src` should be `models` (Hugging Face's Model Hub). There is no need to install any dependencies (except `gradio`) since you are not loading the model on your computer.

You might notice that the first inference takes a little bit longer. This happens since the Inference Endpoints is loading the model in the server. You get some benefits afterward:

- The inference will be much faster.
- The server caches your requests.
- You get built-in automatic scaling.

## Hosting your Gradio demos on Spaces

[Hugging Face Spaces](https://hf.co/spaces) allows anyone to host their Gradio demos freely, and uploading your Gradio demos take a couple of minutes. You can head to [hf.co/new-space](https://huggingface.co/new-space), select the Gradio SDK, create an `app.py` file, and voila! You have a demo you can share with anyone else. To learn more, read [this guide how to host on Hugging Face Spaces using the website](https://huggingface.co/blog/gradio-spaces).

Alternatively, you can create a Space programmatically, making use of the [huggingface_hub client library](https://huggingface.co/docs/huggingface_hub/index) library. Here's an example:

```python
from huggingface_hub import (
    create_repo,
    get_full_repo_name,
    upload_file,
)
create_repo(name=target_space_name, token=hf_token, repo_type="space", space_sdk="gradio")
repo_name = get_full_repo_name(model_id=target_space_name, token=hf_token)
file_url = upload_file(
    path_or_fileobj="file.txt",
    path_in_repo="app.py",
    repo_id=repo_name,
    repo_type="space",
    token=hf_token,
)
```

Here, `create_repo` creates a gradio repo with the target name under a specific account using that account's Write Token. `repo_name` gets the full repo name of the related repo. Finally `upload_file` uploads a file inside the repo with the name `app.py`.


## Loading demos from Spaces

You can also use and remix existing Gradio demos on Hugging Face Spaces. For example, you could take two existing Gradio demos on Spaces and put them as separate tabs and create a new demo. You can run this new demo locally, or upload it to Spaces, allowing endless possibilities to remix and create new demos!

Here's an example that does exactly that:

```python
import gradio as gr

with gr.Blocks() as demo:
  with gr.Tab("Translate to Spanish"):
    gr.load("gradio/en2es", src="spaces")
  with gr.Tab("Translate to French"):
    gr.load("abidlabs/en2fr", src="spaces")

demo.launch()
```

Notice that we use `gr.load()`, the same method we used to load models using Inference Endpoints. However, here we specify that the `src` is `spaces` (Hugging Face Spaces). 

Note: loading a Space in this way may result in slight differences from the original Space. In particular, any attributes that apply to the entire Blocks, such as the theme or custom CSS/JS, will not be loaded. You can copy these properties from the Space you are loading into your own `Blocks` object. 

## Demos with the `Pipeline` in `transformers`

Hugging Face's popular `transformers` library has a very easy-to-use abstraction, [`pipeline()`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.pipeline) that handles most of the complex code to offer a simple API for common tasks. By specifying the task and an (optional) model, you can build a demo around an existing model with few lines of Python:

```python
import gradio as gr

from transformers import pipeline

pipe = pipeline("translation", model="Helsinki-NLP/opus-mt-en-es")

def predict(text):
  return pipe(text)[0]["translation_text"]

demo = gr.Interface(
  fn=predict,
  inputs='text',
  outputs='text',
)

demo.launch()
```

But `gradio` actually makes it even easier to convert a `pipeline` to a demo, simply by using the `gradio.Interface.from_pipeline` methods, which skips the need to specify the input and output components:

```python
from transformers import pipeline
import gradio as gr

pipe = pipeline("translation", model="Helsinki-NLP/opus-mt-en-es")

demo = gr.Interface.from_pipeline(pipe)
demo.launch()
```

The previous code produces the following interface, which you can try right here in your browser:

<gradio-app space="gradio/en2es"></gradio-app>


## Recap

That's it! Let's recap the various ways Gradio and Hugging Face work together:

1. You can build a demo around Inference Endpoints without having to load the model, by using `gr.load()`.
2. You host your Gradio demo on Hugging Face Spaces, either using the GUI or entirely in Python.
3. You can load demos from Hugging Face Spaces to remix and create new Gradio demos using `gr.load()`.
4. You can convert a `transformers` pipeline into a Gradio demo using `from_pipeline()`.

ðŸ¤—

---

<!-- Source: guides/cn/01_getting-started/01_quickstart.md -->
# å¿«é€Ÿå¼€å§‹

**å…ˆå†³æ¡ä»¶**ï¼šGradio éœ€è¦ Python 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œå°±æ˜¯è¿™æ ·ï¼

## Gradio æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ

ä¸Žä»–äººåˆ†äº«æ‚¨çš„æœºå™¨å­¦ä¹ æ¨¡åž‹ã€API æˆ–æ•°æ®ç§‘å­¦æµç¨‹çš„*æœ€ä½³æ–¹å¼ä¹‹ä¸€*æ˜¯åˆ›å»ºä¸€ä¸ª**äº¤äº’å¼åº”ç”¨ç¨‹åº**ï¼Œè®©æ‚¨çš„ç”¨æˆ·æˆ–åŒäº‹å¯ä»¥åœ¨ä»–ä»¬çš„æµè§ˆå™¨ä¸­å°è¯•æ¼”ç¤ºã€‚

Gradio å…è®¸æ‚¨**ä½¿ç”¨ Python æž„å»ºæ¼”ç¤ºå¹¶å…±äº«è¿™äº›æ¼”ç¤º**ã€‚é€šå¸¸åªéœ€å‡ è¡Œä»£ç ï¼é‚£ä¹ˆæˆ‘ä»¬å¼€å§‹å§ã€‚

## Hello, World

è¦é€šè¿‡ä¸€ä¸ªç®€å•çš„â€œHello, Worldâ€ç¤ºä¾‹è¿è¡Œ Gradioï¼Œè¯·éµå¾ªä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. ä½¿ç”¨ pip å®‰è£… Gradioï¼š

```bash
pip install gradio
```

2. å°†ä¸‹é¢çš„ä»£ç ä½œä¸º Python è„šæœ¬è¿è¡Œæˆ–åœ¨ Jupyter Notebook ä¸­è¿è¡Œï¼ˆæˆ–è€… [Google Colab](https://colab.research.google.com/drive/18ODkJvyxHutTN0P5APWyGFO_xwNcgHDZ?usp=sharing)ï¼‰ï¼š

$code_hello_world

æˆ‘ä»¬å°†å¯¼å…¥çš„åç§°ç¼©çŸ­ä¸º `gr`ï¼Œä»¥ä¾¿ä»¥åŽåœ¨ä½¿ç”¨ Gradio çš„ä»£ç ä¸­æ›´å®¹æ˜“ç†è§£ã€‚è¿™æ˜¯ä¸€ç§å¹¿æ³›é‡‡ç”¨çš„çº¦å®šï¼Œæ‚¨åº”è¯¥éµå¾ªï¼Œä»¥ä¾¿ä¸Žæ‚¨çš„ä»£ç ä¸€èµ·å·¥ä½œçš„ä»»ä½•äººéƒ½å¯ä»¥è½»æ¾ç†è§£ã€‚

3. åœ¨ Jupyter Notebook ä¸­ï¼Œè¯¥æ¼”ç¤ºå°†è‡ªåŠ¨æ˜¾ç¤ºï¼›å¦‚æžœä»Žè„šæœ¬è¿è¡Œï¼Œåˆ™ä¼šåœ¨æµè§ˆå™¨ä¸­å¼¹å‡ºï¼Œç½‘å€ä¸º [http://localhost:7860](http://localhost:7860)ï¼š

$demo_hello_world

åœ¨æœ¬åœ°å¼€å‘æ—¶ï¼Œå¦‚æžœæ‚¨æƒ³å°†ä»£ç ä½œä¸º Python è„šæœ¬è¿è¡Œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Gradio CLI ä»¥**é‡è½½æ¨¡å¼**å¯åŠ¨åº”ç”¨ç¨‹åºï¼Œè¿™å°†æä¾›æ— ç¼å’Œå¿«é€Ÿçš„å¼€å‘ã€‚äº†è§£æœ‰å…³[è‡ªåŠ¨é‡è½½æŒ‡å—](https://gradio.app/developing-faster-with-reload-mode/)ä¸­é‡æ–°åŠ è½½çš„æ›´å¤šä¿¡æ¯ã€‚

```bash
gradio app.py
```

æ³¨æ„ï¼šæ‚¨ä¹Ÿå¯ä»¥è¿è¡Œ `python app.py`ï¼Œä½†å®ƒä¸ä¼šæä¾›è‡ªåŠ¨é‡æ–°åŠ è½½æœºåˆ¶ã€‚

## `Interface` ç±»

æ‚¨ä¼šæ³¨æ„åˆ°ä¸ºäº†åˆ›å»ºæ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª `gr.Interface`ã€‚`Interface` ç±»å¯ä»¥å°†ä»»ä½• Python å‡½æ•°ä¸Žç”¨æˆ·ç•Œé¢é…å¯¹ã€‚åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€ä¸ªç®€å•çš„åŸºäºŽæ–‡æœ¬çš„å‡½æ•°ï¼Œä½†è¯¥å‡½æ•°å¯ä»¥æ˜¯ä»»ä½•å†…å®¹ï¼Œä»ŽéŸ³ä¹ç”Ÿæˆå™¨åˆ°ç¨Žæ¬¾è®¡ç®—å™¨å†åˆ°é¢„è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡åž‹çš„é¢„æµ‹å‡½æ•°ã€‚

`Interface` ç±»çš„æ ¸å¿ƒæ˜¯ä½¿ç”¨ä¸‰ä¸ªå¿…éœ€å‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼š

- `fn`ï¼šè¦åœ¨å…¶å‘¨å›´åŒ…è£… UI çš„å‡½æ•°
- `inputs`ï¼šç”¨äºŽè¾“å…¥çš„ç»„ä»¶ï¼ˆä¾‹å¦‚ `"text"`ã€`"image"` æˆ– `"audio"`ï¼‰
- `outputs`ï¼šç”¨äºŽè¾“å‡ºçš„ç»„ä»¶ï¼ˆä¾‹å¦‚ `"text"`ã€`"image"` æˆ– `"label"`ï¼‰

è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°äº†è§£ç”¨äºŽæä¾›è¾“å…¥å’Œè¾“å‡ºçš„ç»„ä»¶ã€‚

## ç»„ä»¶å±žæ€§ (Components Attributes)

æˆ‘ä»¬åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­çœ‹åˆ°äº†ä¸€äº›ç®€å•çš„ `Textbox` ç»„ä»¶ï¼Œä½†æ˜¯å¦‚æžœæ‚¨æƒ³æ›´æ”¹ UI ç»„ä»¶çš„å¤–è§‚æˆ–è¡Œä¸ºæ€Žä¹ˆåŠžï¼Ÿ

å‡è®¾æ‚¨æƒ³è‡ªå®šä¹‰è¾“å…¥æ–‡æœ¬å­—æ®µ - ä¾‹å¦‚ï¼Œæ‚¨å¸Œæœ›å®ƒæ›´å¤§å¹¶å…·æœ‰æ–‡æœ¬å ä½ç¬¦ã€‚å¦‚æžœæˆ‘ä»¬ä½¿ç”¨å®žé™…çš„ `Textbox` ç±»è€Œä¸æ˜¯ä½¿ç”¨å­—ç¬¦ä¸²å¿«æ·æ–¹å¼ï¼Œæ‚¨å¯ä»¥é€šè¿‡ç»„ä»¶å±žæ€§èŽ·å¾—æ›´å¤šçš„è‡ªå®šä¹‰åŠŸèƒ½ã€‚

$code_hello_world_2
$demo_hello_world_2

## å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºç»„ä»¶

å‡è®¾æ‚¨æœ‰ä¸€ä¸ªæ›´å¤æ‚çš„å‡½æ•°ï¼Œå…·æœ‰å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæŽ¥å—å­—ç¬¦ä¸²ã€å¸ƒå°”å€¼å’Œæ•°å­—ï¼Œå¹¶è¿”å›žå­—ç¬¦ä¸²å’Œæ•°å­—çš„å‡½æ•°ã€‚è¯·çœ‹ä¸€ä¸‹å¦‚ä½•ä¼ é€’è¾“å…¥å’Œè¾“å‡ºç»„ä»¶çš„åˆ—è¡¨ã€‚

$code_hello_world_3
$demo_hello_world_3

åªéœ€å°†ç»„ä»¶åŒ…è£…åœ¨åˆ—è¡¨ä¸­ã€‚`inputs` åˆ—è¡¨ä¸­çš„æ¯ä¸ªç»„ä»¶å¯¹åº”å‡½æ•°çš„ä¸€ä¸ªå‚æ•°ï¼Œé¡ºåºç›¸åŒã€‚`outputs` åˆ—è¡¨ä¸­çš„æ¯ä¸ªç»„ä»¶å¯¹åº”å‡½æ•°è¿”å›žçš„ä¸€ä¸ªå€¼ï¼ŒåŒæ ·æ˜¯é¡ºåºã€‚

## å›¾åƒç¤ºä¾‹

Gradio æ”¯æŒè®¸å¤šç±»åž‹çš„ç»„ä»¶ï¼Œä¾‹å¦‚ `Image`ã€`DataFrame`ã€`Video` æˆ– `Label`ã€‚è®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªå›¾åƒåˆ°å›¾åƒçš„å‡½æ•°ï¼Œä»¥äº†è§£è¿™äº›ç»„ä»¶çš„æ„Ÿè§‰ï¼

$code_sepia_filter
$demo_sepia_filter

ä½¿ç”¨ `Image` ç»„ä»¶ä½œä¸ºè¾“å…¥æ—¶ï¼Œæ‚¨çš„å‡½æ•°å°†æŽ¥æ”¶åˆ°ä¸€ä¸ªå½¢çŠ¶ä¸º`ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œ3ï¼‰` çš„ NumPy æ•°ç»„ï¼Œå…¶ä¸­æœ€åŽä¸€ä¸ªç»´åº¦è¡¨ç¤º RGB å€¼ã€‚æˆ‘ä»¬è¿˜å°†è¿”å›žä¸€ä¸ªå›¾åƒï¼Œå½¢å¼ä¸º NumPy æ•°ç»„ã€‚

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ `type=` å…³é”®å­—å‚æ•°è®¾ç½®ç»„ä»¶ä½¿ç”¨çš„æ•°æ®ç±»åž‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨å¸Œæœ›å‡½æ•°æŽ¥å—å›¾åƒæ–‡ä»¶è·¯å¾„è€Œä¸æ˜¯ NumPy æ•°ç»„ï¼Œè¾“å…¥ `Image` ç»„ä»¶å¯ä»¥å†™æˆï¼š

```python
gr.Image(type="filepath")
```

è¿˜è¦æ³¨æ„ï¼Œæˆ‘ä»¬çš„è¾“å…¥ `Image` ç»„ä»¶é™„å¸¦æœ‰ä¸€ä¸ªç¼–è¾‘æŒ‰é’®ðŸ–‰ï¼Œå…è®¸è£å‰ªå’Œç¼©æ”¾å›¾åƒã€‚é€šè¿‡è¿™ç§æ–¹å¼æ“ä½œå›¾åƒå¯ä»¥å¸®åŠ©æ­ç¤ºæœºå™¨å­¦ä¹ æ¨¡åž‹ä¸­çš„åè§æˆ–éšè—çš„ç¼ºé™·ï¼

æ‚¨å¯ä»¥åœ¨[Gradio æ–‡æ¡£](https://gradio.app/docs)ä¸­é˜…è¯»æœ‰å…³è®¸å¤šç»„ä»¶ä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒä»¬çš„æ›´å¤šä¿¡æ¯ã€‚

## Blocksï¼šæ›´çµæ´»å’Œå¯æŽ§

Gradio æä¾›äº†ä¸¤ä¸ªç±»æ¥æž„å»ºåº”ç”¨ç¨‹åºï¼š

1. **Interface**ï¼Œæä¾›äº†ç”¨äºŽåˆ›å»ºæ¼”ç¤ºçš„é«˜çº§æŠ½è±¡ï¼Œæˆ‘ä»¬åˆ°ç›®å‰ä¸ºæ­¢ä¸€ç›´åœ¨è®¨è®ºã€‚

2. **Blocks**ï¼Œç”¨äºŽä»¥æ›´çµæ´»çš„å¸ƒå±€å’Œæ•°æ®æµè®¾è®¡ Web åº”ç”¨ç¨‹åºçš„ä½Žçº§ APIã€‚Blocks å…è®¸æ‚¨æ‰§è¡Œè¯¸å¦‚ç‰¹æ€§å¤šä¸ªæ•°æ®æµå’Œæ¼”ç¤ºï¼ŒæŽ§åˆ¶ç»„ä»¶åœ¨é¡µé¢ä¸Šçš„å‡ºçŽ°ä½ç½®ï¼Œå¤„ç†å¤æ‚çš„æ•°æ®æµï¼ˆä¾‹å¦‚ï¼Œè¾“å‡ºå¯ä»¥ä½œä¸ºå…¶ä»–å‡½æ•°çš„è¾“å…¥ï¼‰ï¼Œå¹¶åŸºäºŽç”¨æˆ·äº¤äº’æ›´æ–°ç»„ä»¶çš„å±žæ€§ / å¯è§æ€§ç­‰æ“ä½œ - ä»ç„¶å…¨éƒ¨ä½¿ç”¨ Pythonã€‚å¦‚æžœæ‚¨éœ€è¦è¿™ç§å¯å®šåˆ¶æ€§ï¼Œè¯·å°è¯•ä½¿ç”¨ `Blocks`ï¼

## Hello, Blocks

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ã€‚è¯·æ³¨æ„ï¼Œæ­¤å¤„çš„ API ä¸Ž `Interface` ä¸åŒã€‚

$code_hello_blocks
$demo_hello_blocks

éœ€è¦æ³¨æ„çš„äº‹é¡¹ï¼š

- `Blocks` å¯ä»¥ä½¿ç”¨ `with` å­å¥åˆ›å»ºï¼Œæ­¤å­å¥ä¸­åˆ›å»ºçš„ä»»ä½•ç»„ä»¶éƒ½ä¼šè‡ªåŠ¨æ·»åŠ åˆ°åº”ç”¨ç¨‹åºä¸­ã€‚
- ç»„ä»¶ä»¥æŒ‰åˆ›å»ºé¡ºåºåž‚ç›´æ”¾ç½®åœ¨åº”ç”¨ç¨‹åºä¸­ã€‚ï¼ˆç¨åŽæˆ‘ä»¬å°†ä»‹ç»è‡ªå®šä¹‰å¸ƒå±€ï¼ï¼‰
- åˆ›å»ºäº†ä¸€ä¸ª `Button`ï¼Œç„¶åŽåœ¨æ­¤æŒ‰é’®ä¸Šæ·»åŠ äº†ä¸€ä¸ª `click` äº‹ä»¶ç›‘å¬å™¨ã€‚å¯¹äºŽè¿™ä¸ª APIï¼Œåº”è¯¥å¾ˆç†Ÿæ‚‰ï¼ä¸Ž `Interface` ç±»ä¼¼ï¼Œ`click` æ–¹æ³•æŽ¥å—ä¸€ä¸ª Python å‡½æ•°ã€è¾“å…¥ç»„ä»¶å’Œè¾“å‡ºç»„ä»¶ã€‚

## æ›´å¤æ‚çš„åº”ç”¨

ä¸‹é¢æ˜¯ä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œä»¥è®©æ‚¨å¯¹ `Blocks` å¯ä»¥å®žçŽ°çš„æ›´å¤šå†…å®¹æœ‰æ‰€äº†è§£ï¼š

$code_blocks_flipper
$demo_blocks_flipper

è¿™é‡Œæœ‰æ›´å¤šçš„ä¸œè¥¿ï¼åœ¨[building with blocks](https://gradio.app/building_with_blocks)éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•åˆ›å»ºåƒè¿™æ ·çš„å¤æ‚çš„ `Blocks` åº”ç”¨ç¨‹åºã€‚

æ­å–œï¼Œæ‚¨å·²ç»ç†Ÿæ‚‰äº† Gradio çš„åŸºç¡€çŸ¥è¯†ï¼ ðŸ¥³ è½¬åˆ°æˆ‘ä»¬çš„[ä¸‹ä¸€ä¸ªæŒ‡å—](https://gradio.app/key_features)äº†è§£æ›´å¤šå…³äºŽ Gradio çš„ä¸»è¦åŠŸèƒ½ã€‚

---

<!-- Source: guides/cn/02_building-interfaces/01_interface-state.md -->
# æŽ¥å£çŠ¶æ€ (Interface State)

æœ¬æŒ‡å—ä»‹ç»äº† Gradio ä¸­å¦‚ä½•å¤„ç†çŠ¶æ€ã€‚äº†è§£å…¨å±€çŠ¶æ€å’Œä¼šè¯çŠ¶æ€çš„åŒºåˆ«ï¼Œä»¥åŠå¦‚ä½•åŒæ—¶ä½¿ç”¨å®ƒä»¬ã€‚

## å…¨å±€çŠ¶æ€ (Global State)

æ‚¨çš„å‡½æ•°å¯èƒ½ä½¿ç”¨è¶…å‡ºå•ä¸ªå‡½æ•°è°ƒç”¨çš„æŒä¹…æ€§æ•°æ®ã€‚å¦‚æžœæ•°æ®æ˜¯æ‰€æœ‰å‡½æ•°è°ƒç”¨å’Œæ‰€æœ‰ç”¨æˆ·éƒ½å¯è®¿é—®çš„å†…å®¹ï¼Œæ‚¨å¯ä»¥åœ¨å‡½æ•°è°ƒç”¨å¤–éƒ¨åˆ›å»ºä¸€ä¸ªå˜é‡ï¼Œå¹¶åœ¨å‡½æ•°å†…éƒ¨è®¿é—®å®ƒã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½ä¼šåœ¨å‡½æ•°å¤–éƒ¨åŠ è½½ä¸€ä¸ªå¤§æ¨¡åž‹ï¼Œå¹¶åœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨å®ƒï¼Œä»¥ä¾¿æ¯ä¸ªå‡½æ•°è°ƒç”¨éƒ½ä¸éœ€è¦é‡æ–°åŠ è½½æ¨¡åž‹ã€‚

$code_score_tracker

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œ'scores' æ•°ç»„åœ¨æ‰€æœ‰ç”¨æˆ·ä¹‹é—´å…±äº«ã€‚å¦‚æžœå¤šä¸ªç”¨æˆ·è®¿é—®æ­¤æ¼”ç¤ºï¼Œä»–ä»¬çš„å¾—åˆ†å°†å…¨éƒ¨æ·»åŠ åˆ°åŒä¸€åˆ—è¡¨ä¸­ï¼Œå¹¶ä¸”è¿”å›žçš„å‰ 3 ä¸ªå¾—åˆ†å°†ä»Žæ­¤å…±äº«å¼•ç”¨ä¸­æ”¶é›†ã€‚

## å…¨å±€çŠ¶æ€ (Global State)

Gradio æ”¯æŒçš„å¦ä¸€ç§æ•°æ®æŒä¹…æ€§æ˜¯ä¼šè¯çŠ¶æ€ï¼Œå…¶ä¸­æ•°æ®åœ¨é¡µé¢ä¼šè¯ä¸­çš„å¤šä¸ªæäº¤ä¹‹é—´æŒä¹…å­˜åœ¨ã€‚ä½†æ˜¯ï¼Œä¸åŒç”¨æˆ·ä¹‹é—´çš„æ•°æ®*ä¸*å…±äº«ã€‚è¦å°†æ•°æ®å­˜å‚¨åœ¨ä¼šè¯çŠ¶æ€ä¸­ï¼Œéœ€è¦æ‰§è¡Œä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. å°†é¢å¤–çš„å‚æ•°ä¼ é€’ç»™æ‚¨çš„å‡½æ•°ï¼Œè¡¨ç¤ºæŽ¥å£çš„çŠ¶æ€ã€‚
2. åœ¨å‡½æ•°çš„æœ«å°¾ï¼Œä½œä¸ºé¢å¤–çš„è¿”å›žå€¼è¿”å›žçŠ¶æ€çš„æ›´æ–°å€¼ã€‚
3. åœ¨åˆ›å»ºç•Œé¢æ—¶æ·»åŠ  `'state'` è¾“å…¥å’Œ `'state'` è¾“å‡ºç»„ä»¶ã€‚

èŠå¤©æœºå™¨äººå°±æ˜¯éœ€è¦ä¼šè¯çŠ¶æ€çš„ä¸€ä¸ªä¾‹å­ - æ‚¨å¸Œæœ›è®¿é—®ç”¨æˆ·ä¹‹å‰çš„æäº¤ï¼Œä½†ä¸èƒ½å°†èŠå¤©è®°å½•å­˜å‚¨åœ¨å…¨å±€å˜é‡ä¸­ï¼Œå› ä¸ºè¿™æ ·èŠå¤©è®°å½•ä¼šåœ¨ä¸åŒç”¨æˆ·ä¹‹é—´æ··ä¹±ã€‚

$code_chatbot_dialogpt
$demo_chatbot_dialogpt

è¯·æ³¨æ„ï¼Œåœ¨æ¯ä¸ªé¡µé¢ä¸­ï¼ŒçŠ¶æ€åœ¨æäº¤ä¹‹é—´ä¿æŒä¸å˜ï¼Œä½†æ˜¯å¦‚æžœåœ¨å¦ä¸€ä¸ªæ ‡ç­¾ä¸­åŠ è½½æ­¤æ¼”ç¤ºï¼ˆæˆ–åˆ·æ–°é¡µé¢ï¼‰ï¼Œæ¼”ç¤ºå°†ä¸å…±äº«èŠå¤©è®°å½•ã€‚

`state` çš„é»˜è®¤å€¼ä¸º Noneã€‚å¦‚æžœæ‚¨å°†é»˜è®¤å€¼ä¼ é€’ç»™å‡½æ•°çš„çŠ¶æ€å‚æ•°ï¼Œåˆ™è¯¥é»˜è®¤å€¼å°†ç”¨ä½œçŠ¶æ€çš„é»˜è®¤å€¼ã€‚`Interface` ç±»ä»…æ”¯æŒå•ä¸ªè¾“å…¥å’Œè¾“å‡ºçŠ¶æ€å˜é‡ï¼Œä½†å¯ä»¥æ˜¯å…·æœ‰å¤šä¸ªå…ƒç´ çš„åˆ—è¡¨ã€‚å¯¹äºŽæ›´å¤æ‚çš„ç”¨ä¾‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Blocksï¼Œ[å®ƒæ”¯æŒå¤šä¸ª `State` å˜é‡](/state_in_blocks/)ã€‚

---

<!-- Source: guides/cn/03_building-with-blocks/01_blocks-and-event-listeners.md -->
# åŒºå—å’Œäº‹ä»¶ç›‘å¬å™¨ (Blocks and Event Listeners)

æˆ‘ä»¬åœ¨[å¿«é€Ÿå…¥é—¨](https://gradio.app/blocks-and-event-listeners)ä¸­ç®€è¦ä»‹ç»äº†åŒºå—ã€‚è®©æˆ‘ä»¬æ·±å…¥æŽ¢è®¨ä¸€ä¸‹ã€‚æœ¬æŒ‡å—å°†æ¶µç›–åŒºå—çš„ç»“æž„ã€äº‹ä»¶ç›‘å¬å™¨åŠå…¶ç±»åž‹ã€è¿žç»­è¿è¡Œäº‹ä»¶ã€æ›´æ–°é…ç½®ä»¥åŠä½¿ç”¨å­—å…¸ä¸Žåˆ—è¡¨ã€‚

## åŒºå—ç»“æž„ (Blocks Structure)

è¯·æŸ¥çœ‹ä¸‹é¢çš„æ¼”ç¤ºã€‚

$code_hello_blocks
$demo_hello_blocks

- é¦–å…ˆï¼Œæ³¨æ„ `with gr.Blocks() as demo:` å­å¥ã€‚åŒºå—åº”ç”¨ç¨‹åºä»£ç å°†è¢«åŒ…å«åœ¨è¯¥å­å¥ä¸­ã€‚
- æŽ¥ä¸‹æ¥æ˜¯ç»„ä»¶ã€‚è¿™äº›ç»„ä»¶æ˜¯åœ¨ `Interface` ä¸­ä½¿ç”¨çš„ç›¸åŒç»„ä»¶ã€‚ä½†æ˜¯ï¼Œä¸Žå°†ç»„ä»¶ä¼ é€’ç»™æŸä¸ªæž„é€ å‡½æ•°ä¸åŒï¼Œç»„ä»¶åœ¨ `with` å­å¥å†…åˆ›å»ºæ—¶ä¼šè‡ªåŠ¨æ·»åŠ åˆ°åŒºå—ä¸­ã€‚
- æœ€åŽï¼Œ`click()` äº‹ä»¶ç›‘å¬å™¨ã€‚äº‹ä»¶ç›‘å¬å™¨å®šä¹‰äº†åº”ç”¨ç¨‹åºå†…çš„æ•°æ®æµã€‚åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œç›‘å¬å™¨å°†ä¸¤ä¸ªæ–‡æœ¬æ¡†ç›¸äº’å…³è”ã€‚æ–‡æœ¬æ¡† `name` ä½œä¸ºè¾“å…¥ï¼Œæ–‡æœ¬æ¡† `output` ä½œä¸º `greet` æ–¹æ³•çš„è¾“å‡ºã€‚å½“å•å‡»æŒ‰é’® `greet_btn` æ—¶è§¦å‘æ­¤æ•°æ®æµã€‚ä¸Žç•Œé¢ç±»ä¼¼ï¼Œäº‹ä»¶ç›‘å¬å™¨å¯ä»¥å…·æœ‰å¤šä¸ªè¾“å…¥æˆ–è¾“å‡ºã€‚

## äº‹ä»¶ç›‘å¬å™¨ä¸Žäº¤äº’æ€§ (Event Listeners and Interactivity)

åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°å¯ä»¥ç¼–è¾‘æ–‡æœ¬æ¡† `name`ï¼Œä½†æ— æ³•ç¼–è¾‘æ–‡æœ¬æ¡† `output`ã€‚è¿™æ˜¯å› ä¸ºä½œä¸ºäº‹ä»¶ç›‘å¬å™¨çš„ä»»ä½•ç»„ä»¶éƒ½å…·æœ‰äº¤äº’æ€§ã€‚ç„¶è€Œï¼Œç”±äºŽæ–‡æœ¬æ¡† `output` ä»…ä½œä¸ºè¾“å‡ºï¼Œå®ƒæ²¡æœ‰äº¤äº’æ€§ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `interactive=` å…³é”®å­—å‚æ•°ç›´æŽ¥é…ç½®ç»„ä»¶çš„äº¤äº’æ€§ã€‚

```python
output = gr.Textbox(label="è¾“å‡º", interactive=True)
```

## äº‹ä»¶ç›‘å¬å™¨çš„ç±»åž‹ (Types of Event Listeners)

è¯·æŸ¥çœ‹ä¸‹é¢çš„æ¼”ç¤ºï¼š

$code_blocks_hello
$demo_blocks_hello

`welcome` å‡½æ•°ä¸æ˜¯ç”±ç‚¹å‡»è§¦å‘çš„ï¼Œè€Œæ˜¯ç”±åœ¨æ–‡æœ¬æ¡† `inp` ä¸­è¾“å…¥æ–‡å­—è§¦å‘çš„ã€‚è¿™æ˜¯ç”±äºŽ `change()` äº‹ä»¶ç›‘å¬å™¨ã€‚ä¸åŒçš„ç»„ä»¶æ”¯æŒä¸åŒçš„äº‹ä»¶ç›‘å¬å™¨ã€‚ä¾‹å¦‚ï¼Œ`Video` ç»„ä»¶æ”¯æŒä¸€ä¸ª `play()` äº‹ä»¶ç›‘å¬å™¨ï¼Œå½“ç”¨æˆ·æŒ‰ä¸‹æ’­æ”¾æŒ‰é’®æ—¶è§¦å‘ã€‚æœ‰å…³æ¯ä¸ªç»„ä»¶çš„äº‹ä»¶ç›‘å¬å™¨ï¼Œè¯·å‚è§[æ–‡æ¡£](http://gradio.app/docs/components)ã€‚

## å¤šä¸ªæ•°æ®æµ (Multiple Data Flows)

åŒºå—åº”ç”¨ç¨‹åºä¸åƒç•Œé¢é‚£æ ·é™åˆ¶äºŽå•ä¸ªæ•°æ®æµã€‚è¯·æŸ¥çœ‹ä¸‹é¢çš„æ¼”ç¤ºï¼š

$code_reversible_flow
$demo_reversible_flow

è¯·æ³¨æ„ï¼Œ`num1` å¯ä»¥å……å½“ `num2` çš„è¾“å…¥ï¼Œåä¹‹äº¦ç„¶ï¼éšç€åº”ç”¨ç¨‹åºå˜å¾—æ›´åŠ å¤æ‚ï¼Œæ‚¨å°†èƒ½å¤Ÿè¿žæŽ¥å„ç§ç»„ä»¶çš„å¤šä¸ªæ•°æ®æµã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ª " å¤šæ­¥éª¤ " ç¤ºä¾‹ï¼Œå…¶ä¸­ä¸€ä¸ªæ¨¡åž‹çš„è¾“å‡ºï¼ˆè¯­éŸ³åˆ°æ–‡æœ¬æ¨¡åž‹ï¼‰è¢«ä¼ é€’ç»™ä¸‹ä¸€ä¸ªæ¨¡åž‹ï¼ˆæƒ…æ„Ÿåˆ†ç±»å™¨ï¼‰ã€‚

$code_blocks_speech_text_sentiment
$demo_blocks_speech_text_sentiment

## å‡½æ•°è¾“å…¥åˆ—è¡¨ä¸Žå­—å…¸ (Function Input List vs Dict)

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‚¨çœ‹åˆ°çš„äº‹ä»¶ç›‘å¬å™¨éƒ½åªæœ‰ä¸€ä¸ªè¾“å…¥ç»„ä»¶ã€‚å¦‚æžœæ‚¨å¸Œæœ›æœ‰å¤šä¸ªè¾“å…¥ç»„ä»¶å°†æ•°æ®ä¼ é€’ç»™å‡½æ•°ï¼Œæœ‰ä¸¤ç§é€‰é¡¹å¯ä¾›å‡½æ•°æŽ¥å—è¾“å…¥ç»„ä»¶å€¼ï¼š

1. ä½œä¸ºå‚æ•°åˆ—è¡¨ï¼Œæˆ–
2. ä½œä¸ºä»¥ç»„ä»¶ä¸ºé”®çš„å•ä¸ªå€¼å­—å…¸

è®©æˆ‘ä»¬åˆ†åˆ«çœ‹ä¸€ä¸ªä¾‹å­ï¼š
$code_calculator_list_and_dict

`add()` å’Œ `sub()` éƒ½å°† `a` å’Œ `b` ä½œä¸ºè¾“å…¥ã€‚ç„¶è€Œï¼Œè¿™äº›ç›‘å¬å™¨ä¹‹é—´çš„è¯­æ³•ä¸åŒã€‚

1. å¯¹äºŽ `add_btn` ç›‘å¬å™¨ï¼Œæˆ‘ä»¬å°†è¾“å…¥ä½œä¸ºåˆ—è¡¨ä¼ é€’ã€‚å‡½æ•° `add()` å°†æ¯ä¸ªè¾“å…¥ä½œä¸ºå‚æ•°ã€‚`a` çš„å€¼æ˜ å°„åˆ°å‚æ•° `num1`ï¼Œ`b` çš„å€¼æ˜ å°„åˆ°å‚æ•° `num2`ã€‚
2. å¯¹äºŽ `sub_btn` ç›‘å¬å™¨ï¼Œæˆ‘ä»¬å°†è¾“å…¥ä½œä¸ºé›†åˆä¼ é€’ï¼ˆæ³¨æ„èŠ±æ‹¬å·ï¼ï¼‰ã€‚å‡½æ•° `sub()` æŽ¥å—ä¸€ä¸ªåä¸º `data` çš„å•ä¸ªå­—å…¸å‚æ•°ï¼Œå…¶ä¸­é”®æ˜¯è¾“å…¥ç»„ä»¶ï¼Œå€¼æ˜¯è¿™äº›ç»„ä»¶çš„å€¼ã€‚

ä½¿ç”¨å“ªç§è¯­æ³•æ˜¯ä¸ªäººåå¥½ï¼å¯¹äºŽå…·æœ‰è®¸å¤šè¾“å…¥ç»„ä»¶çš„å‡½æ•°ï¼Œé€‰é¡¹ 2 å¯èƒ½æ›´å®¹æ˜“ç®¡ç†ã€‚

$demo_calculator_list_and_dict

## å‡½æ•°è¿”å›žåˆ—è¡¨ä¸Žå­—å…¸ (Function Return List vs Dict)

ç±»ä¼¼åœ°ï¼Œæ‚¨å¯ä»¥è¿”å›žå¤šä¸ªè¾“å‡ºç»„ä»¶çš„å€¼ï¼Œå¯ä»¥æ˜¯ï¼š

1. å€¼åˆ—è¡¨ï¼Œæˆ–
2. ä»¥ç»„ä»¶ä¸ºé”®çš„å­—å…¸

é¦–å…ˆè®©æˆ‘ä»¬çœ‹ä¸€ä¸ªï¼ˆ1ï¼‰çš„ç¤ºä¾‹ï¼Œå…¶ä¸­æˆ‘ä»¬é€šè¿‡è¿”å›žä¸¤ä¸ªå€¼æ¥è®¾ç½®ä¸¤ä¸ªè¾“å‡ºç»„ä»¶çš„å€¼ï¼š

```python
with gr.Blocks() as demo:
    food_box = gr.Number(value=10, label="Food Count")
    status_box = gr.Textbox()
    def eat(food):
        if food > 0:
            return food - 1, "full"
        else:
            return 0, "hungry"
    gr.Button("EAT").click(
        fn=eat,
        inputs=food_box,
        outputs=[food_box, status_box]
    )
```

ä¸Šé¢çš„æ¯ä¸ªè¿”å›žè¯­å¥åˆ†åˆ«è¿”å›žä¸Ž `food_box` å’Œ `status_box` ç›¸å¯¹åº”çš„ä¸¤ä¸ªå€¼ã€‚

é™¤äº†è¿”å›žä¸Žæ¯ä¸ªè¾“å‡ºç»„ä»¶é¡ºåºç›¸å¯¹åº”çš„å€¼åˆ—è¡¨å¤–ï¼Œæ‚¨è¿˜å¯ä»¥è¿”å›žä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­é”®å¯¹åº”äºŽè¾“å‡ºç»„ä»¶ï¼Œå€¼ä½œä¸ºæ–°å€¼ã€‚è¿™è¿˜å…è®¸æ‚¨è·³è¿‡æ›´æ–°æŸäº›è¾“å‡ºç»„ä»¶ã€‚

```python
with gr.Blocks() as demo:
    food_box = gr.Number(value=10, label="Food Count")
    status_box = gr.Textbox()
    def eat(food):
        if food > 0:
            return {food_box: food - 1, status_box: "full"}
        else:
            return {status_box: "hungry"}
    gr.Button("EAT").click(
        fn=eat,
        inputs=food_box,
        outputs=[food_box, status_box]
    )
```

æ³¨æ„ï¼Œåœ¨æ²¡æœ‰é£Ÿç‰©çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªæ›´æ–° `status_box` å…ƒç´ ã€‚æˆ‘ä»¬è·³è¿‡æ›´æ–° `food_box` ç»„ä»¶ã€‚

å­—å…¸è¿”å›žåœ¨äº‹ä»¶ç›‘å¬å™¨å½±å“å¤šä¸ªç»„ä»¶çš„è¿”å›žå€¼æˆ–æœ‰æ¡ä»¶åœ°å½±å“è¾“å‡ºæ—¶éžå¸¸æœ‰ç”¨ã€‚

è¯·è®°ä½ï¼Œå¯¹äºŽå­—å…¸è¿”å›žï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦åœ¨äº‹ä»¶ç›‘å¬å™¨ä¸­æŒ‡å®šå¯èƒ½çš„è¾“å‡ºç»„ä»¶ã€‚

## æ›´æ–°ç»„ä»¶é…ç½® (Updating Component Configurations)

äº‹ä»¶ç›‘å¬å™¨å‡½æ•°çš„è¿”å›žå€¼é€šå¸¸æ˜¯ç›¸åº”è¾“å‡ºç»„ä»¶çš„æ›´æ–°å€¼ã€‚æœ‰æ—¶æˆ‘ä»¬è¿˜å¸Œæœ›æ›´æ–°ç»„ä»¶çš„é…ç½®ï¼Œä¾‹å¦‚å¯è§æ€§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¿”å›žä¸€ä¸ª `gr.update()` å¯¹è±¡ï¼Œè€Œä¸ä»…ä»…æ˜¯æ›´æ–°ç»„ä»¶çš„å€¼ã€‚

$code_blocks_essay_simple
$demo_blocks_essay_simple

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ `gr.update()` æ–¹æ³•è‡ªæˆ‘é…ç½®æ–‡æœ¬æ¡†ã€‚`value=` å‚æ•°ä»ç„¶å¯ä»¥ç”¨äºŽæ›´æ–°å€¼ä»¥åŠç»„ä»¶é…ç½®ã€‚

## è¿žç»­è¿è¡Œäº‹ä»¶ (Running Events Consecutively)

ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨äº‹ä»¶ç›‘å¬å™¨çš„ `then` æ–¹æ³•æŒ‰é¡ºåºè¿è¡Œäº‹ä»¶ã€‚åœ¨å‰ä¸€ä¸ªäº‹ä»¶è¿è¡Œå®ŒæˆåŽï¼Œè¿™å°†è¿è¡Œä¸‹ä¸€ä¸ªäº‹ä»¶ã€‚è¿™å¯¹äºŽå¤šæ­¥æ›´æ–°ç»„ä»¶çš„äº‹ä»¶éžå¸¸æœ‰ç”¨ã€‚

ä¾‹å¦‚ï¼Œåœ¨ä¸‹é¢çš„èŠå¤©æœºå™¨äººç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆç«‹å³ä½¿ç”¨ç”¨æˆ·æ¶ˆæ¯æ›´æ–°èŠå¤©æœºå™¨äººï¼Œç„¶åŽåœ¨æ¨¡æ‹Ÿå»¶è¿ŸåŽä½¿ç”¨è®¡ç®—æœºå›žå¤æ›´æ–°èŠå¤©æœºå™¨äººã€‚

$code_chatbot_simple
$demo_chatbot_simple

äº‹ä»¶ç›‘å¬å™¨çš„ `.then()` æ–¹æ³•ä¼šæ‰§è¡ŒåŽç»­äº‹ä»¶ï¼Œæ— è®ºå‰ä¸€ä¸ªäº‹ä»¶æ˜¯å¦å¼•å‘ä»»ä½•é”™è¯¯ã€‚å¦‚æžœåªæƒ³åœ¨å‰ä¸€ä¸ªäº‹ä»¶æˆåŠŸæ‰§è¡ŒåŽæ‰è¿è¡ŒåŽç»­äº‹ä»¶ï¼Œè¯·ä½¿ç”¨ `.success()` æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸Ž `.then()` æŽ¥å—ç›¸åŒçš„å‚æ•°ã€‚

## è¿žç»­è¿è¡Œäº‹ä»¶ (Running Events Continuously)

æ‚¨å¯ä»¥ä½¿ç”¨äº‹ä»¶ç›‘å¬å™¨çš„ `every` å‚æ•°æŒ‰å›ºå®šè®¡åˆ’è¿è¡Œäº‹ä»¶ã€‚è¿™å°†åœ¨å®¢æˆ·ç«¯è¿žæŽ¥æ‰“å¼€çš„æƒ…å†µä¸‹ï¼Œæ¯éš”ä¸€å®šç§’æ•°è¿è¡Œä¸€æ¬¡äº‹ä»¶ã€‚å¦‚æžœè¿žæŽ¥å…³é—­ï¼Œäº‹ä»¶å°†åœ¨ä¸‹ä¸€æ¬¡è¿­ä»£åŽåœæ­¢è¿è¡Œã€‚
è¯·æ³¨æ„ï¼Œè¿™ä¸è€ƒè™‘äº‹ä»¶æœ¬èº«çš„è¿è¡Œæ—¶é—´ã€‚å› æ­¤ï¼Œä½¿ç”¨ `every=gr.Timer(5)` è¿è¡Œæ—¶é—´ä¸º 1 ç§’çš„å‡½æ•°å®žé™…ä¸Šæ¯ 6 ç§’è¿è¡Œä¸€æ¬¡ã€‚

ä»¥ä¸‹æ˜¯æ¯ç§’æ›´æ–°çš„æ­£å¼¦æ›²çº¿ç¤ºä¾‹ï¼

$code_sine_curve
$demo_sine_curve

## æ”¶é›†äº‹ä»¶æ•°æ® (Gathering Event Data)

æ‚¨å¯ä»¥é€šè¿‡å°†ç›¸å…³çš„äº‹ä»¶æ•°æ®ç±»ä½œä¸ºç±»åž‹æç¤ºæ·»åŠ åˆ°äº‹ä»¶ç›‘å¬å™¨å‡½æ•°çš„å‚æ•°ä¸­ï¼Œæ”¶é›†æœ‰å…³äº‹ä»¶çš„ç‰¹å®šæ•°æ®ã€‚

ä¾‹å¦‚ï¼Œä½¿ç”¨ `gradio.SelectData` å‚æ•°å¯ä»¥ä¸º `.select()` çš„äº‹ä»¶æ•°æ®æ·»åŠ ç±»åž‹æç¤ºã€‚å½“ç”¨æˆ·é€‰æ‹©è§¦å‘ç»„ä»¶çš„ä¸€éƒ¨åˆ†æ—¶ï¼Œå°†è§¦å‘æ­¤äº‹ä»¶ï¼Œå¹¶ä¸”äº‹ä»¶æ•°æ®åŒ…å«æœ‰å…³ç”¨æˆ·çš„å…·ä½“é€‰æ‹©çš„ä¿¡æ¯ã€‚å¦‚æžœç”¨æˆ·åœ¨ `Textbox` ä¸­é€‰æ‹©äº†ç‰¹å®šå•è¯ï¼Œåœ¨ `Gallery` ä¸­é€‰æ‹©äº†ç‰¹å®šå›¾åƒæˆ–åœ¨ `DataFrame` ä¸­é€‰æ‹©äº†ç‰¹å®šå•å…ƒæ ¼ï¼Œåˆ™äº‹ä»¶æ•°æ®å‚æ•°å°†åŒ…å«æœ‰å…³å…·ä½“é€‰æ‹©çš„ä¿¡æ¯ã€‚

åœ¨ä¸‹é¢çš„åŒäººäº•å­—æ¸¸æˆæ¼”ç¤ºä¸­ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹© `DataFrame` ä¸­çš„ä¸€ä¸ªå•å…ƒæ ¼è¿›è¡Œç§»åŠ¨ã€‚äº‹ä»¶æ•°æ®å‚æ•°åŒ…å«æœ‰å…³æ‰€é€‰å•å…ƒæ ¼çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥é¦–å…ˆæ£€æŸ¥å•å…ƒæ ¼æ˜¯å¦ä¸ºç©ºï¼Œç„¶åŽç”¨ç”¨æˆ·çš„ç§»åŠ¨æ›´æ–°å•å…ƒæ ¼ã€‚

$code_tictactoe

$demo_tictactoe

---

<!-- Source: guides/cn/04_integrating-other-frameworks/01_using-hugging-face-integrations.md -->
# ä½¿ç”¨ Hugging Face é›†æˆ

ç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/gradio/helsinki_translation_en_es
æ ‡ç­¾ï¼šHUBï¼ŒSPACESï¼ŒEMBED

ç”± <a href="https://huggingface.co/osanseviero">Omar Sanseviero</a> è´¡çŒ®ðŸ¦™

## ä»‹ç»

Hugging Face Hub æ˜¯ä¸€ä¸ªé›†æˆå¹³å°ï¼Œæ‹¥æœ‰è¶…è¿‡ 190,000 ä¸ª[æ¨¡åž‹](https://huggingface.co/models)ï¼Œ32,000 ä¸ª[æ•°æ®é›†](https://huggingface.co/datasets)å’Œ 40,000 ä¸ª[æ¼”ç¤º](https://huggingface.co/spaces)ï¼Œä¹Ÿè¢«ç§°ä¸º Spacesã€‚è™½ç„¶ Hugging Face ä»¥å…¶ðŸ¤— transformers å’Œ diffusers åº“è€Œé—»åï¼Œä½† Hub è¿˜æ”¯æŒè®¸å¤šæœºå™¨å­¦ä¹ åº“ï¼Œå¦‚ PyTorchï¼ŒTensorFlowï¼ŒspaCy ç­‰ï¼Œæ¶µç›–äº†ä»Žè®¡ç®—æœºè§†è§‰åˆ°å¼ºåŒ–å­¦ä¹ ç­‰å„ä¸ªé¢†åŸŸã€‚

Gradio æ‹¥æœ‰å¤šä¸ªåŠŸèƒ½ï¼Œä½¿å…¶éžå¸¸å®¹æ˜“åˆ©ç”¨ Hub ä¸Šçš„çŽ°æœ‰æ¨¡åž‹å’Œ Spacesã€‚æœ¬æŒ‡å—å°†ä»‹ç»è¿™äº›åŠŸèƒ½ã€‚

## ä½¿ç”¨ `pipeline` è¿›è¡Œå¸¸è§„æŽ¨ç†

é¦–å…ˆï¼Œè®©æˆ‘ä»¬æž„å»ºä¸€ä¸ªç®€å•çš„ç•Œé¢ï¼Œå°†è‹±æ–‡ç¿»è¯‘æˆè¥¿ç­ç‰™æ–‡ã€‚åœ¨èµ«å°”è¾›åŸºå¤§å­¦å…±äº«çš„ä¸€åƒå¤šä¸ªæ¨¡åž‹ä¸­ï¼Œæœ‰ä¸€ä¸ª[çŽ°æœ‰æ¨¡åž‹](https://huggingface.co/Helsinki-NLP/opus-mt-en-es)ï¼Œåä¸º `opus-mt-en-es`ï¼Œå¯ä»¥æ­£å¥½åšåˆ°è¿™ä¸€ç‚¹ï¼

ðŸ¤— transformers åº“æœ‰ä¸€ä¸ªéžå¸¸æ˜“äºŽä½¿ç”¨çš„æŠ½è±¡å±‚ï¼Œ[`pipeline()`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.pipeline)å¤„ç†å¤§éƒ¨åˆ†å¤æ‚ä»£ç ï¼Œä¸ºå¸¸è§ä»»åŠ¡æä¾›ç®€å•çš„ APIã€‚é€šè¿‡æŒ‡å®šä»»åŠ¡å’Œï¼ˆå¯é€‰ï¼‰æ¨¡åž‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å‡ è¡Œä»£ç ä½¿ç”¨çŽ°æœ‰æ¨¡åž‹ï¼š

```python
import gradio as gr

from transformers import pipeline

pipe = pipeline("translation", model="Helsinki-NLP/opus-mt-en-es")

def predict(text):
  return pipe(text)[0]["translation_text"]

demo = gr.Interface(
  fn=predict,
  inputs='text',
  outputs='text',
)

demo.launch()
```

ä½†æ˜¯ï¼Œ`gradio` å®žé™…ä¸Šä½¿å°† `pipeline` è½¬æ¢ä¸ºæ¼”ç¤ºæ›´åŠ å®¹æ˜“ï¼Œåªéœ€ä½¿ç”¨ `gradio.Interface.from_pipeline` æ–¹æ³•ï¼Œæ— éœ€æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºç»„ä»¶ï¼š

```python
from transformers import pipeline
import gradio as gr

pipe = pipeline("translation", model="Helsinki-NLP/opus-mt-en-es")

demo = gr.Interface.from_pipeline(pipe)
demo.launch()
```

ä¸Šè¿°ä»£ç ç”Ÿæˆäº†ä»¥ä¸‹ç•Œé¢ï¼Œæ‚¨å¯ä»¥åœ¨æµè§ˆå™¨ä¸­ç›´æŽ¥å°è¯•ï¼š

<gradio-app space="Helsinki-NLP/opus-mt-en-es"></gradio-app>

## Using Hugging Face Inference Endpoints

Hugging Face æä¾›äº†ä¸€ä¸ªåä¸º[Serverless Inference Endpoints](https://huggingface.co/inference-api)çš„å…è´¹æœåŠ¡ï¼Œå…è®¸æ‚¨å‘ Hub ä¸­çš„æ¨¡åž‹å‘é€ HTTP è¯·æ±‚ã€‚å¯¹äºŽåŸºäºŽ transformers æˆ– diffusers çš„æ¨¡åž‹ï¼ŒAPI çš„é€Ÿåº¦å¯ä»¥æ¯”è‡ªå·±è¿è¡ŒæŽ¨ç†å¿« 2 åˆ° 10 å€ã€‚è¯¥ API æ˜¯å…è´¹çš„ï¼ˆå—é€ŸçŽ‡é™åˆ¶ï¼‰ï¼Œæ‚¨å¯ä»¥åœ¨æƒ³è¦åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨æ—¶åˆ‡æ¢åˆ°ä¸“ç”¨çš„[æŽ¨ç†ç«¯ç‚¹](https://huggingface.co/pricing)ã€‚

è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨æŽ¨ç† API è€Œä¸æ˜¯è‡ªå·±åŠ è½½æ¨¡åž‹çš„æ–¹å¼è¿›è¡Œç›¸åŒçš„æ¼”ç¤ºã€‚é‰´äºŽ Inference Endpoints æ”¯æŒçš„ Hugging Face æ¨¡åž‹ï¼ŒGradio å¯ä»¥è‡ªåŠ¨æŽ¨æ–­å‡ºé¢„æœŸçš„è¾“å…¥å’Œè¾“å‡ºï¼Œå¹¶è¿›è¡Œåº•å±‚æœåŠ¡å™¨è°ƒç”¨ï¼Œå› æ­¤æ‚¨ä¸å¿…æ‹…å¿ƒå®šä¹‰é¢„æµ‹å‡½æ•°ã€‚ä»¥ä¸‹æ˜¯ä»£ç ç¤ºä¾‹ï¼

```python
import gradio as gr

demo = gr.load("Helsinki-NLP/opus-mt-en-es", src="models")

demo.launch()
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€æŒ‡å®šæ¨¡åž‹åç§°å¹¶è¯´æ˜Ž `src` åº”ä¸º `models`ï¼ˆHugging Face çš„ Model Hubï¼‰ã€‚ç”±äºŽæ‚¨ä¸ä¼šåœ¨è®¡ç®—æœºä¸ŠåŠ è½½æ¨¡åž‹ï¼Œå› æ­¤æ— éœ€å®‰è£…ä»»ä½•ä¾èµ–é¡¹ï¼ˆé™¤äº† `gradio`ï¼‰ã€‚

æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ï¼Œç¬¬ä¸€æ¬¡æŽ¨ç†å¤§çº¦éœ€è¦ 20 ç§’ã€‚è¿™æ˜¯å› ä¸ºæŽ¨ç† API æ­£åœ¨æœåŠ¡å™¨ä¸­åŠ è½½æ¨¡åž‹ã€‚ä¹‹åŽæ‚¨ä¼šèŽ·å¾—ä¸€äº›å¥½å¤„ï¼š

- æŽ¨ç†é€Ÿåº¦æ›´å¿«ã€‚
- æœåŠ¡å™¨ç¼“å­˜æ‚¨çš„è¯·æ±‚ã€‚
- æ‚¨èŽ·å¾—å†…ç½®çš„è‡ªåŠ¨ç¼©æ”¾åŠŸèƒ½ã€‚

## æ‰˜ç®¡æ‚¨çš„ Gradio æ¼”ç¤º

[Hugging Face Spaces](https://hf.co/spaces)å…è®¸ä»»ä½•äººå…è´¹æ‰˜ç®¡å…¶ Gradio æ¼”ç¤ºï¼Œä¸Šä¼  Gradio æ¼”ç¤ºåªéœ€å‡ åˆ†é’Ÿã€‚æ‚¨å¯ä»¥å‰å¾€[hf.co/new-space](https://huggingface.co/new-space)ï¼Œé€‰æ‹© Gradio SDKï¼Œåˆ›å»ºä¸€ä¸ª `app.py` æ–‡ä»¶ï¼Œå®Œæˆï¼æ‚¨å°†æ‹¥æœ‰ä¸€ä¸ªå¯ä»¥ä¸Žä»»ä½•äººå…±äº«çš„æ¼”ç¤ºã€‚è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·é˜…è¯»[æ­¤æŒ‡å—ä»¥ä½¿ç”¨ç½‘ç«™åœ¨ Hugging Face Spaces ä¸Šæ‰˜ç®¡](https://huggingface.co/blog/gradio-spaces)ã€‚

æˆ–è€…ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨[huggingface_hub client library](https://huggingface.co/docs/huggingface_hub/index)åº“æ¥ä»¥ç¼–ç¨‹æ–¹å¼åˆ›å»ºä¸€ä¸ª Spaceã€‚è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š

```python
from huggingface_hub import (
    create_repo,
    get_full_repo_name,
    upload_file,
)
create_repo(name=target_space_name, token=hf_token, repo_type="space", space_sdk="gradio")
repo_name = get_full_repo_name(model_id=target_space_name, token=hf_token)
file_url = upload_file(
    path_or_fileobj="file.txt",
    path_in_repo="app.py",
    repo_id=repo_name,
    repo_type="space",
    token=hf_token,
)
```

åœ¨è¿™é‡Œï¼Œ`create_repo` ä½¿ç”¨ç‰¹å®šå¸æˆ·çš„ Write Token åœ¨ç‰¹å®šå¸æˆ·ä¸‹åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ç›®æ ‡åç§°çš„ gradio repoã€‚`repo_name` èŽ·å–ç›¸å…³å­˜å‚¨åº“çš„å®Œæ•´å­˜å‚¨åº“åç§°ã€‚æœ€åŽï¼Œ`upload_file` å°†æ–‡ä»¶ä¸Šä¼ åˆ°å­˜å‚¨åº“ä¸­ï¼Œå¹¶å°†å…¶å‘½åä¸º `app.py`ã€‚

## åœ¨å…¶ä»–ç½‘ç«™ä¸ŠåµŒå…¥æ‚¨çš„ Space æ¼”ç¤º

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å·²ç»çœ‹åˆ°äº†è®¸å¤šåµŒå…¥çš„ Gradio æ¼”ç¤ºã€‚æ‚¨ä¹Ÿå¯ä»¥åœ¨è‡ªå·±çš„ç½‘ç«™ä¸Šè¿™æ ·åšï¼ç¬¬ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ªåŒ…å«æ‚¨æƒ³å±•ç¤ºçš„æ¼”ç¤ºçš„ Hugging Face Spaceã€‚ç„¶åŽï¼Œ[æŒ‰ç…§æ­¤å¤„çš„æ­¥éª¤å°† Space åµŒå…¥åˆ°æ‚¨çš„ç½‘ç«™ä¸Š](/sharing-your-app/#embedding-hosted-spaces)ã€‚

## ä»Ž Spaces åŠ è½½æ¼”ç¤º

æ‚¨è¿˜å¯ä»¥åœ¨ Hugging Face Spaces ä¸Šä½¿ç”¨å’Œæ··åˆçŽ°æœ‰çš„ Gradio æ¼”ç¤ºã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥å°†ä¸¤ä¸ªçŽ°æœ‰çš„ Gradio æ¼”ç¤ºæ”¾åœ¨å•ç‹¬çš„é€‰é¡¹å¡ä¸­å¹¶åˆ›å»ºä¸€ä¸ªæ–°çš„æ¼”ç¤ºã€‚æ‚¨å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œæ­¤æ–°æ¼”ç¤ºï¼Œæˆ–å°†å…¶ä¸Šä¼ åˆ° Spacesï¼Œä¸ºæ··åˆå’Œåˆ›å»ºæ–°çš„æ¼”ç¤ºæä¾›æ— é™å¯èƒ½æ€§ï¼

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œå…¨å®žçŽ°æ­¤ç›®æ ‡çš„ç¤ºä¾‹ï¼š

```python
import gradio as gr

with gr.Blocks() as demo:
  with gr.Tab("Translate to Spanish"):
    gr.load("gradio/helsinki_translation_en_es", src="spaces")
  with gr.Tab("Translate to French"):
    gr.load("abidlabs/en2fr", src="spaces")

demo.launch()
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `gr.load()`ï¼Œè¿™ä¸Žä½¿ç”¨æŽ¨ç† API åŠ è½½æ¨¡åž‹æ‰€ä½¿ç”¨çš„æ–¹æ³•ç›¸åŒã€‚ä½†æ˜¯ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŒ‡å®š `src` ä¸º `spaces`ï¼ˆHugging Face Spacesï¼‰ã€‚

## å°ç»“

å°±æ˜¯è¿™æ ·ï¼è®©æˆ‘ä»¬å›žé¡¾ä¸€ä¸‹ Gradio å’Œ Hugging Face å…±åŒå·¥ä½œçš„å„ç§æ–¹å¼ï¼š

1. æ‚¨å¯ä»¥ä½¿ç”¨ `from_pipeline()` å°† `transformers` pipeline è½¬æ¢ä¸º Gradio æ¼”ç¤º
2. æ‚¨å¯ä»¥ä½¿ç”¨ `gr.load()` è½»æ¾åœ°å›´ç»•æŽ¨ç† API æž„å»ºæ¼”ç¤ºï¼Œè€Œæ— éœ€åŠ è½½æ¨¡åž‹
3. æ‚¨å¯ä»¥åœ¨ Hugging Face Spaces ä¸Šæ‰˜ç®¡æ‚¨çš„ Gradio æ¼”ç¤ºï¼Œå¯ä»¥ä½¿ç”¨ GUI æˆ–å®Œå…¨ä½¿ç”¨ Pythonã€‚
4. æ‚¨å¯ä»¥å°†æ‰˜ç®¡åœ¨ Hugging Face Spaces ä¸Šçš„ Gradio æ¼”ç¤ºåµŒå…¥åˆ°è‡ªå·±çš„ç½‘ç«™ä¸Šã€‚
5. æ‚¨å¯ä»¥ä½¿ç”¨ `gr.load()` ä»Ž Hugging Face Spaces åŠ è½½æ¼”ç¤ºï¼Œä»¥é‡æ–°æ··åˆå’Œåˆ›å»ºæ–°çš„ Gradio æ¼”ç¤ºã€‚

ðŸ¤—

---

<!-- Source: guides/cn/05_tabular-data-science-and-plots/01_connecting-to-a-database.md -->
# è¿žæŽ¥åˆ°æ•°æ®åº“

ç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/gradio/chicago-bike-share-dashboard
æ ‡ç­¾ï¼šTABULAR, PLOTS

## ä»‹ç»

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ Gradio è¿žæŽ¥æ‚¨çš„åº”ç”¨ç¨‹åºåˆ°æ•°æ®åº“ã€‚æˆ‘ä»¬å°†ä¼š
è¿žæŽ¥åˆ°åœ¨ AWS ä¸Šæ‰˜ç®¡çš„ PostgreSQL æ•°æ®åº“ï¼Œä½† Gradio å¯¹äºŽæ‚¨è¿žæŽ¥çš„æ•°æ®åº“ç±»åž‹å’Œæ‰˜ç®¡ä½ç½®æ²¡æœ‰ä»»ä½•é™åˆ¶ã€‚å› æ­¤ï¼Œåªè¦æ‚¨èƒ½ç¼–å†™ Python ä»£ç æ¥è¿žæŽ¥
æ‚¨çš„æ•°æ®ï¼Œæ‚¨å°±å¯ä»¥ä½¿ç”¨ Gradio åœ¨ Web ç•Œé¢ä¸­æ˜¾ç¤ºå®ƒ ðŸ’ª

## æ¦‚è¿°

æˆ‘ä»¬å°†åˆ†æžæ¥è‡ªèŠåŠ å“¥çš„è‡ªè¡Œè½¦å…±äº«æ•°æ®ã€‚æ•°æ®æ‰˜ç®¡åœ¨ kaggle [è¿™é‡Œ](https://www.kaggle.com/datasets/evangower/cyclistic-bike-share?select=202203-divvy-tripdata.csv)ã€‚
æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªä»ªè¡¨ç›˜ï¼Œè®©æˆ‘ä»¬çš„ä¸šåŠ¡åˆ©ç›Šç›¸å…³è€…èƒ½å¤Ÿå›žç­”ä»¥ä¸‹é—®é¢˜ï¼š

1. ç”µåŠ¨è‡ªè¡Œè½¦æ˜¯å¦æ¯”æ™®é€šè‡ªè¡Œè½¦æ›´å—æ¬¢è¿Žï¼Ÿ
2. å“ªäº›å‡ºå‘è‡ªè¡Œè½¦ç«™ç‚¹æœ€å—æ¬¢è¿Žï¼Ÿ

åœ¨æœ¬æŒ‡å—ç»“æŸæ—¶ï¼Œæˆ‘ä»¬å°†æ‹¥æœ‰ä¸€ä¸ªå¦‚ä¸‹æ‰€ç¤ºçš„åŠŸèƒ½é½å…¨çš„åº”ç”¨ç¨‹åºï¼š

<gradio-app space="gradio/chicago-bike-share-dashboard"> </gradio-app>

## æ­¥éª¤ 1 - åˆ›å»ºæ•°æ®åº“

æˆ‘ä»¬å°†åœ¨ Amazon çš„ RDS æœåŠ¡ä¸Šæ‰˜ç®¡æˆ‘ä»¬çš„æ•°æ®ã€‚å¦‚æžœè¿˜æ²¡æœ‰ AWS è´¦å·ï¼Œè¯·åˆ›å»ºä¸€ä¸ª
å¹¶åœ¨å…è´¹å±‚çº§ä¸Šåˆ›å»ºä¸€ä¸ª PostgreSQL æ•°æ®åº“ã€‚

**é‡è¦æç¤º**ï¼šå¦‚æžœæ‚¨è®¡åˆ’åœ¨ HuggingFace Spaces ä¸Šæ‰˜ç®¡æ­¤æ¼”ç¤ºï¼Œè¯·ç¡®ä¿æ•°æ®åº“åœ¨ **8080** ç«¯å£ä¸Šã€‚Spaces
å°†é˜»æ­¢é™¤ç«¯å£ 80ã€443 æˆ– 8080 ä¹‹å¤–çš„æ‰€æœ‰å¤–éƒ¨è¿žæŽ¥ï¼Œå¦‚æ­¤[å¤„æ‰€ç¤º](https://huggingface.co/docs/hub/spaces-overview#networking)ã€‚
RDS ä¸å…è®¸æ‚¨åœ¨ 80 æˆ– 443 ç«¯å£ä¸Šåˆ›å»º postgreSQL å®žä¾‹ã€‚

åˆ›å»ºå®Œæ•°æ®åº“åŽï¼Œä»Ž Kaggle ä¸‹è½½æ•°æ®é›†å¹¶å°†å…¶ä¸Šä¼ åˆ°æ•°æ®åº“ä¸­ã€‚
ä¸ºäº†æ¼”ç¤ºçš„ç›®çš„ï¼Œæˆ‘ä»¬åªä¼šä¸Šä¼  2022 å¹´ 3 æœˆçš„æ•°æ®ã€‚

## æ­¥éª¤ 2.a - ç¼–å†™ ETL ä»£ç 

æˆ‘ä»¬å°†æŸ¥è¯¢æ•°æ®åº“ï¼ŒæŒ‰è‡ªè¡Œè½¦ç±»åž‹ï¼ˆç”µåŠ¨ã€æ ‡å‡†æˆ–æœ‰ç ï¼‰è¿›è¡Œåˆ†ç»„ï¼Œå¹¶èŽ·å–æ€»éª‘è¡Œæ¬¡æ•°ã€‚
æˆ‘ä»¬è¿˜å°†æŸ¥è¯¢æ¯ä¸ªç«™ç‚¹çš„å‡ºå‘éª‘è¡Œæ¬¡æ•°ï¼Œå¹¶èŽ·å–å‰ 5 ä¸ªã€‚

ç„¶åŽï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ matplotlib å°†æŸ¥è¯¢ç»“æžœå¯è§†åŒ–ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ pandas çš„[read_sql](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html)
æ–¹æ³•æ¥è¿žæŽ¥æ•°æ®åº“ã€‚è¿™éœ€è¦å®‰è£… `psycopg2` åº“ã€‚

ä¸ºäº†è¿žæŽ¥åˆ°æ•°æ®åº“ï¼Œæˆ‘ä»¬å°†æŒ‡å®šæ•°æ®åº“çš„ç”¨æˆ·åã€å¯†ç å’Œä¸»æœºä½œä¸ºçŽ¯å¢ƒå˜é‡ã€‚
è¿™æ ·å¯ä»¥é€šè¿‡é¿å…å°†æ•æ„Ÿä¿¡æ¯ä»¥æ˜Žæ–‡å½¢å¼å­˜å‚¨åœ¨åº”ç”¨ç¨‹åºæ–‡ä»¶ä¸­ï¼Œä½¿æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºæ›´å®‰å…¨ã€‚

```python
import os
import pandas as pd
import matplotlib.pyplot as plt

DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_HOST = os.getenv("DB_HOST")
PORT = 8080
DB_NAME = "bikeshare"

connection_string = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}?port={PORT}&dbname={DB_NAME}"

def get_count_ride_type():
    df = pd.read_sql(
    """
        SELECT COUNT(ride_id) as n, rideable_type
        FROM rides
        GROUP BY rideable_type
        ORDER BY n DESC
    """,
    con=connection_string
    )
    fig_m, ax = plt.subplots()
    ax.bar(x=df['rideable_type'], height=df['n'])
    ax.set_title("Number of rides by bycycle type")
    ax.set_ylabel("Number of Rides")
    ax.set_xlabel("Bicycle Type")
    return fig_m


def get_most_popular_stations():

    df = pd.read_sql(
        """
    SELECT COUNT(ride_id) as n, MAX(start_station_name) as station
    FROM RIDES
    WHERE start_station_name is NOT NULL
    GROUP BY start_station_id
    ORDER BY n DESC
    LIMIT 5
    """,
    con=connection_string
    )
    fig_m, ax = plt.subplots()
    ax.bar(x=df['station'], height=df['n'])
    ax.set_title("Most popular stations")
    ax.set_ylabel("Number of Rides")
    ax.set_xlabel("Station Name")
    ax.set_xticklabels(
        df['station'], rotation=45, ha="right", rotation_mode="anchor"
    )
    ax.tick_params(axis="x", labelsize=8)
    fig_m.tight_layout()
    return fig_m
```

å¦‚æžœæ‚¨åœ¨æœ¬åœ°è¿è¡Œæˆ‘ä»¬çš„è„šæœ¬ï¼Œå¯ä»¥åƒä¸‹é¢è¿™æ ·å°†å‡­æ®ä½œä¸ºçŽ¯å¢ƒå˜é‡ä¼ é€’ï¼š

```bash
DB_USER='username' DB_PASSWORD='password' DB_HOST='host' python app.py
```

## æ­¥éª¤ 2.c - ç¼–å†™æ‚¨çš„ gradio åº”ç”¨ç¨‹åº

æˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ªå•ç‹¬çš„ `gr.Plot` ç»„ä»¶å°†æˆ‘ä»¬çš„ matplotlib å›¾è¡¨å¹¶æŽ’æ˜¾ç¤ºåœ¨ä¸€èµ·ï¼Œä½¿ç”¨ `gr.Row()`ã€‚
å› ä¸ºæˆ‘ä»¬å·²ç»åœ¨ `demo.load()` äº‹ä»¶è§¦å‘å™¨ä¸­å°è£…äº†èŽ·å–æ•°æ®çš„å‡½æ•°ï¼Œ
æˆ‘ä»¬çš„æ¼”ç¤ºå°†åœ¨æ¯æ¬¡ç½‘é¡µåŠ è½½æ—¶ä»Žæ•°æ®åº“**åŠ¨æ€**èŽ·å–æœ€æ–°æ•°æ®ã€‚ðŸª„

```python
import gradio as gr

with gr.Blocks() as demo:
    with gr.Row():
        bike_type = gr.Plot()
        station = gr.Plot()

    demo.load(get_count_ride_type, inputs=None, outputs=bike_type)
    demo.load(get_most_popular_stations, inputs=None, outputs=station)

demo.launch()
```

## æ­¥éª¤ 3 - éƒ¨ç½²

å¦‚æžœæ‚¨è¿è¡Œä¸Šè¿°ä»£ç ï¼Œæ‚¨çš„åº”ç”¨ç¨‹åºå°†åœ¨æœ¬åœ°è¿è¡Œã€‚
æ‚¨ç”šè‡³å¯ä»¥é€šè¿‡å°† `share=True` å‚æ•°ä¼ é€’ç»™ `launch` æ¥èŽ·å¾—ä¸€ä¸ªä¸´æ—¶å…±äº«é“¾æŽ¥ã€‚

ä½†æ˜¯å¦‚æžœæ‚¨æƒ³è¦ä¸€ä¸ªæ°¸ä¹…çš„éƒ¨ç½²è§£å†³æ–¹æ¡ˆå‘¢ï¼Ÿ
è®©æˆ‘ä»¬å°†æˆ‘ä»¬çš„ Gradio åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ°å…è´¹çš„ HuggingFace Spaces å¹³å°ä¸Šã€‚

å¦‚æžœæ‚¨ä¹‹å‰æ²¡æœ‰ä½¿ç”¨è¿‡ Spacesï¼Œè¯·æŒ‰ç…§ä¹‹å‰çš„æŒ‡å—[è¿™é‡Œ](/using_hugging_face_integrations)è¿›è¡Œæ“ä½œã€‚
æ‚¨å°†éœ€è¦å°† `DB_USER`ã€`DB_PASSWORD` å’Œ `DB_HOST` å˜é‡æ·»åŠ ä¸º "Repo Secrets"ã€‚æ‚¨å¯ä»¥åœ¨ " è®¾ç½® " é€‰é¡¹å¡ä¸­è¿›è¡Œæ­¤æ“ä½œã€‚

![secrets](/assets/guides/secrets.png)

## ç»“è®º

æ­å–œä½ ï¼æ‚¨çŸ¥é“å¦‚ä½•å°†æ‚¨çš„ Gradio åº”ç”¨ç¨‹åºè¿žæŽ¥åˆ°äº‘ç«¯æ‰˜ç®¡çš„æ•°æ®åº“ï¼â˜ï¸

æˆ‘ä»¬çš„ä»ªè¡¨æ¿çŽ°åœ¨æ­£åœ¨[Spaces](https://huggingface.co/spaces/gradio/chicago-bike-share-dashboard)ä¸Šè¿è¡Œã€‚
å®Œæ•´ä»£ç åœ¨[è¿™é‡Œ](https://huggingface.co/spaces/gradio/chicago-bike-share-dashboard/blob/main/app.py)

æ­£å¦‚æ‚¨æ‰€è§ï¼ŒGradio ä½¿æ‚¨å¯ä»¥è¿žæŽ¥åˆ°æ‚¨çš„æ•°æ®å¹¶ä»¥æ‚¨æƒ³è¦çš„æ–¹å¼æ˜¾ç¤ºï¼ðŸ”¥

---

<!-- Source: guides/cn/06_client-libraries/01_getting-started-with-the-python-client.md -->
# ä½¿ç”¨ Gradio Python å®¢æˆ·ç«¯å…¥é—¨

Tags: CLIENT, API, SPACES

Gradio Python å®¢æˆ·ç«¯ä½¿å¾—å°†ä»»ä½• Gradio åº”ç”¨ç¨‹åºä½œä¸º API ä½¿ç”¨å˜å¾—éžå¸¸å®¹æ˜“ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ä¸€ä¸‹ä»Žéº¦å…‹é£Žå½•åˆ¶çš„[Whisper éŸ³é¢‘æ–‡ä»¶](https://huggingface.co/spaces/abidlabs/whisper)çš„è½¬å½•ã€‚

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)

ä½¿ç”¨ `gradio_client` åº“ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å°† Gradio ç”¨ä½œ APIï¼Œä»¥ç¼–ç¨‹æ–¹å¼è½¬å½•éŸ³é¢‘æ–‡ä»¶ã€‚

ä¸‹é¢æ˜¯å®Œæˆæ­¤æ“ä½œçš„æ•´ä¸ªä»£ç ï¼š

```python
from gradio_client import Client

client = Client("abidlabs/whisper")
client.predict("audio_sample.wav")

>> "è¿™æ˜¯Whisperè¯­éŸ³è¯†åˆ«æ¨¡åž‹çš„æµ‹è¯•ã€‚"
```

Gradio å®¢æˆ·ç«¯é€‚ç”¨äºŽä»»ä½•æ‰˜ç®¡åœ¨ Hugging Face Spaces ä¸Šçš„ Gradio åº”ç”¨ç¨‹åºï¼Œæ— è®ºæ˜¯å›¾åƒç”Ÿæˆå™¨ã€æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨ã€æœ‰çŠ¶æ€èŠå¤©æœºå™¨äººã€ç¨Žé‡‘è®¡ç®—å™¨è¿˜æ˜¯å…¶ä»–ä»»ä½•åº”ç”¨ç¨‹åºï¼Gradio å®¢æˆ·ç«¯ä¸»è¦ç”¨äºŽæ‰˜ç®¡åœ¨[Hugging Face Spaces](https://hf.space)ä¸Šçš„åº”ç”¨ç¨‹åºï¼Œä½†ä½ çš„åº”ç”¨ç¨‹åºå¯ä»¥æ‰˜ç®¡åœ¨ä»»ä½•åœ°æ–¹ï¼Œæ¯”å¦‚ä½ è‡ªå·±çš„æœåŠ¡å™¨ã€‚

**å…ˆå†³æ¡ä»¶**ï¼šè¦ä½¿ç”¨ Gradio å®¢æˆ·ç«¯ï¼Œä½ ä¸éœ€è¦è¯¦ç»†äº†è§£ `gradio` åº“ã€‚ä½†æ˜¯ï¼Œäº†è§£ Gradio çš„è¾“å…¥å’Œè¾“å‡ºç»„ä»¶çš„æ¦‚å¿µä¼šæœ‰æ‰€å¸®åŠ©ã€‚

## å®‰è£…

å¦‚æžœä½ å·²ç»å®‰è£…äº†æœ€æ–°ç‰ˆæœ¬çš„ `gradio`ï¼Œé‚£ä¹ˆ `gradio_client` å°±ä½œä¸ºä¾èµ–é¡¹åŒ…å«åœ¨å…¶ä¸­ã€‚

å¦åˆ™ï¼Œå¯ä»¥ä½¿ç”¨ pipï¼ˆæˆ– pip3ï¼‰å®‰è£…è½»é‡çº§çš„ `gradio_client` åŒ…ï¼Œå¹¶ä¸”å·²ç»æµ‹è¯•å¯ä»¥åœ¨ Python 3.9 æˆ–æ›´é«˜ç‰ˆæœ¬ä¸Šè¿è¡Œï¼š

```bash
$ pip install gradio_client
```

## è¿žæŽ¥åˆ°è¿è¡Œä¸­çš„ Gradio åº”ç”¨ç¨‹åº

é¦–å…ˆåˆ›å»ºä¸€ä¸ª `Client` å¯¹è±¡ï¼Œå¹¶å°†å…¶è¿žæŽ¥åˆ°è¿è¡Œåœ¨ Hugging Face Spaces ä¸Šæˆ–å…¶ä»–ä»»ä½•åœ°æ–¹çš„ Gradio åº”ç”¨ç¨‹åºã€‚

## è¿žæŽ¥åˆ° Hugging Face ç©ºé—´

```python
from gradio_client import Client

client = Client("abidlabs/en2fr")  # ä¸€ä¸ªå°†è‹±æ–‡ç¿»è¯‘ä¸ºæ³•æ–‡çš„Space
```

ä½ è¿˜å¯ä»¥é€šè¿‡åœ¨ `hf_token` å‚æ•°ä¸­ä¼ é€’ä½ çš„ HF ä»¤ç‰Œæ¥è¿žæŽ¥åˆ°ç§æœ‰ç©ºé—´ã€‚ä½ å¯ä»¥åœ¨è¿™é‡ŒèŽ·å–ä½ çš„ HF ä»¤ç‰Œï¼šhttps://huggingface.co/settings/tokens

```python
from gradio_client import Client

client = Client("abidlabs/my-private-space", hf_token="...")
```

## å¤åˆ¶ç©ºé—´ä»¥ä¾›ç§äººä½¿ç”¨

è™½ç„¶ä½ å¯ä»¥å°†ä»»ä½•å…¬å…±ç©ºé—´ç”¨ä½œ APIï¼Œä½†å¦‚æžœä½ å‘å‡ºå¤ªå¤šè¯·æ±‚ï¼Œä½ å¯èƒ½ä¼šå—åˆ° Hugging Face çš„é¢‘çŽ‡é™åˆ¶ã€‚è¦æ— é™åˆ¶åœ°ä½¿ç”¨ä¸€ä¸ªç©ºé—´ï¼Œåªéœ€å°†å…¶å¤åˆ¶ä»¥åˆ›å»ºä¸€ä¸ªç§æœ‰ç©ºé—´ï¼Œç„¶åŽå¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œå¤šä¸ªè¯·æ±‚ï¼

`gradio_client` åŒ…æ‹¬ä¸€ä¸ªç±»æ–¹æ³•ï¼š`Client.duplicate()`ï¼Œä½¿è¿™ä¸ªè¿‡ç¨‹å˜å¾—ç®€å•ï¼ˆä½ éœ€è¦ä¼ é€’ä½ çš„[Hugging Face ä»¤ç‰Œ](https://huggingface.co/settings/tokens)æˆ–ä½¿ç”¨ Hugging Face CLI ç™»å½•ï¼‰ï¼š

```python
import os
from gradio_client import Client

HF_TOKEN = os.environ.get("HF_TOKEN")

client = Client.duplicate("abidlabs/whisper", hf_token=HF_TOKEN)
client.predict("audio_sample.wav")

>> "This is a test of the whisper speech recognition model."
```

> > " è¿™æ˜¯ Whisper è¯­éŸ³è¯†åˆ«æ¨¡åž‹çš„æµ‹è¯•ã€‚"

å¦‚æžœä¹‹å‰å·²å¤åˆ¶äº†ä¸€ä¸ªç©ºé—´ï¼Œé‡æ–°è¿è¡Œ `duplicate()` å°†*ä¸ä¼š*åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºé—´ã€‚ç›¸åï¼Œå®¢æˆ·ç«¯å°†è¿žæŽ¥åˆ°ä¹‹å‰åˆ›å»ºçš„ç©ºé—´ã€‚å› æ­¤ï¼Œå¤šæ¬¡è¿è¡Œ `Client.duplicate()` æ–¹æ³•æ˜¯å®‰å…¨çš„ã€‚

**æ³¨æ„ï¼š** å¦‚æžœåŽŸå§‹ç©ºé—´ä½¿ç”¨äº† GPUï¼Œä½ çš„ç§æœ‰ç©ºé—´ä¹Ÿå°†ä½¿ç”¨ GPUï¼Œå¹¶ä¸”ä½ çš„ Hugging Face è´¦æˆ·å°†æ ¹æ® GPU çš„ä»·æ ¼è®¡è´¹ã€‚ä¸ºäº†é™ä½Žè´¹ç”¨ï¼Œåœ¨ 1 å°æ—¶æ²¡æœ‰æ´»åŠ¨åŽï¼Œä½ çš„ç©ºé—´å°†è‡ªåŠ¨ä¼‘çœ ã€‚ä½ è¿˜å¯ä»¥ä½¿ç”¨ `duplicate()` çš„ `hardware` å‚æ•°æ¥è®¾ç½®ç¡¬ä»¶ã€‚

## è¿žæŽ¥åˆ°é€šç”¨ Gradio åº”ç”¨ç¨‹åº

å¦‚æžœä½ çš„åº”ç”¨ç¨‹åºè¿è¡Œåœ¨å…¶ä»–åœ°æ–¹ï¼Œåªéœ€æä¾›å®Œæ•´çš„ URLï¼ŒåŒ…æ‹¬ "http://" æˆ– "https://"ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªåœ¨å…±äº« URL ä¸Šè¿è¡Œçš„ Gradio åº”ç”¨ç¨‹åºè¿›è¡Œé¢„æµ‹çš„ç¤ºä¾‹ï¼š

```python
from gradio_client import Client

client = Client("https://bec81a83-5b5c-471e.gradio.live")
```

## æ£€æŸ¥ API ç«¯ç‚¹

ä¸€æ—¦è¿žæŽ¥åˆ° Gradio åº”ç”¨ç¨‹åºï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨ `Client.view_api()` æ–¹æ³•æŸ¥çœ‹å¯ç”¨çš„ API ç«¯ç‚¹ã€‚å¯¹äºŽ Whisper ç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š

```bash
Client.predict() Usage Info
---------------------------
Named API endpoints: 1

 - predict(input_audio, api_name="/predict") -> value_0
    Parameters:
     - [Audio] input_audio: str (filepath or URL)
    Returns:
     - [Textbox] value_0: str (value)
```

è¿™æ˜¾ç¤ºäº†åœ¨æ­¤ç©ºé—´ä¸­æœ‰ 1 ä¸ª API ç«¯ç‚¹ï¼Œå¹¶æ˜¾ç¤ºäº†å¦‚ä½•ä½¿ç”¨ API ç«¯ç‚¹è¿›è¡Œé¢„æµ‹ï¼šæˆ‘ä»¬åº”è¯¥è°ƒç”¨ `.predict()` æ–¹æ³•ï¼ˆæˆ‘ä»¬å°†åœ¨ä¸‹é¢æŽ¢è®¨ï¼‰ï¼Œæä¾›ç±»åž‹ä¸º `str` çš„å‚æ•° `input_audio`ï¼Œå®ƒæ˜¯ä¸€ä¸ª`æ–‡ä»¶è·¯å¾„æˆ– URL`ã€‚

æˆ‘ä»¬è¿˜åº”è¯¥æä¾› `api_name='/predict'` å‚æ•°ç»™ `predict()` æ–¹æ³•ã€‚è™½ç„¶å¦‚æžœä¸€ä¸ª Gradio åº”ç”¨ç¨‹åºåªæœ‰ä¸€ä¸ªå‘½åçš„ç«¯ç‚¹ï¼Œè¿™ä¸æ˜¯å¿…éœ€çš„ï¼Œä½†å®ƒå…è®¸æˆ‘ä»¬åœ¨å•ä¸ªåº”ç”¨ç¨‹åºä¸­è°ƒç”¨ä¸åŒçš„ç«¯ç‚¹ï¼ˆå¦‚æžœå®ƒä»¬å¯ç”¨ï¼‰ã€‚å¦‚æžœä¸€ä¸ªåº”ç”¨ç¨‹åºæœ‰æ— åçš„ API ç«¯ç‚¹ï¼Œå¯ä»¥é€šè¿‡è¿è¡Œ `.view_api(all_endpoints=True)` æ¥æ˜¾ç¤ºå®ƒä»¬ã€‚

## è¿›è¡Œé¢„æµ‹

è¿›è¡Œé¢„æµ‹çš„æœ€ç®€å•æ–¹æ³•æ˜¯åªéœ€ä½¿ç”¨ç›¸åº”çš„å‚æ•°è°ƒç”¨ `.predict()` å‡½æ•°ï¼š

```python
from gradio_client import Client

client = Client("abidlabs/en2fr", api_name='/predict')
client.predict("Hello")

>> Bonjour
```

å¦‚æžœæœ‰å¤šä¸ªå‚æ•°ï¼Œé‚£ä¹ˆä½ åº”è¯¥å°†å®ƒä»¬ä½œä¸ºå•ç‹¬çš„å‚æ•°ä¼ é€’ç»™ `.predict()`ï¼Œå°±åƒè¿™æ ·ï¼š

````python
from gradio_client import Client

client = Client("gradio/calculator")
client.predict(4, "add", 5)

>> 9.0


å¯¹äºŽæŸäº›è¾“å…¥ï¼Œä¾‹å¦‚å›¾åƒï¼Œä½ åº”è¯¥ä¼ é€’æ–‡ä»¶çš„æ–‡ä»¶è·¯å¾„æˆ–URLã€‚åŒæ ·ï¼Œå¯¹åº”çš„è¾“å‡ºç±»åž‹ï¼Œä½ å°†èŽ·å¾—ä¸€ä¸ªæ–‡ä»¶è·¯å¾„æˆ–URLã€‚


```python
from gradio_client import Client

client = Client("abidlabs/whisper")
client.predict("https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3")

>> "My thought I have nobody by a beauty and will as you poured. Mr. Rochester is serve in that so don't find simpus, and devoted abode, to at might in a râ€”"

````

## å¼‚æ­¥è¿è¡Œä»»åŠ¡ï¼ˆRunning jobs asynchronouslyï¼‰

åº”æ³¨æ„`.predict()`æ˜¯ä¸€ä¸ª*é˜»å¡ž*æ“ä½œï¼Œå› ä¸ºå®ƒåœ¨è¿”å›žé¢„æµ‹ä¹‹å‰ç­‰å¾…æ“ä½œå®Œæˆã€‚

åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œç›´åˆ°ä½ éœ€è¦é¢„æµ‹ç»“æžœä¹‹å‰ï¼Œä½ æœ€å¥½è®©ä½œä¸šåœ¨åŽå°è¿è¡Œã€‚ä½ å¯ä»¥é€šè¿‡ä½¿ç”¨`.submit()`æ–¹æ³•åˆ›å»ºä¸€ä¸ª`Job`å®žä¾‹ï¼Œç„¶åŽç¨åŽè°ƒç”¨`.result()`åœ¨ä½œä¸šä¸ŠèŽ·å–ç»“æžœã€‚ä¾‹å¦‚ï¼š

```python
from gradio_client import Client

client = Client(space="abidlabs/en2fr")
job = client.submit("Hello", api_name="/predict")  # è¿™ä¸æ˜¯é˜»å¡žçš„

# åšå…¶ä»–äº‹æƒ…

job.result()  # è¿™æ˜¯é˜»å¡žçš„

>> Bonjour
```

## æ·»åŠ å›žè°ƒ ï¼ˆAdding callbacksï¼‰

æˆ–è€…ï¼Œå¯ä»¥æ·»åŠ ä¸€ä¸ªæˆ–å¤šä¸ªå›žè°ƒæ¥åœ¨ä½œä¸šå®ŒæˆåŽæ‰§è¡Œæ“ä½œï¼Œåƒè¿™æ ·ï¼š

```python
from gradio_client import Client

def print_result(x):
    print(" ç¿»è¯‘çš„ç»“æžœæ˜¯ï¼š{x}")

client = Client(space="abidlabs/en2fr")

job = client.submit("Hello", api_name="/predict", result_callbacks=[print_result])

# åšå…¶ä»–äº‹æƒ…

>> ç¿»è¯‘çš„ç»“æžœæ˜¯ï¼šBonjour

```

## çŠ¶æ€ ï¼ˆStatusï¼‰

`Job`å¯¹è±¡è¿˜å…è®¸æ‚¨é€šè¿‡è°ƒç”¨`.status()`æ–¹æ³•èŽ·å–è¿è¡Œä½œä¸šçš„çŠ¶æ€ã€‚è¿™å°†è¿”å›žä¸€ä¸ª`StatusUpdate`å¯¹è±¡ï¼Œå…·æœ‰ä»¥ä¸‹å±žæ€§ï¼š`code`ï¼ˆçŠ¶æ€ä»£ç ï¼Œå…¶ä¸­ä¹‹ä¸€è¡¨ç¤ºçŠ¶æ€çš„ä¸€ç»„å®šä¹‰çš„å­—ç¬¦ä¸²ã€‚å‚è§`utils.Status`ç±»ï¼‰ã€`rank`ï¼ˆæ­¤ä½œä¸šåœ¨é˜Ÿåˆ—ä¸­çš„å½“å‰ä½ç½®ï¼‰ã€`queue_size`ï¼ˆæ€»é˜Ÿåˆ—å¤§å°ï¼‰ã€`eta`ï¼ˆæ­¤ä½œä¸šå°†å®Œæˆçš„é¢„è®¡æ—¶é—´ï¼‰ã€`success`ï¼ˆè¡¨ç¤ºä½œä¸šæ˜¯å¦æˆåŠŸå®Œæˆçš„å¸ƒå°”å€¼ï¼‰å’Œ`time`ï¼ˆç”ŸæˆçŠ¶æ€çš„æ—¶é—´ï¼‰ã€‚

```py
from gradio_client import Client

client = Client(src="gradio/calculator")
job = client.submit(5, "add", 4, api_name="/predict")
job.status()

>> <Status.STARTING: 'STARTING'>
```

_æ³¨æ„_ï¼š`Job`ç±»è¿˜æœ‰ä¸€ä¸ª`.done()`å®žä¾‹æ–¹æ³•ï¼Œè¿”å›žä¸€ä¸ªå¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºä½œä¸šæ˜¯å¦å·²å®Œæˆã€‚

## å–æ¶ˆä½œä¸š ï¼ˆCancelling Jobsï¼‰

`Job`ç±»è¿˜æœ‰ä¸€ä¸ª`.cancel()`å®žä¾‹æ–¹æ³•ï¼Œå–æ¶ˆå·²æŽ’é˜Ÿä½†å°šæœªå¼€å§‹çš„ä½œä¸šã€‚ä¾‹å¦‚ï¼Œå¦‚æžœä½ è¿è¡Œï¼š

```py
client = Client("abidlabs/whisper")
job1 = client.submit("audio_sample1.wav")
job2 = client.submit("audio_sample2.wav")
job1.cancel()  # å°†è¿”å›ž Falseï¼Œå‡è®¾ä½œä¸šå·²å¼€å§‹
job2.cancel()  # å°†è¿”å›ž Trueï¼Œè¡¨ç¤ºä½œä¸šå·²å–æ¶ˆ
```

å¦‚æžœç¬¬ä¸€ä¸ªä½œä¸šå·²å¼€å§‹å¤„ç†ï¼Œåˆ™å®ƒå°†ä¸ä¼šè¢«å–æ¶ˆã€‚å¦‚æžœç¬¬äºŒä¸ªä½œä¸šå°šæœªå¼€å§‹ï¼Œåˆ™å®ƒå°†æˆåŠŸå–æ¶ˆå¹¶ä»Žé˜Ÿåˆ—ä¸­åˆ é™¤ã€‚

## ç”Ÿæˆå™¨ç«¯ç‚¹ ï¼ˆGenerator Endpointsï¼‰

æŸäº›Gradio APIç«¯ç‚¹ä¸è¿”å›žå•ä¸ªå€¼ï¼Œè€Œæ˜¯è¿”å›žä¸€ç³»åˆ—å€¼ã€‚ä½ å¯ä»¥éšæ—¶ä»Žè¿™æ ·çš„ç”Ÿæˆå™¨ç«¯ç‚¹èŽ·å–è¿”å›žçš„ä¸€ç³»åˆ—å€¼ï¼Œæ–¹æ³•æ˜¯è¿è¡Œ`job.outputs()`ï¼š

```py
from gradio_client import Client

client = Client(src="gradio/count_generator")
job = client.submit(3, api_name="/count")
while not job.done():
    time.sleep(0.1)
job.outputs()

>> ['0', '1', '2']
```

è¯·æ³¨æ„ï¼Œåœ¨ç”Ÿæˆå™¨ç«¯ç‚¹ä¸Šè¿è¡Œ`job.result()`åªä¼šèŽ·å¾—ç«¯ç‚¹è¿”å›žçš„*ç¬¬ä¸€ä¸ª*å€¼ã€‚

`Job`å¯¹è±¡è¿˜æ˜¯å¯è¿­ä»£çš„ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥ä½¿ç”¨å®ƒæŒ‰ç…§ä»Žç«¯ç‚¹è¿”å›žçš„ç»“æžœé€ä¸ªæ˜¾ç¤ºç”Ÿæˆå™¨å‡½æ•°çš„ç»“æžœã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨`Job`ä½œä¸ºç”Ÿæˆå™¨çš„ç­‰æ•ˆç¤ºä¾‹ï¼š

```py
from gradio_client import Client

client = Client(src="gradio/count_generator")
job = client.submit(3, api_name="/count")

for o in job:
    print(o)

>> 0
>> 1
>> 2
```

ä½ è¿˜å¯ä»¥å–æ¶ˆå…·æœ‰è¿­ä»£è¾“å‡ºçš„ä½œä¸šï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½œä¸šå°†åœ¨å½“å‰è¿­ä»£å®Œæˆè¿è¡ŒåŽå®Œæˆã€‚

```py
from gradio_client import Client
import time

client = Client("abidlabs/test-yield")
job = client.submit("abcdef")
time.sleep(3)
job.cancel()  # ä½œä¸šåœ¨è¿è¡Œ 2 ä¸ªè¿­ä»£åŽå–æ¶ˆ
```

---

<!-- Source: guides/02_building-interfaces/02_flagging.md -->

# Flagging

You may have noticed the "Flag" button that appears by default in your `Interface`. When a user using your demo sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the `flagging_dir=` argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well.

For example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:

```directory
+-- calculator.py
+-- flagged/
|   +-- logs.csv
```

_flagged/logs.csv_

```csv
num1,operation,num2,Output
5,add,7,12
6,subtract,1.5,4.5
```

With the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:

```directory
+-- sepia.py
+-- flagged/
|   +-- logs.csv
|   +-- im/
|   |   +-- 0.png
|   |   +-- 1.png
|   +-- Output/
|   |   +-- 0.png
|   |   +-- 1.png
```

_flagged/logs.csv_

```csv
im,Output
im/0.png,Output/0.png
im/1.png,Output/1.png
```

If you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.

---

<!-- Source: guides/03_building-with-blocks/02_controlling-layout.md -->
# Controlling Layout

By default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).

## Rows

Elements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:

```python
with gr.Blocks() as demo:
    with gr.Row():
        btn1 = gr.Button("Button 1")
        btn2 = gr.Button("Button 2")
```

You can set every element in a Row to have the same height. Configure this with the `equal_height` argument.

```python
with gr.Blocks() as demo:
    with gr.Row(equal_height=True):
        textbox = gr.Textbox()
        btn2 = gr.Button("Button 2")
```

The widths of elements in a Row can be controlled via a combination of `scale` and `min_width` arguments that are present in every Component.

- `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, the element will not expand to take up space. If scale is set to `1` or greater, the element will expand. Multiple elements in a row will expand proportional to their scale. Below, `btn2` will expand twice as much as `btn1`, while `btn0` will not expand at all:

```python
with gr.Blocks() as demo:
    with gr.Row():
        btn0 = gr.Button("Button 0", scale=0)
        btn1 = gr.Button("Button 1", scale=1)
        btn2 = gr.Button("Button 2", scale=2)
```

- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.

Learn more about Rows in the [docs](https://gradio.app/docs/row).

## Columns and Nesting

Components within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:

$code_rows_and_columns
$demo_rows_and_columns

See how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.

Learn more about Columns in the [docs](https://gradio.app/docs/column).

# Fill Browser Height / Width

To make an app take the full width of the browser by removing the side padding, use `gr.Blocks(fill_width=True)`. 

To make top level Components expand to take the full height of the browser, use `fill_height` and apply scale to the expanding Components.

```python
import gradio as gr

with gr.Blocks(fill_height=True) as demo:
    gr.Chatbot(scale=1)
    gr.Textbox(scale=0)
```

## Dimensions

Some components support setting height and width. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element.

Below is an example illustrating the use of viewport width (vw):

```python
import gradio as gr

with gr.Blocks() as demo:
    im = gr.ImageEditor(width="50vw")

demo.launch()
```

## Tabs and Accordions

You can also create Tabs using the `with gr.Tab('tab_name'):` clause. Any component created inside of a `with gr.Tab('tab_name'):` context appears in that tab. Consecutive Tab clauses are grouped together so that a single tab can be selected at one time, and only the components within that Tab's context are shown.

For example:

$code_blocks_flipper
$demo_blocks_flipper

Also note the `gr.Accordion('label')` in this example. The Accordion is a layout that can be toggled open or closed. Like `Tabs`, it is a layout element that can selectively hide or show content. Any components that are defined inside of a `with gr.Accordion('label'):` will be hidden or shown when the accordion's toggle icon is clicked.

Learn more about [Tabs](https://gradio.app/docs/tab) and [Accordions](https://gradio.app/docs/accordion) in the docs.

## Sidebar

The sidebar is a collapsible panel that renders child components on the left side of the screen and can be expanded or collapsed.

For example:

$code_blocks_sidebar

Learn more about [Sidebar](https://gradio.app/docs/gradio/sidebar) in the docs.


## Multi-step walkthroughs

In order to provide a guided set of ordered steps, a controlled workflow, you can use the `Walkthrough` component with accompanying `Step` components.

The `Walkthrough` component has a visual style and user experience tailored for this usecase.

Authoring this component is very similar to `Tab`, except it is the app developers responsibility to progress through each step, by setting the appropriate ID for the parent `Walkthrough` which should correspond to an ID provided to an indvidual `Step`. 

$demo_walkthrough

Learn more about [Walkthrough](https://gradio.app/docs/gradio/walkthrough) in the docs.


## Visibility

Both Components and Layout elements have a `visible` argument that can set initially and also updated. Setting `gr.Column(visible=...)` on a Column can be used to show or hide a set of Components.

$code_blocks_form
$demo_blocks_form

## Defining and Rendering Components Separately

In some cases, you might want to define components before you actually render them in your UI. For instance, you might want to show an examples section using `gr.Examples` above the corresponding `gr.Textbox` input. Since `gr.Examples` requires as a parameter the input component object, you will need to first define the input component, but then render it later, after you have defined the `gr.Examples` object.

The solution to this is to define the `gr.Textbox` outside of the `gr.Blocks()` scope and use the component's `.render()` method wherever you'd like it placed in the UI.

Here's a full code example:

```python
input_textbox = gr.Textbox()

with gr.Blocks() as demo:
    gr.Examples(["hello", "bonjour", "merhaba"], input_textbox)
    input_textbox.render()
```

Similarly, if you have already defined a component in a Gradio app, but wish to unrender it so that you can define in a different part of your application, then you can call the `.unrender()` method. In the following example, the `Textbox` will appear in the third column:

```py
import gradio as gr

with gr.Blocks() as demo:
    with gr.Row():
        with gr.Column():
            gr.Markdown("Row 1")
            textbox = gr.Textbox()
        with gr.Column():
            gr.Markdown("Row 2")
            textbox.unrender()
        with gr.Column():
            gr.Markdown("Row 3")
            textbox.render()

demo.launch()
```

---

<!-- Source: guides/04_additional-features/02_streaming-outputs.md -->
# Streaming outputs

In some cases, you may want to stream a sequence of outputs rather than show a single output at once. For example, you might have an image generation model and you want to show the image that is generated at each step, leading up to the final image. Or you might have a chatbot which streams its response one token at a time instead of returning it all at once.

In such cases, you can supply a **generator** function into Gradio instead of a regular function. Creating generators in Python is very simple: instead of a single `return` value, a function should `yield` a series of values instead. Usually the `yield` statement is put in some kind of loop. Here's an example of an generator that simply counts up to a given number:

```python
def my_generator(x):
    for i in range(x):
        yield i
```

You supply a generator into Gradio the same way as you would a regular function. For example, here's a a (fake) image generation model that generates noise for several steps before outputting an image using the `gr.Interface` class:

$code_fake_diffusion
$demo_fake_diffusion

Note that we've added a `time.sleep(1)` in the iterator to create an artificial pause between steps so that you are able to observe the steps of the iterator (in a real image generation model, this probably wouldn't be necessary).

Similarly, Gradio can handle streaming inputs, e.g. an image generation model that reruns every time a user types a letter in a textbox. This is covered in more details in our guide on building [reactive Interfaces](/guides/reactive-interfaces). 

## Streaming Media

Gradio can stream audio and video directly from your generator function.
This lets your user hear your audio or see your video nearly as soon as it's `yielded` by your function.
All you have to do is 

1. Set `streaming=True` in your `gr.Audio` or `gr.Video` output component.
2. Write a python generator that yields the next "chunk" of audio or video.
3. Set `autoplay=True` so that the media starts playing automatically.

For audio, the next "chunk" can be either an `.mp3` or `.wav` file or a `bytes` sequence of audio.
For video, the next "chunk" has to be either `.mp4` file or a file with `h.264` codec with a `.ts` extension.
For smooth playback, make sure chunks are consistent lengths and larger than 1 second.

We'll finish with some simple examples illustrating these points.

### Streaming Audio

```python
import gradio as gr
from time import sleep

def keep_repeating(audio_file):
    for _ in range(10):
        sleep(0.5)
        yield audio_file

gr.Interface(keep_repeating,
             gr.Audio(sources=["microphone"], type="filepath"),
             gr.Audio(streaming=True, autoplay=True)
).launch()
```

### Streaming Video

```python
import gradio as gr
from time import sleep

def keep_repeating(video_file):
    for _ in range(10):
        sleep(0.5)
        yield video_file

gr.Interface(keep_repeating,
             gr.Video(sources=["webcam"], format="mp4"),
             gr.Video(streaming=True, autoplay=True)
).launch()
```

## End-to-End Examples

For an end-to-end example of streaming media, see the object detection from video [guide](/main/guides/object-detection-from-video) or the streaming AI-generated audio with [transformers](https://huggingface.co/docs/transformers/index) [guide](/main/guides/streaming-ai-generated-audio).

---

<!-- Source: guides/05_chatbots/02_chatinterface-examples.md -->
# Using Popular LLM libraries and APIs

Tags: LLM, CHATBOT, API

In this Guide, we go through several examples of how to use `gr.ChatInterface` with popular LLM libraries and API providers.

We will cover the following libraries and API providers:

* [Llama Index](#llama-index)
* [LangChain](#lang-chain)
* [OpenAI](#open-ai)
* [Hugging Face `transformers`](#hugging-face-transformers)
* [SambaNova](#samba-nova)
* [Hyperbolic](#hyperbolic)
* [Anthropic's Claude](#anthropics-claude)

For many LLM libraries and providers, there exist community-maintained integration libraries that make it even easier to spin up Gradio apps. We reference these libraries in the appropriate sections below.

## Llama Index

Let's start by using `llama-index` on top of `openai` to build a RAG chatbot on any text or PDF files that you can demo and share in less than 30 lines of code. You'll need to have an OpenAI key for this example (keep reading for the free, open-source equivalent!)

$code_llm_llamaindex

## LangChain

Here's an example using `langchain` on top of `openai` to build a general-purpose chatbot. As before, you'll need to have an OpenAI key for this example.

$code_llm_langchain

Tip: For quick prototyping, the community-maintained <a href='https://github.com/AK391/langchain-gradio'>langchain-gradio repo</a>  makes it even easier to build chatbots on top of LangChain.

## OpenAI

Of course, we could also use the `openai` library directy. Here a similar example to the LangChain , but this time with streaming as well:

Tip: For quick prototyping, the  <a href='https://github.com/gradio-app/openai-gradio'>openai-gradio library</a> makes it even easier to build chatbots on top of OpenAI models.


## Hugging Face `transformers`

Of course, in many cases you want to run a chatbot locally. Here's the equivalent example using the SmolLM2-135M-Instruct model using the Hugging Face `transformers` library.

$code_llm_hf_transformers

## SambaNova

The SambaNova Cloud API provides access to full-precision open-source models, such as the Llama family. Here's an example of how to build a Gradio app around the SambaNova API

$code_llm_sambanova

Tip: For quick prototyping, the  <a href='https://github.com/gradio-app/sambanova-gradio'>sambanova-gradio library</a> makes it even easier to build chatbots on top of SambaNova models.

## Hyperbolic

The Hyperbolic AI API provides access to many open-source models, such as the Llama family. Here's an example of how to build a Gradio app around the Hyperbolic

$code_llm_hyperbolic

Tip: For quick prototyping, the  <a href='https://github.com/HyperbolicLabs/hyperbolic-gradio'>hyperbolic-gradio library</a> makes it even easier to build chatbots on top of Hyperbolic models.


## Anthropic's Claude 

Anthropic's Claude model can also be used via API. Here's a simple 20 questions-style game built on top of the Anthropic API:

$code_llm_claude

---

<!-- Source: guides/06_data-science-and-plots/02_time-plots.md -->
# Time Plots

Creating visualizations with a time x-axis is a common use case. Let's dive in!

## Creating a Plot with a pd.Dataframe

Time plots need a datetime column on the x-axis. Here's a simple example with some flight data:

$code_plot_guide_temporal
$demo_plot_guide_temporal

## Aggregating by Time

You may wish to bin data by time buckets. Use `x_bin` to do so, using a string suffix with "s", "m", "h" or "d", such as "15m" or "1d".

$code_plot_guide_aggregate_temporal
$demo_plot_guide_aggregate_temporal

## DateTime Components

You can use `gr.DateTime` to accept input datetime data. This works well with plots for defining the x-axis range for the data.

$code_plot_guide_datetime
$demo_plot_guide_datetime

Note how `gr.DateTime` can accept a full datetime string, or a shorthand using `now - [0-9]+[smhd]` format to refer to a past time.

You will often have many time plots in which case you'd like to keep the x-axes in sync. The `DateTimeRange` custom component keeps a set of datetime plots in sync, and also uses the `.select` listener of plots to allow you to zoom into plots while keeping plots in sync. 

Because it is a custom component, you first need to `pip install gradio_datetimerange`. Then run the following:

$code_plot_guide_datetimerange
$demo_plot_guide_datetimerange

Try zooming around in the plots and see how DateTimeRange updates. All the plots updates their `x_lim` in sync. You also have a "Back" link in the component to allow you to quickly zoom in and out.

## RealTime Data

In many cases, you're working with live, realtime date, not a static dataframe. In this case, you'd update the plot regularly with a `gr.Timer()`. Assuming there's a `get_data` method that gets the latest dataframe:

```python
with gr.Blocks() as demo:
    timer = gr.Timer(5)
    plot1 = gr.BarPlot(x="time", y="price")
    plot2 = gr.BarPlot(x="time", y="price", color="origin")

    timer.tick(lambda: [get_data(), get_data()], outputs=[plot1, plot2])
```

You can also use the `every` shorthand to attach a `Timer` to a component that has a function value:

```python
with gr.Blocks() as demo:
    timer = gr.Timer(5)
    plot1 = gr.BarPlot(get_data, x="time", y="price", every=timer)
    plot2 = gr.BarPlot(get_data, x="time", y="price", color="origin", every=timer)
```

---

<!-- Source: guides/07_streaming/02_object-detection-from-webcam-with-webrtc.md -->
# Real Time Object Detection from a Webcam Stream with FastRTC

Tags: VISION, STREAMING, WEBCAM

In this guide, we'll use YOLOv10 to perform real-time object detection in Gradio from a user's webcam feed. We'll utilize [FastRTC](https://fastrtc.org/) a companion library from the gradio team for building low latency streaming web applications. You can see the finished product in action below:

<video src="https://github.com/user-attachments/assets/4584cec6-8c1a-401b-9b61-a4fe0718b558" controls
height="600" width="600" style="display: block; margin: auto;" autoplay="true" loop="true">
</video>

## Setting up

Start by installing all the dependencies. Add the following lines to a `requirements.txt` file and run `pip install -r requirements.txt`:

```bash
opencv-python
fastrtc
onnxruntime-gpu
```

We'll use the ONNX runtime to speed up YOLOv10 inference. This guide assumes you have access to a GPU. If you don't, change `onnxruntime-gpu` to `onnxruntime`. Without a GPU, the model will run slower, resulting in a laggy demo.

We'll use OpenCV for image manipulation and the [WebRTC](https://webrtc.org/) protocol to achieve near-zero latency.

**Note**: If you want to deploy this app on any cloud provider, you'll need to use your Hugging Face token to connect to a TURN server. Learn more in this [guide](https://fastrtc.org/deployment/). If you're not familiar with TURN servers, consult this [guide](https://www.twilio.com/docs/stun-turn/faq#faq-what-is-nat).

## The Inference Function

We'll download the YOLOv10 model from the Hugging Face hub and instantiate a custom inference class to use this model. 

The implementation of the inference class isn't covered in this guide, but you can find the source code [here](https://huggingface.co/spaces/freddyaboulton/webrtc-yolov10n/blob/main/inference.py#L9) if you're interested. This implementation borrows heavily from this [github repository](https://github.com/ibaiGorordo/ONNX-YOLOv8-Object-Detection).

We're using the `yolov10-n` variant because it has the lowest latency. See the [Performance](https://github.com/THU-MIG/yolov10?tab=readme-ov-file#performance) section of the README in the YOLOv10 GitHub repository.

```python
from huggingface_hub import hf_hub_download
from inference import YOLOv10

model_file = hf_hub_download(
    repo_id="onnx-community/yolov10n", filename="onnx/model.onnx"
)

model = YOLOv10(model_file)

def detection(image, conf_threshold=0.3):
    image = cv2.resize(image, (model.input_width, model.input_height))
    new_image = model.detect_objects(image, conf_threshold)
    return new_image
```

Our inference function, `detection`, accepts a numpy array from the webcam and a desired confidence threshold. Object detection models like YOLO identify many objects and assign a confidence score to each. The lower the confidence, the higher the chance of a false positive. We'll let users adjust the confidence threshold.

The function returns a numpy array corresponding to the same input image with all detected objects in bounding boxes.

## The Gradio Demo

The Gradio demo is straightforward, but we'll implement a few specific features:

1. Use the `WebRTC` custom component to ensure input and output are sent to/from the server with WebRTC. 
2. The [WebRTC](https://github.com/freddyaboulton/gradio-webrtc) component will serve as both an input and output component.
3. Utilize the `time_limit` parameter of the `stream` event. This parameter sets a processing time for each user's stream. In a multi-user setting, such as on Spaces, we'll stop processing the current user's stream after this period and move on to the next. 

We'll also apply custom CSS to center the webcam and slider on the page.

```python
import gradio as gr
from fastrtc import WebRTC

css = """.my-group {max-width: 600px !important; max-height: 600px !important;}
         .my-column {display: flex !important; justify-content: center !important; align-items: center !important;}"""

with gr.Blocks(css=css) as demo:
    gr.HTML(
        """
        <h1 style='text-align: center'>
        YOLOv10 Webcam Stream (Powered by WebRTC âš¡ï¸)
        </h1>
        """
    )
    with gr.Column(elem_classes=["my-column"]):
        with gr.Group(elem_classes=["my-group"]):
            image = WebRTC(label="Stream", rtc_configuration=rtc_configuration)
            conf_threshold = gr.Slider(
                label="Confidence Threshold",
                minimum=0.0,
                maximum=1.0,
                step=0.05,
                value=0.30,
            )

        image.stream(
            fn=detection, inputs=[image, conf_threshold], outputs=[image], time_limit=10
        )

if __name__ == "__main__":
    demo.launch()
```

## Conclusion

Our app is hosted on Hugging Face Spaces [here](https://huggingface.co/spaces/freddyaboulton/webrtc-yolov10n). 

You can use this app as a starting point to build real-time image applications with Gradio. Don't hesitate to open issues in the space or in the [FastRTC GitHub repo](https://github.com/gradio-app/fastrtc) if you have any questions or encounter problems.

---

<!-- Source: guides/08_custom-components/02_key-component-concepts.md -->
# Gradio Components: The Key Concepts

In this section, we discuss a few important concepts when it comes to components in Gradio.
It's important to understand these concepts when developing your own component.
Otherwise, your component may behave very different to other Gradio components!

Tip:  You can skip this section if you are familiar with the internals of the Gradio library, such as each component's preprocess and postprocess methods.

## Interactive vs Static

Every component in Gradio comes in a `static` variant, and most come in an `interactive` version as well.
The `static` version is used when a component is displaying a value, and the user can **NOT** change that value by interacting with it. 
The `interactive` version is used when the user is able to change the value by interacting with the Gradio UI.

Let's see some examples:

```python
import gradio as gr

with gr.Blocks() as demo:
   gr.Textbox(value="Hello", interactive=True)
   gr.Textbox(value="Hello", interactive=False)

demo.launch()

```
This will display two textboxes.
The only difference: you'll be able to edit the value of the Gradio component on top, and you won't be able to edit the variant on the bottom (i.e. the textbox will be disabled).

Perhaps a more interesting example is with the `Image` component:

```python
import gradio as gr

with gr.Blocks() as demo:
   gr.Image(interactive=True)
   gr.Image(interactive=False)

demo.launch()
```

The interactive version of the component is much more complex -- you can upload images or snap a picture from your webcam -- while the static version can only be used to display images.

Not every component has a distinct interactive version. For example, the `gr.AnnotatedImage` only appears as a static version since there's no way to interactively change the value of the annotations or the image.

### What you need to remember

* Gradio will use the interactive version (if available) of a component if that component is used as the **input** to any event; otherwise, the static version will be used.

* When you design custom components, you **must** accept the boolean interactive keyword in the constructor of your Python class. In the frontend, you **may** accept the `interactive` property, a `bool` which represents whether the component should be static or interactive. If you do not use this property in the frontend, the component will appear the same in interactive or static mode.

## The value and how it is preprocessed/postprocessed

The most important attribute of a component is its `value`.
Every component has a `value`.
The value that is typically set by the user in the frontend (if the component is interactive) or displayed to the user (if it is static). 
It is also this value that is sent to the backend function when a user triggers an event, or returned by the user's function e.g. at the end of a prediction.

So this value is passed around quite a bit, but sometimes the format of the value needs to change between the frontend and backend. 
Take a look at this example:

```python
import numpy as np
import gradio as gr

def sepia(input_img):
    sepia_filter = np.array([
        [0.393, 0.769, 0.189], 
        [0.349, 0.686, 0.168], 
        [0.272, 0.534, 0.131]
    ])
    sepia_img = input_img.dot(sepia_filter.T)
    sepia_img /= sepia_img.max()
    return sepia_img

demo = gr.Interface(sepia, gr.Image(width=200, height=200), "image")
demo.launch()
```

This will create a Gradio app which has an `Image` component as the input and the output. 
In the frontend, the Image component will actually **upload** the file to the server and send the **filepath** but this is converted to a `numpy` array before it is sent to a user's function. 
Conversely, when the user returns a `numpy` array from their function, the numpy array is converted to a file so that it can be sent to the frontend and displayed by the `Image` component.

Tip: By default, the `Image` component sends numpy arrays to the python function because it is a common choice for machine learning engineers, though the Image component also supports other formats using the `type` parameter.  Read the `Image` docs [here](https://www.gradio.app/docs/image) to learn more.

Each component does two conversions:

1. `preprocess`: Converts the `value` from the format sent by the frontend to the format expected by the python function. This usually involves going from a web-friendly **JSON** structure to a **python-native** data structure, like a `numpy` array or `PIL` image. The `Audio`, `Image` components are good examples of `preprocess` methods.

2. `postprocess`: Converts the value returned by the python function to the format expected by the frontend. This usually involves going from a **python-native** data-structure, like a `PIL` image to a **JSON** structure.

### What you need to remember

* Every component must implement `preprocess` and `postprocess` methods. In the rare event that no conversion needs to happen, simply return the value as-is. `Textbox` and `Number` are examples of this. 

* As a component author, **YOU** control the format of the data displayed in the frontend as well as the format of the data someone using your component will receive. Think of an ergonomic data-structure a **python** developer will find intuitive, and control the conversion from a **Web-friendly JSON** data structure (and vice-versa) with `preprocess` and `postprocess.`

## The "Example Version" of a Component

Gradio apps support providing example inputs -- and these are very useful in helping users get started using your Gradio app. 
In `gr.Interface`, you can provide examples using the `examples` keyword, and in `Blocks`, you can provide examples using the special `gr.Examples` component.

At the bottom of this screenshot, we show a miniature example image of a cheetah that, when clicked, will populate the same image in the input Image component:

![img](https://user-images.githubusercontent.com/1778297/277548211-a3cb2133-2ffc-4cdf-9a83-3e8363b57ea6.png)


To enable the example view, you must have the following two files in the top of the `frontend` directory:

* `Example.svelte`: this corresponds to the "example version" of your component
* `Index.svelte`: this corresponds to the "regular version"

In the backend, you typically don't need to do anything. The user-provided example `value` is processed using the same `.postprocess()` method described earlier. If you'd like to do process the data differently (for example, if the `.postprocess()` method is computationally expensive), then you can write your own `.process_example()` method for your custom component, which will be used instead. 

The `Example.svelte` file and `process_example()` method will be covered in greater depth in the dedicated [frontend](./frontend) and [backend](./backend) guides respectively.

### What you need to remember

* If you expect your component to be used as input, it is important to define an "Example" view.
* If you don't, Gradio will use a default one but it won't be as informative as it can be!

## Conclusion

Now that you know the most important pieces to remember about Gradio components, you can start to design and build your own!

---

<!-- Source: guides/09_gradio-clients-and-lite/02_getting-started-with-the-js-client.md -->
# Getting Started with the Gradio JavaScript Client

Tags: CLIENT, API, SPACES

The Gradio JavaScript Client makes it very easy to use any Gradio app as an API. As an example, consider this [Hugging Face Space that transcribes audio files](https://huggingface.co/spaces/abidlabs/whisper) that are recorded from the microphone.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)

Using the `@gradio/client` library, we can easily use the Gradio as an API to transcribe audio files programmatically.

Here's the entire code to do it:

```js
import { Client, handle_file } from "@gradio/client";

const response = await fetch(
	"https://github.com/audio-samples/audio-samples.github.io/raw/master/samples/wav/ted_speakers/SalmanKhan/sample-1.wav"
);
const audio_file = await response.blob();

const app = await Client.connect("abidlabs/whisper");
const transcription = await app.predict("/predict", [handle_file(audio_file)]);

console.log(transcription.data);
// [ "I said the same phrase 30 times." ]
```

The Gradio Client works with any hosted Gradio app, whether it be an image generator, a text summarizer, a stateful chatbot, a tax calculator, or anything else! The Gradio Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), but your app can be hosted anywhere, such as your own server.

**Prequisites**: To use the Gradio client, you do _not_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components.

## Installation via npm

Install the @gradio/client package to interact with Gradio APIs using Node.js version >=18.0.0 or in browser-based projects. Use npm or any compatible package manager:

```bash
npm i @gradio/client
```

This command adds @gradio/client to your project dependencies, allowing you to import it in your JavaScript or TypeScript files.

## Installation via CDN

For quick addition to your web project, you can use the jsDelivr CDN to load the latest version of @gradio/client directly into your HTML:

```html
<script type="module">
	import { Client } from "https://cdn.jsdelivr.net/npm/@gradio/client/dist/index.min.js";
	...
</script>
```

Be sure to add this to the `<head>` of your HTML. This will install the latest version but we advise hardcoding the version in production. You can find all available versions [here](https://www.jsdelivr.com/package/npm/@gradio/client). This approach is ideal for experimental or prototying purposes, though has some limitations. A complete example would look like this:

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <script type="module">
        import { Client } from "https://cdn.jsdelivr.net/npm/@gradio/client/dist/index.min.js";
        const client = await Client.connect("abidlabs/en2fr");
        const result = await client.predict("/predict", {
            text: "My name is Hannah"
        });
        console.log(result);
    </script>
</head>
</html>
```

## Connecting to a running Gradio App

Start by connecting instantiating a `client` instance and connecting it to a Gradio app that is running on Hugging Face Spaces or generally anywhere on the web.

## Connecting to a Hugging Face Space

```js
import { Client } from "@gradio/client";

const app = await Client.connect("abidlabs/en2fr"); // a Space that translates from English to French
```

You can also connect to private Spaces by passing in your HF token with the `token` property of the options parameter. You can get your HF token here: https://huggingface.co/settings/tokens

```js
import { Client } from "@gradio/client";

const app = await Client.connect("abidlabs/my-private-space", { token: "hf_..." })
```

## Duplicating a Space for private use

While you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space, and then use it to make as many requests as you'd like! You'll need to pass in your [Hugging Face token](https://huggingface.co/settings/tokens)).

`Client.duplicate` is almost identical to `Client.connect`, the only difference is under the hood:

```js
import { Client, handle_file } from "@gradio/client";

const response = await fetch(
	"https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3"
);
const audio_file = await response.blob();

const app = await Client.duplicate("abidlabs/whisper", { token: "hf_..." });
const transcription = await app.predict("/predict", [handle_file(audio_file)]);
```

If you have previously duplicated a Space, re-running `Client.duplicate` will _not_ create a new Space. Instead, the client will attach to the previously-created Space. So it is safe to re-run the `Client.duplicate` method multiple times with the same space.

**Note:** if the original Space uses GPUs, your private Space will as well, and your Hugging Face account will get billed based on the price of the GPU. To minimize charges, your Space will automatically go to sleep after 5 minutes of inactivity. You can also set the hardware using the `hardware` and `timeout` properties of `duplicate`'s options object like this:

```js
import { Client } from "@gradio/client";

const app = await Client.duplicate("abidlabs/whisper", {
	token: "hf_...",
	timeout: 60,
	hardware: "a10g-small"
});
```

## Connecting a general Gradio app

If your app is running somewhere else, just provide the full URL instead, including the "http://" or "https://". Here's an example of making predictions to a Gradio app that is running on a share URL:

```js
import { Client } from "@gradio/client";

const app = Client.connect("https://bec81a83-5b5c-471e.gradio.live");
```

## Connecting to a Gradio app with auth

If the Gradio application you are connecting to [requires a username and password](/guides/sharing-your-app#authentication), then provide them as a tuple to the `auth` argument of the `Client` class:

```js
import { Client } from "@gradio/client";

Client.connect(
  space_name,
  { auth: [username, password] }
)
```


## Inspecting the API endpoints

Once you have connected to a Gradio app, you can view the APIs that are available to you by calling the `Client`'s `view_api` method.

For the Whisper Space, we can do this:

```js
import { Client } from "@gradio/client";

const app = await Client.connect("abidlabs/whisper");

const app_info = await app.view_api();

console.log(app_info);
```

And we will see the following:

```json
{
	"named_endpoints": {
		"/predict": {
			"parameters": [
				{
					"label": "text",
					"component": "Textbox",
					"type": "string"
				}
			],
			"returns": [
				{
					"label": "output",
					"component": "Textbox",
					"type": "string"
				}
			]
		}
	},
	"unnamed_endpoints": {}
}
```

This shows us that we have 1 API endpoint in this space, and shows us how to use the API endpoint to make a prediction: we should call the `.predict()` method (which we will explore below), providing a parameter `input_audio` of type `string`, which is a url to a file.

We should also provide the `api_name='/predict'` argument to the `predict()` method. Although this isn't necessary if a Gradio app has only 1 named endpoint, it does allow us to call different endpoints in a single app if they are available. If an app has unnamed API endpoints, these can also be displayed by running `.view_api(all_endpoints=True)`.

## The "View API" Page

As an alternative to running the `.view_api()` method, you can click on the "Use via API" link in the footer of the Gradio app, which shows us the same information, along with example usage. 

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api.png)

The View API page also includes an "API Recorder" that lets you interact with the Gradio UI normally and converts your interactions into the corresponding code to run with the JS Client.


## Making a prediction

The simplest way to make a prediction is simply to call the `.predict()` method with the appropriate arguments:

```js
import { Client } from "@gradio/client";

const app = await Client.connect("abidlabs/en2fr");
const result = await app.predict("/predict", ["Hello"]);
```

If there are multiple parameters, then you should pass them as an array to `.predict()`, like this:

```js
import { Client } from "@gradio/client";

const app = await Client.connect("gradio/calculator");
const result = await app.predict("/predict", [4, "add", 5]);
```

For certain inputs, such as images, you should pass in a `Buffer`, `Blob` or `File` depending on what is most convenient. In node, this would be a `Buffer` or `Blob`; in a browser environment, this would be a `Blob` or `File`.

```js
import { Client, handle_file } from "@gradio/client";

const response = await fetch(
	"https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3"
);
const audio_file = await response.blob();

const app = await Client.connect("abidlabs/whisper");
const result = await app.predict("/predict", [handle_file(audio_file)]);
```

## Using events

If the API you are working with can return results over time, or you wish to access information about the status of a job, you can use the iterable interface for more flexibility. This is especially useful for iterative endpoints or generator endpoints that will produce a series of values over time as discrete responses.

```js
import { Client } from "@gradio/client";

function log_result(payload) {
	const {
		data: [translation]
	} = payload;

	console.log(`The translated result is: ${translation}`);
}

const app = await Client.connect("abidlabs/en2fr");
const job = app.submit("/predict", ["Hello"]);

for await (const message of job) {
	log_result(message);
}
```

## Status

The event interface also allows you to get the status of the running job by instantiating the client with the `events` options passing `status` and `data` as an array:


```ts
import { Client } from "@gradio/client";

const app = await Client.connect("abidlabs/en2fr", {
	events: ["status", "data"]
});
```

This ensures that status messages are also reported to the client.

`status`es are returned as an object with the following attributes: `status` (a human readbale status of the current job, `"pending" | "generating" | "complete" | "error"`), `code` (the detailed gradio code for the job), `position` (the current position of this job in the queue), `queue_size` (the total queue size), `eta` (estimated time this job will complete), `success` (a boolean representing whether the job completed successfully), and `time` ( as `Date` object detailing the time that the status was generated).

```js
import { Client } from "@gradio/client";

function log_status(status) {
	console.log(
		`The current status for this job is: ${JSON.stringify(status, null, 2)}.`
	);
}

const app = await Client.connect("abidlabs/en2fr", {
	events: ["status", "data"]
});
const job = app.submit("/predict", ["Hello"]);

for await (const message of job) {
	if (message.type === "status") {
		log_status(message);
	}
}
```

## Cancelling Jobs

The job instance also has a `.cancel()` method that cancels jobs that have been queued but not started. For example, if you run:

```js
import { Client } from "@gradio/client";

const app = await Client.connect("abidlabs/en2fr");
const job_one = app.submit("/predict", ["Hello"]);
const job_two = app.submit("/predict", ["Friends"]);

job_one.cancel();
job_two.cancel();
```

If the first job has started processing, then it will not be canceled but the client will no longer listen for updates (throwing away the job). If the second job has not yet started, it will be successfully canceled and removed from the queue.

## Generator Endpoints

Some Gradio API endpoints do not return a single value, rather they return a series of values. You can listen for these values in real time using the iterable interface:

```js
import { Client } from "@gradio/client";

const app = await Client.connect("gradio/count_generator");
const job = app.submit(0, [9]);

for await (const message of job) {
	console.log(message.data);
}
```

This will log out the values as they are generated by the endpoint.

You can also cancel jobs that that have iterative outputs, in which case the job will finish immediately.

```js
import { Client } from "@gradio/client";

const app = await Client.connect("gradio/count_generator");
const job = app.submit(0, [9]);

for await (const message of job) {
	console.log(message.data);
}

setTimeout(() => {
	job.cancel();
}, 3000);
```

---

<!-- Source: guides/10_mcp/02_file-upload-mcp.md -->
# The File Upload MCP Server

Tags: MCP, TOOL, LLM, SERVER, DOCS

If you've tried to to use a remote Gradio MCP server that takes a file as input (image, video, audio), you've probably run into this error:

<img src="https://huggingface.co/datasets/freddyaboulton/bucket/resolve/main/MCPError.png">

The reason is that since the Gradio server is hosted on a different machine, any input files must be available via a public URL so that they can downloaded in the remote machine.

There are many ways to host files on the internet, but they all require adding a manual step to your workflow. In the age of LLM agents, shouldn't we expect them to handle this step for you?

In this post, we'll show how you can connect your LLM to the "File Upload" MCP server so that it can handle the file uploading for you when appropriate!

## Using the File Upload MCP Server

As of version 5.36.0, Gradio now comes with a built-in MCP server that can upload files to a running Gradio application. In the `View API` page of the server, you should see the following code snippet if any of the tools require file inputs:

<img src="https://huggingface.co/datasets/freddyaboulton/bucket/resolve/main/MCPConnectionDocs.png">

The command to start the MCP server takes two arguments:

- The URL (or Hugging Face space id) of the gradio application to upload the files to. In this case, `http://127.0.0.1:7860`.
- The local directory on your computer with which the server is allowed to upload files from (`<UPLOAD_DIRECTORY>`). For security, please make this directory as narrow as possible to prevent unintended file uploads.

As stated in the image, you need to install [uv](https://docs.astral.sh/uv/getting-started/installation/) (a python package manager that can run python scripts) before connecting from your MCP client. 

If you have gradio installed locally and you don't want to install uv, you can replace the `uvx` command with the path to gradio binary. It should look like this:

```json
"upload-files": {
    "command": "<absoluate-path-to-gradio>",
    "args": [
    "upload-mcp",
    "http://localhost:7860/",
    "/Users/freddyboulton/Pictures"
    ]
}
```

After connecting to the upload server, your LLM agent will know when to upload files for you automatically!

<img src="https://huggingface.co/datasets/freddyaboulton/bucket/resolve/main/Ghibliafy.png">

## Conclusion

In this guide, we've covered how you can connect to the Upload File MCP Server so that your agent can upload files before using Gradio MCP servers. Remember to set the `<UPLOAD_DIRECTORY>` as small as possible to prevent unintended file uploads!

---

<!-- Source: guides/cn/01_getting-started/02_key-features.md -->
# ä¸»è¦ç‰¹ç‚¹

è®©æˆ‘ä»¬æ¥ä»‹ç»ä¸€ä¸‹ Gradio æœ€å—æ¬¢è¿Žçš„ä¸€äº›åŠŸèƒ½ï¼è¿™é‡Œæ˜¯ Gradio çš„ä¸»è¦ç‰¹ç‚¹ï¼š

1. [æ·»åŠ ç¤ºä¾‹è¾“å…¥](#example-inputs)
2. [ä¼ é€’è‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯](#errors)
3. [æ·»åŠ æè¿°å†…å®¹](#descriptive-content)
4. [è®¾ç½®æ——æ ‡](#flagging)
5. [é¢„å¤„ç†å’ŒåŽå¤„ç†](#preprocessing-and-postprocessing)
6. [æ ·å¼åŒ–æ¼”ç¤º](#styling)
7. [æŽ’é˜Ÿç”¨æˆ·](#queuing)
8. [è¿­ä»£è¾“å‡º](#iterative-outputs)
9. [è¿›åº¦æ¡](#progress-bars)
10. [æ‰¹å¤„ç†å‡½æ•°](#batch-functions)
11. [åœ¨åä½œç¬”è®°æœ¬ä¸Šè¿è¡Œ](#colab-notebooks)

## ç¤ºä¾‹è¾“å…¥

æ‚¨å¯ä»¥æä¾›ç”¨æˆ·å¯ä»¥è½»æ¾åŠ è½½åˆ° "Interface" ä¸­çš„ç¤ºä¾‹æ•°æ®ã€‚è¿™å¯¹äºŽæ¼”ç¤ºæ¨¡åž‹æœŸæœ›çš„è¾“å…¥ç±»åž‹ä»¥åŠæ¼”ç¤ºæ•°æ®é›†å’Œæ¨¡åž‹ä¸€èµ·æŽ¢ç´¢çš„æ–¹å¼éžå¸¸æœ‰å¸®åŠ©ã€‚è¦åŠ è½½ç¤ºä¾‹æ•°æ®ï¼Œæ‚¨å¯ä»¥å°†åµŒå¥—åˆ—è¡¨æä¾›ç»™ Interface æž„é€ å‡½æ•°çš„ `examples=` å…³é”®å­—å‚æ•°ã€‚å¤–éƒ¨åˆ—è¡¨ä¸­çš„æ¯ä¸ªå­åˆ—è¡¨è¡¨ç¤ºä¸€ä¸ªæ•°æ®æ ·æœ¬ï¼Œå­åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ è¡¨ç¤ºæ¯ä¸ªè¾“å…¥ç»„ä»¶çš„è¾“å…¥ã€‚æœ‰å…³æ¯ä¸ªç»„ä»¶çš„ç¤ºä¾‹æ•°æ®æ ¼å¼åœ¨[Docs](https://gradio.app/docs/components)ä¸­æœ‰è¯´æ˜Žã€‚

$code_calculator
$demo_calculator

æ‚¨å¯ä»¥å°†å¤§åž‹æ•°æ®é›†åŠ è½½åˆ°ç¤ºä¾‹ä¸­ï¼Œé€šè¿‡ Gradio æµè§ˆå’Œä¸Žæ•°æ®é›†è¿›è¡Œäº¤äº’ã€‚ç¤ºä¾‹å°†è‡ªåŠ¨åˆ†é¡µï¼ˆå¯ä»¥é€šè¿‡ Interface çš„ `examples_per_page` å‚æ•°è¿›è¡Œé…ç½®ï¼‰ã€‚

ç»§ç»­äº†è§£ç¤ºä¾‹ï¼Œè¯·å‚é˜…[æ›´å¤šç¤ºä¾‹](https://gradio.app/more-on-examples)æŒ‡å—ã€‚

## é”™è¯¯

æ‚¨å¸Œæœ›å‘ç”¨æˆ·ä¼ é€’è‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯ã€‚ä¸ºæ­¤ï¼Œwith `gr.Error("custom message")` æ¥æ˜¾ç¤ºé”™è¯¯æ¶ˆæ¯ã€‚å¦‚æžœåœ¨ä¸Šé¢çš„è®¡ç®—å™¨ç¤ºä¾‹ä¸­å°è¯•é™¤ä»¥é›¶ï¼Œå°†æ˜¾ç¤ºè‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯çš„å¼¹å‡ºæ¨¡æ€çª—å£ã€‚äº†è§£æœ‰å…³é”™è¯¯çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://gradio.app/docs/error)ã€‚

## æè¿°æ€§å†…å®¹

åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­ï¼Œæ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ° Interface æž„é€ å‡½æ•°ä¸­çš„ `title=` å’Œ `description=` å…³é”®å­—å‚æ•°ï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£æ‚¨çš„åº”ç”¨ç¨‹åºã€‚

Interface æž„é€ å‡½æ•°ä¸­æœ‰ä¸‰ä¸ªå‚æ•°ç”¨äºŽæŒ‡å®šæ­¤å†…å®¹åº”æ”¾ç½®åœ¨å“ªé‡Œï¼š

- `title`ï¼šæŽ¥å—æ–‡æœ¬ï¼Œå¹¶å¯ä»¥å°†å…¶æ˜¾ç¤ºåœ¨ç•Œé¢çš„é¡¶éƒ¨ï¼Œä¹Ÿå°†æˆä¸ºé¡µé¢æ ‡é¢˜ã€‚
- `description`ï¼šæŽ¥å—æ–‡æœ¬ã€Markdown æˆ– HTMLï¼Œå¹¶å°†å…¶æ”¾ç½®åœ¨æ ‡é¢˜æ­£ä¸‹æ–¹ã€‚
- `article`ï¼šä¹ŸæŽ¥å—æ–‡æœ¬ã€Markdown æˆ– HTMLï¼Œå¹¶å°†å…¶æ”¾ç½®åœ¨ç•Œé¢ä¸‹æ–¹ã€‚

![annotated](/assets/guides/annotated.png)

å¦‚æžœæ‚¨ä½¿ç”¨çš„æ˜¯ `Blocks` APIï¼Œåˆ™å¯ä»¥ with `gr.Markdown(...)` æˆ– `gr.HTML(...)` ç»„ä»¶åœ¨ä»»ä½•ä½ç½®æ’å…¥æ–‡æœ¬ã€Markdown æˆ– HTMLï¼Œå…¶ä¸­æè¿°æ€§å†…å®¹ä½äºŽ `Component` æž„é€ å‡½æ•°å†…éƒ¨ã€‚

å¦ä¸€ä¸ªæœ‰ç”¨çš„å…³é”®å­—å‚æ•°æ˜¯ `label=`ï¼Œå®ƒå­˜åœ¨äºŽæ¯ä¸ª `Component` ä¸­ã€‚è¿™ä¿®æ”¹äº†æ¯ä¸ª `Component` é¡¶éƒ¨çš„æ ‡ç­¾æ–‡æœ¬ã€‚è¿˜å¯ä»¥ä¸ºè¯¸å¦‚ `Textbox` æˆ– `Radio` ä¹‹ç±»çš„è¡¨å•å…ƒç´ æ·»åŠ  `info=` å…³é”®å­—å‚æ•°ï¼Œä»¥æä¾›æœ‰å…³å…¶ç”¨æ³•çš„è¿›ä¸€æ­¥ä¿¡æ¯ã€‚

```python
gr.Number(label='å¹´é¾„', info='ä»¥å¹´ä¸ºå•ä½ï¼Œå¿…é¡»å¤§äºŽ0')
```

## æ——æ ‡

é»˜è®¤æƒ…å†µä¸‹ï¼Œ"Interface" å°†æœ‰ä¸€ä¸ª "Flag" æŒ‰é’®ã€‚å½“ç”¨æˆ·æµ‹è¯•æ‚¨çš„ `Interface` æ—¶ï¼Œå¦‚æžœçœ‹åˆ°æœ‰è¶£çš„è¾“å‡ºï¼Œä¾‹å¦‚é”™è¯¯æˆ–æ„å¤–çš„æ¨¡åž‹è¡Œä¸ºï¼Œä»–ä»¬å¯ä»¥å°†è¾“å…¥æ ‡è®°ä¸ºæ‚¨è¿›è¡ŒæŸ¥çœ‹ã€‚åœ¨ç”± `Interface` æž„é€ å‡½æ•°çš„ `flagging_dir=` å‚æ•°æä¾›çš„ç›®å½•ä¸­ï¼Œå°†è®°å½•æ ‡è®°çš„è¾“å…¥åˆ°ä¸€ä¸ª CSV æ–‡ä»¶ä¸­ã€‚å¦‚æžœç•Œé¢æ¶‰åŠæ–‡ä»¶æ•°æ®ï¼Œä¾‹å¦‚å›¾åƒå’ŒéŸ³é¢‘ç»„ä»¶ï¼Œå°†åˆ›å»ºæ–‡ä»¶å¤¹æ¥å­˜å‚¨è¿™äº›æ ‡è®°çš„æ•°æ®ã€‚

ä¾‹å¦‚ï¼Œå¯¹äºŽä¸Šé¢æ˜¾ç¤ºçš„è®¡ç®—å™¨ç•Œé¢ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„æ——æ ‡ç›®å½•ä¸­å­˜å‚¨æ ‡è®°çš„æ•°æ®ï¼š

```directory
+-- calculator.py
+-- flagged/
|   +-- logs.csv
```

_flagged/logs.csv_

```csv
num1,operation,num2,Output
5,add,7,12
6,subtract,1.5,4.5
```

ä¸Žæ—©æœŸæ˜¾ç¤ºçš„å†·è‰²ç•Œé¢ç›¸å¯¹åº”ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„æ——æ ‡ç›®å½•ä¸­å­˜å‚¨æ ‡è®°çš„æ•°æ®ï¼š

```directory
+-- sepia.py
+-- flagged/
|   +-- logs.csv
|   +-- im/
|   |   +-- 0.png
|   |   +-- 1.png
|   +-- Output/
|   |   +-- 0.png
|   |   +-- 1.png
```

_flagged/logs.csv_

```csv
im,Output
im/0.png,Output/0.png
im/1.png,Output/1.png
```

å¦‚æžœæ‚¨å¸Œæœ›ç”¨æˆ·æä¾›æ——æ ‡åŽŸå› ï¼Œå¯ä»¥å°†å­—ç¬¦ä¸²åˆ—è¡¨ä¼ é€’ç»™ Interface çš„ `flagging_options` å‚æ•°ã€‚ç”¨æˆ·åœ¨è¿›è¡Œæ——æ ‡æ—¶å¿…é¡»é€‰æ‹©å…¶ä¸­ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¿™å°†ä½œä¸ºé™„åŠ åˆ—ä¿å­˜åˆ° CSV ä¸­ã€‚

## é¢„å¤„ç†å’ŒåŽå¤„ç† (Preprocessing and Postprocessing)

![annotated](/assets/img/dataflow.svg)

å¦‚æ‚¨æ‰€è§ï¼ŒGradio åŒ…æ‹¬å¯ä»¥å¤„ç†å„ç§ä¸åŒæ•°æ®ç±»åž‹çš„ç»„ä»¶ï¼Œä¾‹å¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚å¤§å¤šæ•°ç»„ä»¶éƒ½å¯ä»¥ç”¨ä½œè¾“å…¥æˆ–è¾“å‡ºã€‚

å½“ç»„ä»¶ç”¨ä½œè¾“å…¥æ—¶ï¼ŒGradio è‡ªåŠ¨å¤„ç†*é¢„å¤„ç†*ï¼Œå°†æ•°æ®ä»Žç”¨æˆ·æµè§ˆå™¨å‘é€çš„ç±»åž‹ï¼ˆä¾‹å¦‚ç½‘ç»œæ‘„åƒå¤´å¿«ç…§çš„ base64 è¡¨ç¤ºï¼‰è½¬æ¢ä¸ºæ‚¨çš„å‡½æ•°å¯ä»¥æŽ¥å—çš„å½¢å¼ï¼ˆä¾‹å¦‚ `numpy` æ•°ç»„ï¼‰ã€‚

åŒæ ·ï¼Œå½“ç»„ä»¶ç”¨ä½œè¾“å‡ºæ—¶ï¼ŒGradio è‡ªåŠ¨å¤„ç†*åŽå¤„ç†*ï¼Œå°†æ•°æ®ä»Žå‡½æ•°è¿”å›žçš„å½¢å¼ï¼ˆä¾‹å¦‚å›¾åƒè·¯å¾„åˆ—è¡¨ï¼‰è½¬æ¢ä¸ºå¯ä»¥åœ¨ç”¨æˆ·æµè§ˆå™¨ä¸­æ˜¾ç¤ºçš„å½¢å¼ï¼ˆä¾‹å¦‚ä»¥ base64 æ ¼å¼æ˜¾ç¤ºå›¾åƒçš„ `Gallery`ï¼‰ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨æž„å»ºå›¾åƒç»„ä»¶æ—¶çš„å‚æ•°æŽ§åˆ¶*é¢„å¤„ç†*ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨ä½¿ç”¨ä»¥ä¸‹å‚æ•°å®žä¾‹åŒ– `Image` ç»„ä»¶ï¼Œå®ƒå°†å°†å›¾åƒè½¬æ¢ä¸º `PIL` ç±»åž‹ï¼Œå¹¶å°†å…¶é‡å¡‘ä¸º`(100, 100)`ï¼Œè€Œä¸ç®¡æäº¤æ—¶çš„åŽŸå§‹å¤§å°å¦‚ä½•ï¼š

```py
img = gr.Image(width=100, height=100, type="pil")
```

ç›¸åï¼Œè¿™é‡Œæˆ‘ä»¬ä¿ç•™å›¾åƒçš„åŽŸå§‹å¤§å°ï¼Œä½†åœ¨å°†å…¶è½¬æ¢ä¸º numpy æ•°ç»„ä¹‹å‰åè½¬é¢œè‰²ï¼š

```py
img = gr.Image(invert_colors=True, type="numpy")
```

åŽå¤„ç†è¦å®¹æ˜“å¾—å¤šï¼Gradio è‡ªåŠ¨è¯†åˆ«è¿”å›žæ•°æ®çš„æ ¼å¼ï¼ˆä¾‹å¦‚ `Image` æ˜¯ `numpy` æ•°ç»„è¿˜æ˜¯ `str` æ–‡ä»¶è·¯å¾„ï¼Ÿï¼‰ï¼Œå¹¶å°†å…¶åŽå¤„ç†ä¸ºå¯ä»¥ç”±æµè§ˆå™¨æ˜¾ç¤ºçš„æ ¼å¼ã€‚

è¯·æŸ¥çœ‹[æ–‡æ¡£](https://gradio.app/docs)ï¼Œäº†è§£æ¯ä¸ªç»„ä»¶çš„æ‰€æœ‰ä¸Žé¢„å¤„ç†ç›¸å…³çš„å‚æ•°ã€‚

## æ ·å¼ (Styling)

Gradio ä¸»é¢˜æ˜¯è‡ªå®šä¹‰åº”ç”¨ç¨‹åºå¤–è§‚å’Œæ„Ÿè§‰çš„æœ€ç®€å•æ–¹æ³•ã€‚æ‚¨å¯ä»¥é€‰æ‹©å¤šç§ä¸»é¢˜æˆ–åˆ›å»ºè‡ªå·±çš„ä¸»é¢˜ã€‚è¦è¿™æ ·åšï¼Œè¯·å°† `theme=` å‚æ•°ä¼ é€’ç»™ `Interface` æž„é€ å‡½æ•°ã€‚ä¾‹å¦‚ï¼š

```python
demo = gr.Interface(..., theme=gr.themes.Monochrome())
```

Gradio å¸¦æœ‰ä¸€ç»„é¢„å…ˆæž„å»ºçš„ä¸»é¢˜ï¼Œæ‚¨å¯ä»¥ä»Ž `gr.themes.*` åŠ è½½ã€‚æ‚¨å¯ä»¥æ‰©å±•è¿™äº›ä¸»é¢˜æˆ–ä»Žå¤´å¼€å§‹åˆ›å»ºè‡ªå·±çš„ä¸»é¢˜ - æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ä¸»é¢˜æŒ‡å—](https://gradio.app/theming-guide)ã€‚

è¦å¢žåŠ é¢å¤–çš„æ ·å¼èƒ½åŠ›ï¼Œæ‚¨å¯ä»¥ with `css=` å…³é”®å­—å°†ä»»ä½• CSS ä¼ é€’ç»™æ‚¨çš„åº”ç”¨ç¨‹åºã€‚
Gradio åº”ç”¨ç¨‹åºçš„åŸºç±»æ˜¯ `gradio-container`ï¼Œå› æ­¤ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ›´æ”¹ Gradio åº”ç”¨ç¨‹åºèƒŒæ™¯é¢œè‰²çš„ç¤ºä¾‹ï¼š

```python
with `gr.Interface(css=".gradio-container {background-color: red}") as demo:
    ...
```

## é˜Ÿåˆ— (Queuing)

å¦‚æžœæ‚¨çš„åº”ç”¨ç¨‹åºé¢„è®¡ä¼šæœ‰å¤§é‡æµé‡ï¼Œè¯· with `queue()` æ–¹æ³•æ¥æŽ§åˆ¶å¤„ç†é€ŸçŽ‡ã€‚è¿™å°†æŽ’é˜Ÿå¤„ç†è°ƒç”¨ï¼Œå› æ­¤ä¸€æ¬¡åªå¤„ç†ä¸€å®šæ•°é‡çš„è¯·æ±‚ã€‚é˜Ÿåˆ—ä½¿ç”¨ Websocketsï¼Œè¿˜å¯ä»¥é˜²æ­¢ç½‘ç»œè¶…æ—¶ï¼Œå› æ­¤å¦‚æžœæ‚¨çš„å‡½æ•°çš„æŽ¨ç†æ—¶é—´å¾ˆé•¿ï¼ˆ> 1 åˆ†é’Ÿï¼‰ï¼Œåº”ä½¿ç”¨é˜Ÿåˆ—ã€‚

with `Interface`ï¼š

```python
demo = gr.Interface(...).queue()
demo.launch()
```

with `Blocks`ï¼š

```python
with gr.Blocks() as demoï¼š
    #...
demo.queue()
demo.launch()
```

æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æŽ§åˆ¶ä¸€æ¬¡å¤„ç†çš„è¯·æ±‚æ•°é‡ï¼š

```python
demo.queue(concurrency_count=3)
```

æŸ¥çœ‹æœ‰å…³é…ç½®å…¶ä»–é˜Ÿåˆ—å‚æ•°çš„[é˜Ÿåˆ—æ–‡æ¡£](/docs/#queue)ã€‚

åœ¨ Blocks ä¸­æŒ‡å®šä»…å¯¹æŸäº›å‡½æ•°è¿›è¡ŒæŽ’é˜Ÿï¼š

```python
with gr.Blocks() as demo2ï¼š
    num1 = gr.Number()
    num2 = gr.Number()
    output = gr.Number()
    gr.Button("Add").click(
        lambda a, b: a + b, [num1, num2], output)
    gr.Button("Multiply").click(
        lambda a, b: a * b, [num1, num2], output, queue=True)
demo2.launch()
```

## è¿­ä»£è¾“å‡º (Iterative Outputs)

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½éœ€è¦ä¼ è¾“ä¸€ç³»åˆ—è¾“å‡ºè€Œä¸æ˜¯ä¸€æ¬¡æ˜¾ç¤ºå•ä¸ªè¾“å‡ºã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½æœ‰ä¸€ä¸ªå›¾åƒç”Ÿæˆæ¨¡åž‹ï¼Œå¸Œæœ›æ˜¾ç¤ºç”Ÿæˆçš„æ¯ä¸ªæ­¥éª¤çš„å›¾åƒï¼Œç›´åˆ°æœ€ç»ˆå›¾åƒã€‚æˆ–è€…æ‚¨å¯èƒ½æœ‰ä¸€ä¸ªèŠå¤©æœºå™¨äººï¼Œå®ƒé€å­—é€å¥åœ°æµå¼ä¼ è¾“å“åº”ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡è¿”å›žå…¨éƒ¨å“åº”ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥å°†**ç”Ÿæˆå™¨**å‡½æ•°æä¾›ç»™ Gradioï¼Œè€Œä¸æ˜¯å¸¸è§„å‡½æ•°ã€‚åœ¨ Python ä¸­åˆ›å»ºç”Ÿæˆå™¨éžå¸¸ç®€å•ï¼šå‡½æ•°ä¸åº”è¯¥æœ‰ä¸€ä¸ªå•ç‹¬çš„ `return` å€¼ï¼Œè€Œæ˜¯åº”è¯¥ with `yield` è¿žç»­è¿”å›žä¸€ç³»åˆ—å€¼ã€‚é€šå¸¸ï¼Œ`yield` è¯­å¥æ”¾ç½®åœ¨æŸç§å¾ªçŽ¯ä¸­ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼Œç”Ÿæˆå™¨åªæ˜¯ç®€å•è®¡æ•°åˆ°ç»™å®šæ•°å­—ï¼š

```python
def my_generator(x):
    for i in range(x):
        yield i
```

æ‚¨ä»¥ä¸Žå¸¸è§„å‡½æ•°ç›¸åŒçš„æ–¹å¼å°†ç”Ÿæˆå™¨æä¾›ç»™ Gradioã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯ä¸€ä¸ªï¼ˆè™šæ‹Ÿçš„ï¼‰å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼Œå®ƒåœ¨è¾“å‡ºå›¾åƒä¹‹å‰ç”Ÿæˆæ•°ä¸ªæ­¥éª¤çš„å™ªéŸ³ï¼š

$code_fake_diffusion
$demo_fake_diffusion

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨è¿­ä»£å™¨ä¸­æ·»åŠ äº† `time.sleep(1)`ï¼Œä»¥åˆ›å»ºæ­¥éª¤ä¹‹é—´çš„äººå·¥æš‚åœï¼Œä»¥ä¾¿æ‚¨å¯ä»¥è§‚å¯Ÿè¿­ä»£å™¨çš„æ­¥éª¤ï¼ˆåœ¨çœŸå®žçš„å›¾åƒç”Ÿæˆæ¨¡åž‹ä¸­ï¼Œè¿™å¯èƒ½æ˜¯ä¸å¿…è¦çš„ï¼‰ã€‚

å°†ç”Ÿæˆå™¨æä¾›ç»™ Gradio **éœ€è¦**åœ¨åº•å±‚ Interface æˆ– Blocks ä¸­å¯ç”¨é˜Ÿåˆ—ï¼ˆè¯·å‚é˜…ä¸Šé¢çš„é˜Ÿåˆ—éƒ¨åˆ†ï¼‰ã€‚

## è¿›åº¦æ¡

Gradio æ”¯æŒåˆ›å»ºè‡ªå®šä¹‰è¿›åº¦æ¡ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥è‡ªå®šä¹‰å’ŒæŽ§åˆ¶å‘ç”¨æˆ·æ˜¾ç¤ºçš„è¿›åº¦æ›´æ–°ã€‚è¦å¯ç”¨æ­¤åŠŸèƒ½ï¼Œåªéœ€ä¸ºæ–¹æ³•æ·»åŠ ä¸€ä¸ªé»˜è®¤å€¼ä¸º `gr.Progress` å®žä¾‹çš„å‚æ•°å³å¯ã€‚ç„¶åŽï¼Œæ‚¨å¯ä»¥ç›´æŽ¥è°ƒç”¨æ­¤å®žä¾‹å¹¶ä¼ å…¥ 0 åˆ° 1 ä¹‹é—´çš„æµ®ç‚¹æ•°æ¥æ›´æ–°è¿›åº¦çº§åˆ«ï¼Œæˆ–è€… with `Progress` å®žä¾‹çš„ `tqdm()` æ–¹æ³•æ¥è·Ÿè¸ªå¯è¿­ä»£å¯¹è±¡ä¸Šçš„è¿›åº¦ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚å¿…é¡»å¯ç”¨é˜Ÿåˆ—ä»¥è¿›è¡Œè¿›åº¦æ›´æ–°ã€‚

$code_progress_simple
$demo_progress_simple

å¦‚æžœæ‚¨ with `tqdm` åº“ï¼Œå¹¶ä¸”å¸Œæœ›ä»Žå‡½æ•°å†…éƒ¨çš„ä»»ä½• `tqdm.tqdm` è‡ªåŠ¨æŠ¥å‘Šè¿›åº¦æ›´æ–°ï¼Œè¯·å°†é»˜è®¤å‚æ•°è®¾ç½®ä¸º `gr.Progress(track_tqdm=True)`ï¼

## æ‰¹å¤„ç†å‡½æ•° (Batch Functions)

Gradio æ”¯æŒä¼ é€’*æ‰¹å¤„ç†*å‡½æ•°ã€‚æ‰¹å¤„ç†å‡½æ•°åªæ˜¯æŽ¥å—è¾“å…¥åˆ—è¡¨å¹¶è¿”å›žé¢„æµ‹åˆ—è¡¨çš„å‡½æ•°ã€‚

ä¾‹å¦‚ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‰¹å¤„ç†å‡½æ•°ï¼Œå®ƒæŽ¥å—ä¸¤ä¸ªè¾“å…¥åˆ—è¡¨ï¼ˆä¸€ä¸ªå•è¯åˆ—è¡¨å’Œä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼‰ï¼Œå¹¶è¿”å›žä¿®å‰ªè¿‡çš„å•è¯åˆ—è¡¨ä½œä¸ºè¾“å‡ºï¼š

```python
import time

def trim_words(words, lens):
    trimmed_words = []
    time.sleep(5)
    for w, l in zip(words, lens):
        trimmed_words.append(w[:int(l)])
    return [trimmed_words]
    for w, l in zip(words, lens):
```

ä½¿ç”¨æ‰¹å¤„ç†å‡½æ•°çš„ä¼˜ç‚¹æ˜¯ï¼Œå¦‚æžœå¯ç”¨äº†é˜Ÿåˆ—ï¼ŒGradio æœåŠ¡å™¨å¯ä»¥è‡ªåŠ¨*æ‰¹å¤„ç†*ä¼ å…¥çš„è¯·æ±‚å¹¶å¹¶è¡Œå¤„ç†å®ƒä»¬ï¼Œä»Žè€Œå¯èƒ½åŠ å¿«æ¼”ç¤ºé€Ÿåº¦ã€‚ä»¥ä¸‹æ˜¯ Gradio ä»£ç çš„ç¤ºä¾‹ï¼ˆè¯·æ³¨æ„ `batch=True` å’Œ `max_batch_size=16` - è¿™ä¸¤ä¸ªå‚æ•°éƒ½å¯ä»¥ä¼ é€’ç»™äº‹ä»¶è§¦å‘å™¨æˆ– `Interface` ç±»ï¼‰

with `Interface`ï¼š

```python
demo = gr.Interface(trim_words, ["textbox", "number"], ["output"],
                    batch=True, max_batch_size=16)
demo.queue()
demo.launch()
```

with `Blocks`ï¼š

```python
import gradio as gr

with gr.Blocks() as demo:
    with gr.Row():
        word = gr.Textbox(label="word")
        leng = gr.Number(label="leng")
        output = gr.Textbox(label="Output")
    with gr.Row():
        run = gr.Button()

    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)

demo.queue()
demo.launch()
```

åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç† 16 ä¸ªè¯·æ±‚ï¼ˆæ€»æŽ¨ç†æ—¶é—´ä¸º 5 ç§’ï¼‰ï¼Œè€Œä¸æ˜¯åˆ†åˆ«å¤„ç†æ¯ä¸ªè¯·æ±‚ï¼ˆæ€»æŽ¨ç†æ—¶é—´ä¸º 80 ç§’ï¼‰ã€‚è®¸å¤š Hugging Face çš„ `transformers` å’Œ `diffusers` æ¨¡åž‹åœ¨ Gradio çš„æ‰¹å¤„ç†æ¨¡å¼ä¸‹è‡ªç„¶å·¥ä½œï¼šè¿™æ˜¯[ä½¿ç”¨æ‰¹å¤„ç†ç”Ÿæˆå›¾åƒçš„ç¤ºä¾‹æ¼”ç¤º](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)

æ³¨æ„ï¼šä½¿ç”¨ Gradio çš„æ‰¹å¤„ç†å‡½æ•° **requires** åœ¨åº•å±‚ Interface æˆ– Blocks ä¸­å¯ç”¨é˜Ÿåˆ—ï¼ˆè¯·å‚é˜…ä¸Šé¢çš„é˜Ÿåˆ—éƒ¨åˆ†ï¼‰ã€‚

## Gradio ç¬”è®°æœ¬ (Colab Notebooks)

Gradio å¯ä»¥åœ¨ä»»ä½•è¿è¡Œ Python çš„åœ°æ–¹è¿è¡Œï¼ŒåŒ…æ‹¬æœ¬åœ° Jupyter ç¬”è®°æœ¬å’Œåä½œç¬”è®°æœ¬ï¼Œå¦‚[Google Colab](https://colab.research.google.com/)ã€‚å¯¹äºŽæœ¬åœ° Jupyter ç¬”è®°æœ¬å’Œ Google Colab ç¬”è®°æœ¬ï¼ŒGradio åœ¨æœ¬åœ°æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œæ‚¨å¯ä»¥åœ¨æµè§ˆå™¨ä¸­ä¸Žä¹‹äº¤äº’ã€‚ï¼ˆæ³¨æ„ï¼šå¯¹äºŽ Google Colabï¼Œè¿™æ˜¯é€šè¿‡[æœåŠ¡å·¥ä½œå™¨éš§é“](https://github.com/tensorflow/tensorboard/blob/master/docs/design/colab_integration.md)å®žçŽ°çš„ï¼Œæ‚¨çš„æµè§ˆå™¨éœ€è¦å¯ç”¨ cookiesã€‚ï¼‰å¯¹äºŽå…¶ä»–è¿œç¨‹ç¬”è®°æœ¬ï¼ŒGradio ä¹Ÿå°†åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œä½†æ‚¨éœ€è¦ä½¿ç”¨[SSH éš§é“](https://coderwall.com/p/ohk6cg/remote-access-to-ipython-notebooks-via-ssh)åœ¨æœ¬åœ°æµè§ˆå™¨ä¸­æŸ¥çœ‹åº”ç”¨ç¨‹åºã€‚é€šå¸¸ï¼Œæ›´ç®€å•çš„é€‰æ‹©æ˜¯ä½¿ç”¨ Gradio å†…ç½®çš„å…¬å…±é“¾æŽ¥ï¼Œ[åœ¨ä¸‹ä¸€ç¯‡æŒ‡å—ä¸­è®¨è®º](/sharing-your-app/#sharing-demos)ã€‚

---

<!-- Source: guides/cn/02_building-interfaces/02_reactive-interfaces.md -->
# ååº”å¼ç•Œé¢ (Reactive Interfaces)

æœ¬æŒ‡å—ä»‹ç»äº†å¦‚ä½•ä½¿ Gradio ç•Œé¢è‡ªåŠ¨åˆ·æ–°æˆ–è¿žç»­æµå¼ä¼ è¾“æ•°æ®ã€‚

## å®žæ—¶ç•Œé¢ (Live Interfaces)

æ‚¨å¯ä»¥é€šè¿‡åœ¨ç•Œé¢ä¸­è®¾ç½® `live=True` æ¥ä½¿ç•Œé¢è‡ªåŠ¨åˆ·æ–°ã€‚çŽ°åœ¨ï¼Œåªè¦ç”¨æˆ·è¾“å…¥å‘ç”Ÿå˜åŒ–ï¼Œç•Œé¢å°±ä¼šé‡æ–°è®¡ç®—ã€‚

$code_calculator_live
$demo_calculator_live

æ³¨æ„ï¼Œå› ä¸ºç•Œé¢åœ¨æ›´æ”¹æ—¶ä¼šè‡ªåŠ¨é‡æ–°æäº¤ï¼Œæ‰€ä»¥æ²¡æœ‰æäº¤æŒ‰é’®ã€‚

## æµå¼ç»„ä»¶ (Streaming Components)

æŸäº›ç»„ä»¶å…·æœ‰â€œæµå¼â€æ¨¡å¼ï¼Œæ¯”å¦‚éº¦å…‹é£Žæ¨¡å¼ä¸‹çš„ `Audio` ç»„ä»¶æˆ–ç½‘ç»œæ‘„åƒå¤´æ¨¡å¼ä¸‹çš„ `Image` ç»„ä»¶ã€‚æµå¼ä¼ è¾“æ„å‘³ç€æ•°æ®ä¼šæŒç»­å‘é€åˆ°åŽç«¯ï¼Œå¹¶ä¸” `Interface` å‡½æ•°ä¼šæŒç»­é‡æ–°è¿è¡Œã€‚

å½“åœ¨ `gr.Interface(live=True)` ä¸­åŒæ—¶ä½¿ç”¨ `gr.Audio(sources=['microphone'])` å’Œ `gr.Audio(sources=['microphone'], streaming=True)` æ—¶ï¼Œä¸¤è€…çš„åŒºåˆ«åœ¨äºŽç¬¬ä¸€ä¸ª `Component` ä¼šåœ¨ç”¨æˆ·åœæ­¢å½•åˆ¶æ—¶è‡ªåŠ¨æäº¤æ•°æ®å¹¶è¿è¡Œ `Interface` å‡½æ•°ï¼Œè€Œç¬¬äºŒä¸ª `Component` ä¼šåœ¨å½•åˆ¶è¿‡ç¨‹ä¸­æŒç»­å‘é€æ•°æ®å¹¶è¿è¡Œ `Interface` å‡½æ•°ã€‚

ä»¥ä¸‹æ˜¯ä»Žç½‘ç»œæ‘„åƒå¤´å®žæ—¶æµå¼ä¼ è¾“å›¾åƒçš„ç¤ºä¾‹ä»£ç ã€‚

$code_stream_frames

---

<!-- Source: guides/cn/03_building-with-blocks/02_controlling-layout.md -->
# æŽ§åˆ¶å¸ƒå±€ (Controlling Layout)

é»˜è®¤æƒ…å†µä¸‹ï¼Œå—ä¸­çš„ç»„ä»¶æ˜¯åž‚ç›´æŽ’åˆ—çš„ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•é‡æ–°æŽ’åˆ—ç»„ä»¶ã€‚åœ¨å¹•åŽï¼Œè¿™ç§å¸ƒå±€ç»“æž„ä½¿ç”¨äº†[Web å¼€å‘çš„ flexbox æ¨¡åž‹](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox)ã€‚

## Row è¡Œ

`with gr.Row` ä¸‹çš„å…ƒç´ å°†æ°´å¹³æ˜¾ç¤ºã€‚ä¾‹å¦‚ï¼Œè¦å¹¶æŽ’æ˜¾ç¤ºä¸¤ä¸ªæŒ‰é’®ï¼š

```python
with gr.Blocks() as demo:
    with gr.Row():
        btn1 = gr.Button("æŒ‰é’®1")
        btn2 = gr.Button("æŒ‰é’®2")
```

è¦ä½¿è¡Œä¸­çš„æ¯ä¸ªå…ƒç´ å…·æœ‰ç›¸åŒçš„é«˜åº¦ï¼Œè¯·ä½¿ç”¨ `style` æ–¹æ³•çš„ `equal_height` å‚æ•°ã€‚

```python
with gr.Blocks() as demo:
    with gr.Row(equal_height=True):
        textbox = gr.Textbox()
        btn2 = gr.Button("æŒ‰é’®2")
```

å¯ä»¥é€šè¿‡æ¯ä¸ªç»„ä»¶ä¸­å­˜åœ¨çš„ `scale` å’Œ `min_width` å‚æ•°æ¥æŽ§åˆ¶è¡Œä¸­å…ƒç´ çš„å®½åº¦ã€‚

- `scale` æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œå®šä¹‰äº†å…ƒç´ åœ¨è¡Œä¸­çš„å ç”¨ç©ºé—´ã€‚å¦‚æžœå°† scale è®¾ç½®ä¸º `0`ï¼Œåˆ™å…ƒç´ ä¸ä¼šæ‰©å±•å ç”¨ç©ºé—´ã€‚å¦‚æžœå°† scale è®¾ç½®ä¸º `1` æˆ–æ›´å¤§ï¼Œåˆ™å…ƒç´ å°†æ‰©å±•ã€‚è¡Œä¸­çš„å¤šä¸ªå…ƒç´ å°†æŒ‰æ¯”ä¾‹æ‰©å±•ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œ`btn1` å°†æ¯” `btn2` æ‰©å±•ä¸¤å€ï¼Œè€Œ `btn0` å°†æ ¹æœ¬ä¸ä¼šæ‰©å±•ï¼š

```python
with gr.Blocks() as demo:
    with gr.Row():
        btn0 = gr.Button("æŒ‰é’®0", scale=0)
        btn1 = gr.Button("æŒ‰é’®1", scale=1)
        btn2 = gr.Button("æŒ‰é’®2", scale=2)
```

- `min_width` å°†è®¾ç½®å…ƒç´ çš„æœ€å°å®½åº¦ã€‚å¦‚æžœæ²¡æœ‰è¶³å¤Ÿçš„ç©ºé—´æ»¡è¶³æ‰€æœ‰çš„ `min_width` å€¼ï¼Œè¡Œå°†æ¢è¡Œã€‚

åœ¨[æ–‡æ¡£](https://gradio.app/docs/row)ä¸­äº†è§£æœ‰å…³è¡Œçš„æ›´å¤šä¿¡æ¯ã€‚

## åˆ—å’ŒåµŒå¥— (Columns and Nesting)

åˆ—ä¸­çš„ç»„ä»¶å°†åž‚ç›´æ”¾ç½®åœ¨ä¸€èµ·ã€‚ç”±äºŽé»˜è®¤å¸ƒå±€å¯¹äºŽå—åº”ç”¨ç¨‹åºæ¥è¯´æ˜¯åž‚ç›´å¸ƒå±€ï¼Œå› æ­¤ä¸ºäº†æœ‰ç”¨ï¼Œåˆ—é€šå¸¸åµŒå¥—åœ¨è¡Œä¸­ã€‚ä¾‹å¦‚ï¼š

$code_rows_and_columns
$demo_rows_and_columns

æŸ¥çœ‹ç¬¬ä¸€åˆ—å¦‚ä½•åž‚ç›´æŽ’åˆ—ä¸¤ä¸ªæ–‡æœ¬æ¡†ã€‚ç¬¬äºŒåˆ—åž‚ç›´æŽ’åˆ—å›¾åƒå’ŒæŒ‰é’®ã€‚æ³¨æ„ä¸¤åˆ—çš„ç›¸å¯¹å®½åº¦ç”± `scale` å‚æ•°è®¾ç½®ã€‚å…·æœ‰ä¸¤å€ `scale` å€¼çš„åˆ—å æ®ä¸¤å€çš„å®½åº¦ã€‚

åœ¨[æ–‡æ¡£](https://gradio.app/docs/column)ä¸­äº†è§£æœ‰å…³åˆ—çš„æ›´å¤šä¿¡æ¯ã€‚

## é€‰é¡¹å¡å’Œæ‰‹é£Žç´ (Tabs and Accordions)

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ `with gr.Tab('tab_name'):` è¯­å¥åˆ›å»ºé€‰é¡¹å¡ã€‚åœ¨ `with gr.Tab('tab_name'):` ä¸Šä¸‹æ–‡ä¸­åˆ›å»ºçš„ä»»ä½•ç»„ä»¶éƒ½å°†æ˜¾ç¤ºåœ¨è¯¥é€‰é¡¹å¡ä¸­ã€‚è¿žç»­çš„ Tab å­å¥è¢«åˆ†ç»„åœ¨ä¸€èµ·ï¼Œä»¥ä¾¿ä¸€æ¬¡åªèƒ½é€‰æ‹©ä¸€ä¸ªé€‰é¡¹å¡ï¼Œå¹¶ä¸”åªæ˜¾ç¤ºè¯¥é€‰é¡¹å¡ä¸Šä¸‹æ–‡ä¸­çš„ç»„ä»¶ã€‚

ä¾‹å¦‚ï¼š

$code_blocks_flipper
$demo_blocks_flipper

è¿˜è¯·æ³¨æ„æœ¬ç¤ºä¾‹ä¸­çš„ `gr.Accordion('label')`ã€‚æ‰‹é£Žç´æ˜¯ä¸€ç§å¯ä»¥åˆ‡æ¢æ‰“å¼€æˆ–å…³é—­çš„å¸ƒå±€ã€‚ä¸Ž `Tabs` ä¸€æ ·ï¼Œå®ƒæ˜¯å¯ä»¥é€‰æ‹©æ€§éšè—æˆ–æ˜¾ç¤ºå†…å®¹çš„å¸ƒå±€å…ƒç´ ã€‚åœ¨ `with gr.Accordion('label'):` å†…å®šä¹‰çš„ä»»ä½•ç»„ä»¶åœ¨å•å‡»æ‰‹é£Žç´çš„åˆ‡æ¢å›¾æ ‡æ—¶éƒ½ä¼šè¢«éšè—æˆ–æ˜¾ç¤ºã€‚

åœ¨æ–‡æ¡£ä¸­äº†è§£æœ‰å…³[Tabs](https://gradio.app/docs/tab)å’Œ[Accordions](https://gradio.app/docs/accordion)çš„æ›´å¤šä¿¡æ¯ã€‚

## å¯è§æ€§ (Visibility)

ç»„ä»¶å’Œå¸ƒå±€å…ƒç´ éƒ½æœ‰ä¸€ä¸ª `visible` å‚æ•°ï¼Œå¯ä»¥åœ¨åˆå§‹æ—¶è®¾ç½®ï¼Œå¹¶ä½¿ç”¨ `gr.update()` è¿›è¡Œæ›´æ–°ã€‚åœ¨ Column ä¸Šè®¾ç½® `gr.update(visible=...)` å¯ç”¨äºŽæ˜¾ç¤ºæˆ–éšè—ä¸€ç»„ç»„ä»¶ã€‚

$code_blocks_form
$demo_blocks_form

## å¯å˜æ•°é‡çš„è¾“å‡º (Variable Number of Outputs)

é€šè¿‡ä»¥åŠ¨æ€æ–¹å¼è°ƒæ•´ç»„ä»¶çš„å¯è§æ€§ï¼Œå¯ä»¥åˆ›å»ºæ”¯æŒ _å¯å˜æ•°é‡è¾“å‡º_ çš„ Gradio æ¼”ç¤ºã€‚è¿™æ˜¯ä¸€ä¸ªéžå¸¸ç®€å•çš„ä¾‹å­ï¼Œå…¶ä¸­è¾“å‡ºæ–‡æœ¬æ¡†çš„æ•°é‡ç”±è¾“å…¥æ»‘å—æŽ§åˆ¶ï¼š

ä¾‹å¦‚ï¼š

$code_variable_outputs
$demo_variable_outputs

## åˆ†å¼€å®šä¹‰å’Œæ¸²æŸ“ç»„ä»¶ (Defining and Rendering Components Separately)

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨å®žé™…æ¸²æŸ“ UI ä¹‹å‰å®šä¹‰ç»„ä»¶ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨ç›¸åº”çš„ `gr.Textbox` è¾“å…¥ä¸Šæ–¹æ˜¾ç¤ºç¤ºä¾‹éƒ¨åˆ†ï¼Œä½¿ç”¨ `gr.Examples`ã€‚ç”±äºŽ `gr.Examples` éœ€è¦ä¸€ä¸ªå‚æ•°ä½œä¸ºè¾“å…¥ç»„ä»¶å¯¹è±¡ï¼Œæ‚¨éœ€è¦å…ˆå®šä¹‰è¾“å…¥ç»„ä»¶ï¼Œç„¶åŽåœ¨å®šä¹‰ `gr.Examples` å¯¹è±¡ä¹‹åŽå†æ¸²æŸ“å®ƒã€‚

è§£å†³æ–¹æ³•æ˜¯åœ¨ `gr.Blocks()` èŒƒå›´ä¹‹å¤–å®šä¹‰ `gr.Textbox`ï¼Œå¹¶åœ¨ UI ä¸­æƒ³è¦æ”¾ç½®å®ƒçš„ä½ç½®ä½¿ç”¨ç»„ä»¶çš„ `.render()` æ–¹æ³•ã€‚

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ä»£ç ç¤ºä¾‹ï¼š

```python
input_textbox = gr.Textbox()

with gr.Blocks() as demo:
    gr.Examples(["hello", "bonjour", "merhaba"], input_textbox)
    input_textbox.render()
```

---

<!-- Source: guides/cn/06_client-libraries/02_getting-started-with-the-js-client.md -->
# ä½¿ç”¨Gradio JavaScriptå®¢æˆ·ç«¯å¿«é€Ÿå…¥é—¨

Tags: CLIENT, API, SPACES

Gradio JavaScriptå®¢æˆ·ç«¯ä½¿å¾—ä½¿ç”¨ä»»ä½•Gradioåº”ç”¨ä½œä¸ºAPIéžå¸¸ç®€å•ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ä¸€ä¸‹è¿™ä¸ª[ä»Žéº¦å…‹é£Žå½•éŸ³çš„Hugging Face Spaceï¼Œç”¨äºŽè½¬å½•éŸ³é¢‘æ–‡ä»¶](https://huggingface.co/spaces/abidlabs/whisper)ã€‚

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)

ä½¿ç”¨`@gradio/client`åº“ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°ä»¥ç¼–ç¨‹æ–¹å¼ä½¿ç”¨Gradioä½œä¸ºAPIæ¥è½¬å½•éŸ³é¢‘æ–‡ä»¶ã€‚

ä»¥ä¸‹æ˜¯å®Œæˆæ­¤æ“ä½œçš„å®Œæ•´ä»£ç ï¼š

```js
import { Client } from "@gradio/client";

const response = await fetch(
	"https://github.com/audio-samples/audio-samples.github.io/raw/master/samples/wav/ted_speakers/SalmanKhan/sample-1.wav"
);
const audio_file = await response.blob();

const app = await Client.connect("abidlabs/whisper");
const transcription = await app.predict("/predict", [audio_file]);

console.log(transcription.data);
// [ "I said the same phrase 30 times." ]
```

Gradioå®¢æˆ·ç«¯é€‚ç”¨äºŽä»»ä½•æ‰˜ç®¡çš„Gradioåº”ç”¨ï¼Œæ— è®ºæ˜¯å›¾åƒç”Ÿæˆå™¨ã€æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨ã€æœ‰çŠ¶æ€çš„èŠå¤©æœºå™¨äººã€ç¨Žæ”¶è®¡ç®—å™¨è¿˜æ˜¯å…¶ä»–ä»»ä½•åº”ç”¨ï¼Gradioå®¢æˆ·ç«¯é€šå¸¸ä¸Žæ‰˜ç®¡åœ¨[Hugging Face Spaces](https://hf.space)ä¸Šçš„åº”ç”¨ä¸€èµ·ä½¿ç”¨ï¼Œä½†æ‚¨çš„åº”ç”¨å¯ä»¥æ‰˜ç®¡åœ¨ä»»ä½•åœ°æ–¹ï¼Œæ¯”å¦‚æ‚¨è‡ªå·±çš„æœåŠ¡å™¨ã€‚

**å…ˆå†³æ¡ä»¶**ï¼šè¦ä½¿ç”¨Gradioå®¢æˆ·ç«¯ï¼Œæ‚¨ä¸éœ€è¦æ·±å…¥äº†è§£`gradio`åº“çš„ç»†èŠ‚ã€‚ä½†æ˜¯ï¼Œç†Ÿæ‚‰Gradioçš„è¾“å…¥å’Œè¾“å‡ºç»„ä»¶çš„æ¦‚å¿µä¼šæœ‰æ‰€å¸®åŠ©ã€‚

## å®‰è£…

å¯ä»¥ä½¿ç”¨æ‚¨é€‰æ‹©çš„è½¯ä»¶åŒ…ç®¡ç†å™¨ä»Žnpmæ³¨å†Œè¡¨å®‰è£…è½»é‡çº§çš„`@gradio/client`åŒ…ï¼Œå¹¶æ”¯æŒ18åŠä»¥ä¸Šçš„Nodeç‰ˆæœ¬ï¼š

```bash
npm i @gradio/client
```

## è¿žæŽ¥åˆ°æ­£åœ¨è¿è¡Œçš„Gradioåº”ç”¨

é¦–å…ˆï¼Œé€šè¿‡å®žä¾‹åŒ–`Client`å¯¹è±¡å¹¶å°†å…¶è¿žæŽ¥åˆ°åœ¨Hugging Face Spacesæˆ–ä»»ä½•å…¶ä»–ä½ç½®è¿è¡Œçš„Gradioåº”ç”¨æ¥å»ºç«‹è¿žæŽ¥ã€‚

## è¿žæŽ¥åˆ°Hugging Face Space

```js
import { Client } from "@gradio/client";

const app = Client.connect("abidlabs/en2fr"); // ä¸€ä¸ªä»Žè‹±è¯­ç¿»è¯‘ä¸ºæ³•è¯­çš„ Space
```

æ‚¨è¿˜å¯ä»¥é€šè¿‡åœ¨optionså‚æ•°çš„`hf_token`å±žæ€§ä¸­ä¼ å…¥æ‚¨çš„HF tokenæ¥è¿žæŽ¥åˆ°ç§æœ‰Spacesã€‚æ‚¨å¯ä»¥åœ¨æ­¤å¤„èŽ·å–æ‚¨çš„HF tokenï¼šhttps://huggingface.co/settings/tokens

```js
import { Client } from "@gradio/client";

const app = Client.connect("abidlabs/my-private-space", { hf_token="hf_..." })
```

## ä¸ºç§äººä½¿ç”¨å¤åˆ¶ä¸€ä¸ªSpace

è™½ç„¶æ‚¨å¯ä»¥å°†ä»»ä½•å…¬å…±Spaceç”¨ä½œAPIï¼Œä½†æ˜¯å¦‚æžœæ‚¨å‘å‡ºçš„è¯·æ±‚è¿‡å¤šï¼ŒHugging Faceå¯èƒ½ä¼šå¯¹æ‚¨è¿›è¡Œé€ŸçŽ‡é™åˆ¶ã€‚ä¸ºäº†æ— é™åˆ¶ä½¿ç”¨Spaceï¼Œåªéœ€å¤åˆ¶Spaceä»¥åˆ›å»ºç§æœ‰Spaceï¼Œç„¶åŽä½¿ç”¨å®ƒæ¥è¿›è¡Œä»»æ„æ•°é‡çš„è¯·æ±‚ï¼

`@gradio/client`è¿˜å¯¼å‡ºäº†å¦ä¸€ä¸ªå‡½æ•°`duplicate`ï¼Œä»¥ä½¿æ­¤è¿‡ç¨‹å˜å¾—ç®€å•ï¼ˆæ‚¨å°†éœ€è¦ä¼ å…¥æ‚¨çš„[Hugging Face token](https://huggingface.co/settings/tokens)ï¼‰ã€‚

`duplicate`ä¸Ž`Client`å‡ ä¹Žç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äºŽåº•å±‚å®žçŽ°ï¼š

```js
import { Client } from "@gradio/client";

const response = await fetch(
	"https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3"
);
const audio_file = await response.blob();

const app = await Client.duplicate("abidlabs/whisper", { hf_token: "hf_..." });
const transcription = app.predict("/predict", [audio_file]);
```

å¦‚æžœæ‚¨ä¹‹å‰å¤åˆ¶è¿‡ä¸€ä¸ªSpaceï¼Œåˆ™é‡æ–°è¿è¡Œ`duplicate`ä¸ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„Spaceã€‚è€Œæ˜¯å®¢æˆ·ç«¯å°†è¿žæŽ¥åˆ°å…ˆå‰åˆ›å»ºçš„Spaceã€‚å› æ­¤ï¼Œå¯ä»¥å®‰å…¨åœ°å¤šæ¬¡ä½¿ç”¨ç›¸åŒçš„Spaceé‡æ–°è¿è¡Œ`duplicate`æ–¹æ³•ã€‚

**æ³¨æ„ï¼š**å¦‚æžœåŽŸå§‹Spaceä½¿ç”¨äº†GPUï¼Œæ‚¨çš„ç§æœ‰Spaceä¹Ÿå°†ä½¿ç”¨GPUï¼Œå¹¶ä¸”å°†æ ¹æ®GPUçš„ä»·æ ¼å‘æ‚¨çš„Hugging Faceè´¦æˆ·è®¡è´¹ã€‚ä¸ºäº†æœ€å¤§ç¨‹åº¦åœ°å‡å°‘è´¹ç”¨ï¼Œåœ¨5åˆ†é’Ÿä¸æ´»åŠ¨åŽï¼Œæ‚¨çš„Spaceå°†è‡ªåŠ¨è¿›å…¥ä¼‘çœ çŠ¶æ€ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`duplicate`çš„optionså¯¹è±¡çš„`hardware`å’Œ`timeout`å±žæ€§æ¥è®¾ç½®ç¡¬ä»¶ï¼Œä¾‹å¦‚ï¼š

```js
import { Client } from "@gradio/client";

const app = await Client.duplicate("abidlabs/whisper", {
	hf_token: "hf_...",
	timeout: 60,
	hardware: "a10g-small"
});
```

## è¿žæŽ¥åˆ°é€šç”¨çš„Gradioåº”ç”¨

å¦‚æžœæ‚¨çš„åº”ç”¨ç¨‹åºåœ¨å…¶ä»–åœ°æ–¹è¿è¡Œï¼Œåªéœ€æä¾›å®Œæ•´çš„URLï¼ŒåŒ…æ‹¬"http://"æˆ–"https://"ã€‚ä»¥ä¸‹æ˜¯å‘è¿è¡Œåœ¨å…±äº«URLä¸Šçš„Gradioåº”ç”¨è¿›è¡Œé¢„æµ‹çš„ç¤ºä¾‹ï¼š

```js
import { Client } from "@gradio/client";

const app = Client.connect("https://bec81a83-5b5c-471e.gradio.live");
```

## æ£€æŸ¥APIç«¯ç‚¹

ä¸€æ—¦è¿žæŽ¥åˆ°Gradioåº”ç”¨ç¨‹åºï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨`Client`çš„`view_api`æ–¹æ³•æ¥æŸ¥çœ‹å¯ç”¨çš„APIç«¯ç‚¹ã€‚

å¯¹äºŽWhisper Spaceï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š

```js
import { Client } from "@gradio/client";

const app = await Client.connect("abidlabs/whisper");

const app_info = await app.view_info();

console.log(app_info);
```

ç„¶åŽæˆ‘ä»¬ä¼šçœ‹åˆ°ä»¥ä¸‹å†…å®¹ï¼š

```json
{
	"named_endpoints": {
		"/predict": {
			"parameters": [
				{
					"label": "text",
					"component": "Textbox",
					"type": "string"
				}
			],
			"returns": [
				{
					"label": "output",
					"component": "Textbox",
					"type": "string"
				}
			]
		}
	},
	"unnamed_endpoints": {}
}
```

è¿™å‘Šè¯‰æˆ‘ä»¬è¯¥Spaceä¸­æœ‰1ä¸ªAPIç«¯ç‚¹ï¼Œå¹¶æ˜¾ç¤ºäº†å¦‚ä½•ä½¿ç”¨APIç«¯ç‚¹è¿›è¡Œé¢„æµ‹ï¼šæˆ‘ä»¬åº”è¯¥è°ƒç”¨`.predict()`æ–¹æ³•ï¼ˆä¸‹é¢å°†è¿›è¡Œæ›´å¤šæŽ¢ç´¢ï¼‰ï¼Œå¹¶æä¾›ç±»åž‹ä¸º`string`çš„å‚æ•°`input_audio`ï¼Œå®ƒæ˜¯æŒ‡å‘æ–‡ä»¶çš„URLã€‚

æˆ‘ä»¬è¿˜åº”è¯¥æä¾›`api_name='/predict'`å‚æ•°ç»™`predict()`æ–¹æ³•ã€‚è™½ç„¶å¦‚æžœä¸€ä¸ªGradioåº”ç”¨åªæœ‰1ä¸ªå‘½åçš„ç«¯ç‚¹ï¼Œè¿™ä¸æ˜¯å¿…éœ€çš„ï¼Œä½†å®ƒå¯ä»¥å…è®¸æˆ‘ä»¬åœ¨å•ä¸ªåº”ç”¨ä¸­è°ƒç”¨ä¸åŒçš„ç«¯ç‚¹ã€‚å¦‚æžœåº”ç”¨æœ‰æœªå‘½åçš„APIç«¯ç‚¹ï¼Œå¯ä»¥é€šè¿‡è¿è¡Œ`.view_api(all_endpoints=True)`æ¥æ˜¾ç¤ºå®ƒä»¬ã€‚

## è¿›è¡Œé¢„æµ‹

è¿›è¡Œé¢„æµ‹çš„æœ€ç®€å•æ–¹æ³•å°±æ˜¯ä½¿ç”¨é€‚å½“çš„å‚æ•°è°ƒç”¨`.predict()`æ–¹æ³•ï¼š

```js
import { Client } from "@gradio/client";

const app = await Client.connect("abidlabs/en2fr");
const result = await app.predict("/predict", ["Hello"]);
```

å¦‚æžœæœ‰å¤šä¸ªå‚æ•°ï¼Œæ‚¨åº”è¯¥å°†å®ƒä»¬ä½œä¸ºä¸€ä¸ªæ•°ç»„ä¼ é€’ç»™`.predict()`ï¼Œåƒè¿™æ ·ï¼š

```js
import { Client } from "@gradio/client";

const app = await Client.connect("gradio/calculator");
const result = await app.predict("/predict", [4, "add", 5]);
```

å¯¹äºŽæŸäº›è¾“å…¥ï¼Œä¾‹å¦‚å›¾åƒï¼Œæ‚¨åº”è¯¥æ ¹æ®æ‰€éœ€è¦çš„æ–¹ä¾¿ç¨‹åº¦ä¼ å…¥`Buffer`ã€`Blob`æˆ–`File`ã€‚åœ¨Node.jsä¸­ï¼Œå¯ä»¥ä½¿ç”¨`Buffer`æˆ–`Blob`ï¼›åœ¨æµè§ˆå™¨çŽ¯å¢ƒä¸­ï¼Œå¯ä»¥ä½¿ç”¨`Blob`æˆ–`File`ã€‚

```js
import { Client } from "@gradio/client";

const response = await fetch(
	"https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3"
);
const audio_file = await response.blob();

const app = await Client.connect("abidlabs/whisper");
const result = await Client.connect.predict("/predict", [audio_file]);
```

## ä½¿ç”¨äº‹ä»¶

å¦‚æžœæ‚¨ä½¿ç”¨çš„APIå¯ä»¥éšæ—¶é—´è¿”å›žç»“æžœï¼Œæˆ–è€…æ‚¨å¸Œæœ›è®¿é—®æœ‰å…³ä½œä¸šçŠ¶æ€çš„ä¿¡æ¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨äº‹ä»¶æŽ¥å£èŽ·å–æ›´å¤§çš„çµæ´»æ€§ã€‚è¿™å¯¹äºŽè¿­ä»£çš„æˆ–ç”Ÿæˆå™¨çš„ç«¯ç‚¹ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä»¬ä¼šç”Ÿæˆä¸€ç³»åˆ—ç¦»æ•£çš„å“åº”å€¼ã€‚

```js
import { Client } from "@gradio/client";

function log_result(payload) {
	const {
		data: [translation]
	} = payload;

	console.log(`ç¿»è¯‘ç»“æžœä¸ºï¼š${translation}`);
}

const app = await Client.connect("abidlabs/en2fr");
const job = app.submit("/predict", ["Hello"]);

job.on("data", log_result);
```

## çŠ¶æ€

äº‹ä»¶æŽ¥å£è¿˜å¯ä»¥é€šè¿‡ç›‘å¬`"status"`äº‹ä»¶æ¥èŽ·å–è¿è¡Œä½œä¸šçš„çŠ¶æ€ã€‚è¿™å°†è¿”å›žä¸€ä¸ªå¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«ä»¥ä¸‹å±žæ€§ï¼š`status`ï¼ˆå½“å‰ä½œä¸šçš„äººç±»å¯è¯»çŠ¶æ€ï¼Œ`"pending" | "generating" | "complete" | "error"`ï¼‰ï¼Œ`code`ï¼ˆä½œä¸šçš„è¯¦ç»†gradio codeï¼‰ï¼Œ`position`ï¼ˆæ­¤ä½œä¸šåœ¨é˜Ÿåˆ—ä¸­çš„å½“å‰ä½ç½®ï¼‰ï¼Œ`queue_size`ï¼ˆæ€»é˜Ÿåˆ—å¤§å°ï¼‰ï¼Œ`eta`ï¼ˆä½œä¸šå®Œæˆçš„é¢„è®¡æ—¶é—´ï¼‰ï¼Œ`success`ï¼ˆè¡¨ç¤ºä½œä¸šæ˜¯å¦æˆåŠŸå®Œæˆçš„å¸ƒå°”å€¼ï¼‰å’Œ`time`ï¼ˆä½œä¸šçŠ¶æ€ç”Ÿæˆçš„æ—¶é—´ï¼Œæ˜¯ä¸€ä¸ª`Date`å¯¹è±¡ï¼‰ã€‚

```js
import { Client } from "@gradio/client";

function log_status(status) {
	console.log(`æ­¤ä½œä¸šçš„å½“å‰çŠ¶æ€ä¸ºï¼š${JSON.stringify(status, null, 2)}`);
}

const app = await Client.connect("abidlabs/en2fr");
const job = app.submit("/predict", ["Hello"]);

job.on("status", log_status);
```

## å–æ¶ˆä½œä¸š

ä½œä¸šå®žä¾‹è¿˜å…·æœ‰`.cancel()`æ–¹æ³•ï¼Œç”¨äºŽå–æ¶ˆå·²æŽ’é˜Ÿä½†å°šæœªå¯åŠ¨çš„ä½œä¸šã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```js
import { Client } from "@gradio/client";

const app = await Client.connect("abidlabs/en2fr");
const job_one = app.submit("/predict", ["Hello"]);
const job_two = app.submit("/predict", ["Friends"]);

job_one.cancel();
job_two.cancel();
```

å¦‚æžœç¬¬ä¸€ä¸ªä½œä¸šå·²ç»å¼€å§‹å¤„ç†ï¼Œé‚£ä¹ˆå®ƒå°†ä¸ä¼šè¢«å–æ¶ˆï¼Œä½†å®¢æˆ·ç«¯å°†ä¸å†ç›‘å¬æ›´æ–°ï¼ˆä¸¢å¼ƒè¯¥ä½œä¸šï¼‰ã€‚å¦‚æžœç¬¬äºŒä¸ªä½œä¸šå°šæœªå¯åŠ¨ï¼Œå®ƒå°†è¢«æˆåŠŸå–æ¶ˆå¹¶ä»Žé˜Ÿåˆ—ä¸­ç§»é™¤ã€‚

## ç”Ÿæˆå™¨ç«¯ç‚¹

æŸäº›Gradio APIç«¯ç‚¹ä¸è¿”å›žå•ä¸ªå€¼ï¼Œè€Œæ˜¯è¿”å›žä¸€ç³»åˆ—å€¼ã€‚æ‚¨å¯ä»¥ä½¿ç”¨äº‹ä»¶æŽ¥å£å®žæ—¶ä¾¦å¬è¿™äº›å€¼ï¼š

```js
import { Client } from "@gradio/client";

const app = await Client.connect("gradio/count_generator");
const job = app.submit(0, [9]);

job.on("data", (data) => console.log(data));
```

è¿™å°†æŒ‰ç”Ÿæˆç«¯ç‚¹ç”Ÿæˆçš„å€¼è¿›è¡Œæ—¥å¿—è®°å½•ã€‚

æ‚¨è¿˜å¯ä»¥å–æ¶ˆå…·æœ‰è¿­ä»£è¾“å‡ºçš„ä½œä¸šï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½œä¸šå°†ç«‹å³å®Œæˆã€‚

```js
import { Client } from "@gradio/client";

const app = await Client.connect("gradio/count_generator");
const job = app.submit(0, [9]);

job.on("data", (data) => console.log(data));

setTimeout(() => {
	job.cancel();
}, 3000);
```

---

<!-- Source: guides/02_building-interfaces/03_interface-state.md -->
# Interface State

So far, we've assumed that your demos are *stateless*: that they do not persist information beyond a single function call. What if you want to modify the behavior of your demo based on previous interactions with the demo? There are two approaches in Gradio: *global state* and *session state*.

## Global State

If the state is something that should be accessible to all function calls and all users, you can create a variable outside the function call and access it inside the function. For example, you may load a large model outside the function and use it inside the function so that every function call does not need to reload the model.

$code_score_tracker

In the code above, the `scores` array is shared between all users. If multiple users are accessing this demo, their scores will all be added to the same list, and the returned top 3 scores will be collected from this shared reference.

## Session State

Another type of data persistence Gradio supports is session state, where data persists across multiple submits within a page session. However, data is _not_ shared between different users of your model. To store data in a session state, you need to do three things:

1. Pass in an extra parameter into your function, which represents the state of the interface.
2. At the end of the function, return the updated value of the state as an extra return value.
3. Add the `'state'` input and `'state'` output components when creating your `Interface`

Here's a simple app to illustrate session state - this app simply stores users previous submissions and displays them back to the user:


$code_interface_state
$demo_interface_state


Notice how the state persists across submits within each page, but if you load this demo in another tab (or refresh the page), the demos will not share chat history. Here, we could not store the submission history in a global variable, otherwise the submission history would then get jumbled between different users.

The initial value of the `State` is `None` by default. If you pass a parameter to the `value` argument of `gr.State()`, it is used as the default value of the state instead. 

Note: the `Interface` class only supports a single session state variable (though it can be a list with multiple elements). For more complex use cases, you can use Blocks, [which supports multiple `State` variables](/guides/state-in-blocks/). Alternatively, if you are building a chatbot that maintains user state, consider using the `ChatInterface` abstraction, [which manages state automatically](/guides/creating-a-chatbot-fast).

---

<!-- Source: guides/03_building-with-blocks/03_state-in-blocks.md -->
# Managing State

When building a Gradio application with `gr.Blocks()`, you may want to share certain values between users (e.g. a count of visitors to your page), or persist values for a single user across certain interactions (e.g. a chat history). This referred to as **state** and there are three general ways to manage state in a Gradio application:

* **Global state**: persist and share values among all users of your Gradio application while your Gradio application is running
* **Session state**: persist values for each user of your Gradio application while they are using your Gradio application in a single session. If they refresh the page, session state will be reset.
* **Browser state**: persist values for each user of your Gradio application in the browser's localStorage, allowing data to persist even after the page is refreshed or closed.

## Global State

Global state in Gradio apps is very simple: any variable created outside of a function is shared globally between all users.

This makes managing global state very simple and without the need for external services. For example, in this application, the `visitor_count` variable is shared between all users

```py
import gradio as gr

# Shared between all users
visitor_count = 0

def increment_counter():
    global visitor_count
    visitor_count += 1
    return visitor_count

with gr.Blocks() as demo:    
    number = gr.Textbox(label="Total Visitors", value="Counting...")
    demo.load(increment_counter, inputs=None, outputs=number)

demo.launch()
```

This means that any time you do _not_ want to share a value between users, you should declare it _within_ a function. But what if you need to share values between function calls, e.g. a chat history? In that case, you should use one of the subsequent approaches to manage state.

## Session State

Gradio supports session state, where data persists across multiple submits within a page session. To reiterate, session data is _not_ shared between different users of your model, and does _not_ persist if a user refreshes the page to reload the Gradio app. To store data in a session state, you need to do three things:

1. Create a `gr.State()` object. If there is a default value to this stateful object, pass that into the constructor. Note that `gr.State` objects must be [deepcopy-able](https://docs.python.org/3/library/copy.html), otherwise you will need to use a different approach as described below.
2. In the event listener, put the `State` object as an input and output as needed.
3. In the event listener function, add the variable to the input parameters and the return value.

Let's take a look at a simple example. We have a simple checkout app below where you add items to a cart. You can also see the size of the cart.

$code_simple_state

Notice how we do this with state:

1. We store the cart items in a `gr.State()` object, initialized here to be an empty list.
2. When adding items to the cart, the event listener uses the cart as both input and output - it returns the updated cart with all the items inside. 
3. We can attach a `.change` listener to cart, that uses the state variable as input as well.

You can think of `gr.State` as an invisible Gradio component that can store any kind of value. Here, `cart` is not visible in the frontend but is used for calculations.

The `.change` listener for a state variable triggers after any event listener changes the value of a state variable. If the state variable holds a sequence (like a `list`, `set`, or `dict`), a change is triggered if any of the elements inside change. If it holds an object or primitive, a change is triggered if the **hash** of the  value changes. So if you define a custom class and create a `gr.State` variable that is an instance of that class, make sure that the the class includes a sensible `__hash__` implementation.

The value of a session State variable is cleared when the user refreshes the page. The value is stored on in the app backend for 60 minutes after the user closes the tab (this can be configured by the `delete_cache` parameter in `gr.Blocks`).

Learn more about `State` in the [docs](https://gradio.app/docs/gradio/state).

**What about objects that cannot be deepcopied?**

As mentioned earlier, the value stored in `gr.State` must be [deepcopy-able](https://docs.python.org/3/library/copy.html). If you are working with a complex object that cannot be deepcopied, you can take a different approach to manually read the user's `session_hash` and store a global `dictionary` with instances of your object for each user. Here's how you would do that:

```py
import gradio as gr

class NonDeepCopyable:
    def __init__(self):
        from threading import Lock
        self.counter = 0
        self.lock = Lock()  # Lock objects cannot be deepcopied
    
    def increment(self):
        with self.lock:
            self.counter += 1
            return self.counter

# Global dictionary to store user-specific instances
instances = {}

def initialize_instance(request: gr.Request):
    instances[request.session_hash] = NonDeepCopyable()
    return "Session initialized!"

def cleanup_instance(request: gr.Request):
    if request.session_hash in instances:
        del instances[request.session_hash]

def increment_counter(request: gr.Request):
    if request.session_hash in instances:
        instance = instances[request.session_hash]
        return instance.increment()
    return "Error: Session not initialized"

with gr.Blocks() as demo:
    output = gr.Textbox(label="Status")
    counter = gr.Number(label="Counter Value")
    increment_btn = gr.Button("Increment Counter")
    increment_btn.click(increment_counter, inputs=None, outputs=counter)
    
    # Initialize instance when page loads
    demo.load(initialize_instance, inputs=None, outputs=output)    
    # Clean up instance when page is closed/refreshed
    demo.unload(cleanup_instance)    

demo.launch()
```

## Browser State

Gradio also supports browser state, where data persists in the browser's localStorage even after the page is refreshed or closed. This is useful for storing user preferences, settings, API keys, or other data that should persist across sessions. To use local state:

1. Create a `gr.BrowserState` object. You can optionally provide an initial default value and a key to identify the data in the browser's localStorage.
2. Use it like a regular `gr.State` component in event listeners as inputs and outputs.

Here's a simple example that saves a user's username and password across sessions:

$code_browserstate

Note: The value stored in `gr.BrowserState` does not persist if the Grado app is restarted. To persist it, you can hardcode specific values of `storage_key` and `secret` in the `gr.BrowserState` component and restart the Gradio app on the same server name and server port. However, this should only be done if you are running trusted Gradio apps, as in principle, this can allow one Gradio app to access localStorage data that was created by a different Gradio app.

---

<!-- Source: guides/04_additional-features/03_streaming-inputs.md -->
# Streaming inputs

Tip: Check out [FastRTC](https://fastrtc.org/), our companion library for building low latency streaming web apps with a familiar Gradio syntax. 

In the previous guide, we covered how to stream a sequence of outputs from an event handler. Gradio also allows you to stream images from a user's camera or audio chunks from their microphone **into** your event handler. This can be used to create real-time object detection apps or conversational chat applications with Gradio.

Currently, the `gr.Image` and the `gr.Audio` components support input streaming via the `stream` event.
Let's create the simplest streaming app possible, which simply returns the webcam stream unmodified.

$code_streaming_simple
$demo_streaming_simple

Try it out! The streaming event is triggered when the user starts recording. Under the hood, the webcam will take a photo every 0.1 seconds and send it to the server. The server will then return that image.

There are two unique keyword arguments for the `stream` event:

* `time_limit` - This is the amount of time the gradio server will spend processing the event. Media streams are naturally unbounded so it's important to set a time limit so that one user does not hog the Gradio queue. The time limit only counts the time spent processing the stream, not the time spent waiting in the queue. The orange bar displayed at the bottom of the input image represents the remaining time. When the time limit expires, the user will automatically rejoin the queue.

* `stream_every` - This is the frequency (in seconds) with which the stream will capture input and send it to the server. For demos like image detection or manipulation, setting a smaller value is desired to get a "real-time" effect. For demos like speech transcription, a higher value is useful so that the transcription algorithm has more context of what's being said.

## A Realistic Image Demo

Let's create a demo where a user can choose a filter to apply to their webcam stream. Users can choose from an edge-detection filter, a cartoon filter, or simply flipping the stream vertically.

$code_streaming_filter
$demo_streaming_filter

You will notice that if you change the filter value it will immediately take effect in the output stream. That is an important difference of stream events in comparison to other Gradio events. The input values of the stream can be changed while the stream is being processed. 

Tip: We set the "streaming" parameter of the image output component to be "True". Doing so lets the server automatically convert our output images into base64 format, a format that is efficient for streaming.

## Unified Image Demos

For some image streaming demos, like the one above, we don't need to display separate input and output components. Our app would look cleaner if we could just display the modified output stream.

We can do so by just specifying the input image component as the output of the stream event.

$code_streaming_filter_unified
$demo_streaming_filter_unified

## Keeping track of past inputs or outputs

Your streaming function should be stateless. It should take the current input and return its corresponding output. However, there are cases where you may want to keep track of past inputs or outputs. For example, you may want to keep a buffer of the previous `k` inputs to improve the accuracy of your transcription demo. You can do this with Gradio's `gr.State()` component.

Let's showcase this with a sample demo:

```python
def transcribe_handler(current_audio, state, transcript):
    next_text = transcribe(current_audio, history=state)
    state.append(current_audio)
    state = state[-3:]
    return state, transcript + next_text

with gr.Blocks() as demo:
    with gr.Row():
        with gr.Column():
            mic = gr.Audio(sources="microphone")
            state = gr.State(value=[])
        with gr.Column():
            transcript = gr.Textbox(label="Transcript")
    mic.stream(transcribe_handler, [mic, state, transcript], [state, transcript],
               time_limit=10, stream_every=1)


demo.launch()
```

## End-to-End Examples

For an end-to-end example of streaming from the webcam, see the object detection from webcam [guide](/main/guides/object-detection-from-webcam-with-webrtc).

---

<!-- Source: guides/05_chatbots/03_agents-and-tool-usage.md -->
# Building a UI for an LLM Agent

Tags: LLM, AGENTS, CHAT

The Gradio Chatbot can natively display intermediate thoughts and tool usage in a collapsible accordion next to a chat message. This makes it perfect for creating UIs for LLM agents and chain-of-thought (CoT) or reasoning demos. This guide will show you how to display thoughts and tool usage with `gr.Chatbot` and `gr.ChatInterface`.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/nested-thoughts.png)

## The `ChatMessage` dataclass

Every element of the chatbot value is a dictionary of `role` and `content` keys. You can always use plain python dictionaries to add new values to the chatbot but Gradio also provides the `ChatMessage` dataclass to help you with IDE autocompletion. The schema of `ChatMessage` is as follows:

 ```py
MessageContent = Union[str, FileDataDict, FileData, Component]

@dataclass
class ChatMessage:
    content: MessageContent | [MessageContent]
    role: Literal["user", "assistant"]
    metadata: MetadataDict = None
    options: list[OptionDict] = None

class MetadataDict(TypedDict):
    title: NotRequired[str]
    id: NotRequired[int | str]
    parent_id: NotRequired[int | str]
    log: NotRequired[str]
    duration: NotRequired[float]
    status: NotRequired[Literal["pending", "done"]]

class OptionDict(TypedDict):
    label: NotRequired[str]
    value: str
 ```


For our purposes, the most important key is the `metadata` key, which accepts a dictionary. If this dictionary includes a `title` for the message, it will be displayed in a collapsible accordion representing a thought. It's that simple! Take a look at this example:


```python
import gradio as gr

with gr.Blocks() as demo:
    chatbot = gr.Chatbot(
        value=[
            gr.ChatMessage(
                role="user", 
                content="What is the weather in San Francisco?"
            ),
            gr.ChatMessage(
                role="assistant", 
                content="I need to use the weather API tool?",
                metadata={"title":  "ðŸ§  Thinking"}
            )
        ]
    )

demo.launch()
```



In addition to `title`, the dictionary provided to `metadata` can take several optional keys:

* `log`: an optional string value to be displayed in a subdued font next to the thought title.
* `duration`: an optional numeric value representing the duration of the thought/tool usage, in seconds. Displayed in a subdued font next inside parentheses next to the thought title.
* `status`: if set to `"pending"`, a spinner appears next to the thought title and the accordion is initialized open.  If `status` is `"done"`, the thought accordion is initialized closed. If `status` is not provided, the thought accordion is initialized open and no spinner is displayed.
* `id` and `parent_id`: if these are provided, they can be used to nest thoughts inside other thoughts.

Below, we show several complete examples of using `gr.Chatbot` and `gr.ChatInterface` to display tool use or thinking UIs.

## Building with Agents

### A real example using transformers.agents

We'll create a Gradio application simple agent that has access to a text-to-image tool.

Tip: Make sure you read the [smolagents documentation](https://huggingface.co/docs/smolagents/index) first

We'll start by importing the necessary classes from transformers and gradio. 

```python
import gradio as gr
from gradio import ChatMessage
from transformers import Tool, ReactCodeAgent  # type: ignore
from transformers.agents import stream_to_gradio, HfApiEngine  # type: ignore

# Import tool from Hub
image_generation_tool = Tool.from_space(
    space_id="black-forest-labs/FLUX.1-schnell",
    name="image_generator",
    description="Generates an image following your prompt. Returns a PIL Image.",
    api_name="/infer",
)

llm_engine = HfApiEngine("Qwen/Qwen2.5-Coder-32B-Instruct")
# Initialize the agent with both tools and engine
agent = ReactCodeAgent(tools=[image_generation_tool], llm_engine=llm_engine)
```

Then we'll build the UI:

```python
def interact_with_agent(prompt, history):
    messages = []
    yield messages
    for msg in stream_to_gradio(agent, prompt):
        messages.append(asdict(msg))
        yield messages
    yield messages


demo = gr.ChatInterface(
    interact_with_agent,
    chatbot= gr.Chatbot(
        label="Agent",
        avatar_images=(
            None,
            "https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png",
        ),
    ),
    examples=[
        ["Generate an image of an astronaut riding an alligator"],
        ["I am writing a children's book for my daughter. Can you help me with some illustrations?"],
    ],
)
```

You can see the full demo code [here](https://huggingface.co/spaces/gradio/agent_chatbot/blob/main/app.py).


![transformers_agent_code](https://github.com/freddyaboulton/freddyboulton/assets/41651716/c8d21336-e0e6-4878-88ea-e6fcfef3552d)


### A real example using langchain agents

We'll create a UI for langchain agent that has access to a search engine.

We'll begin with imports and setting up the langchain agent. Note that you'll need an .env file with the following environment variables set - 

```
SERPAPI_API_KEY=
HF_TOKEN=
OPENAI_API_KEY=
```

```python
from langchain import hub
from langchain.agents import AgentExecutor, create_openai_tools_agent, load_tools
from langchain_openai import ChatOpenAI
from gradio import ChatMessage
import gradio as gr

from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(temperature=0, streaming=True)

tools = load_tools(["serpapi"])

# Get the prompt to use - you can modify this!
prompt = hub.pull("hwchase17/openai-tools-agent")
agent = create_openai_tools_agent(
    model.with_config({"tags": ["agent_llm"]}), tools, prompt
)
agent_executor = AgentExecutor(agent=agent, tools=tools).with_config(
    {"run_name": "Agent"}
)
```

Then we'll create the Gradio UI

```python
async def interact_with_langchain_agent(prompt, messages):
    messages.append(ChatMessage(role="user", content=prompt))
    yield messages
    async for chunk in agent_executor.astream(
        {"input": prompt}
    ):
        if "steps" in chunk:
            for step in chunk["steps"]:
                messages.append(ChatMessage(role="assistant", content=step.action.log,
                                  metadata={"title": f"ðŸ› ï¸ Used tool {step.action.tool}"}))
                yield messages
        if "output" in chunk:
            messages.append(ChatMessage(role="assistant", content=chunk["output"]))
            yield messages


with gr.Blocks() as demo:
    gr.Markdown("# Chat with a LangChain Agent ðŸ¦œâ›“ï¸ and see its thoughts ðŸ’­")
    chatbot = gr.Chatbot(
        label="Agent",
        avatar_images=(
            None,
            "https://em-content.zobj.net/source/twitter/141/parrot_1f99c.png",
        ),
    )
    input = gr.Textbox(lines=1, label="Chat Message")
    input.submit(interact_with_langchain_agent, [input_2, chatbot_2], [chatbot_2])

demo.launch()
```

![langchain_agent_code](https://github.com/freddyaboulton/freddyboulton/assets/41651716/762283e5-3937-47e5-89e0-79657279ea67)

That's it! See our finished langchain demo [here](https://huggingface.co/spaces/gradio/langchain-agent).


## Building with Visibly Thinking LLMs


The Gradio Chatbot can natively display intermediate thoughts of a _thinking_ LLM. This makes it perfect for creating UIs that show how an AI model "thinks" while generating responses. Below guide will show you how to build a chatbot that displays Gemini AI's thought process in real-time.


### A real example using Gemini 2.0 Flash Thinking API

Let's create a complete chatbot that shows its thoughts and responses in real-time. We'll use Google's Gemini API for accessing Gemini 2.0 Flash Thinking LLM and Gradio for the UI.

We'll begin with imports and setting up the gemini client. Note that you'll need to [acquire a Google Gemini API key](https://aistudio.google.com/apikey) first -

```python
import gradio as gr
from gradio import ChatMessage
from typing import Iterator
import google.generativeai as genai

genai.configure(api_key="your-gemini-api-key")
model = genai.GenerativeModel("gemini-2.0-flash-thinking-exp-1219")
```

First, let's set up our streaming function that handles the model's output:

```python
def stream_gemini_response(user_message: str, messages: list) -> Iterator[list]:
    """
    Streams both thoughts and responses from the Gemini model.
    """
    # Initialize response from Gemini
    response = model.generate_content(user_message, stream=True)
    
    # Initialize buffers
    thought_buffer = ""
    response_buffer = ""
    thinking_complete = False
    
    # Add initial thinking message
    messages.append(
        ChatMessage(
            role="assistant",
            content="",
            metadata={"title": "â³Thinking: *The thoughts produced by the Gemini2.0 Flash model are experimental"}
        )
    )
    
    for chunk in response:
        parts = chunk.candidates[0].content.parts
        current_chunk = parts[0].text
        
        if len(parts) == 2 and not thinking_complete:
            # Complete thought and start response
            thought_buffer += current_chunk
            messages[-1] = ChatMessage(
                role="assistant",
                content=thought_buffer,
                metadata={"title": "â³Thinking: *The thoughts produced by the Gemini2.0 Flash model are experimental"}
            )
            
            # Add response message
            messages.append(
                ChatMessage(
                    role="assistant",
                    content=parts[1].text
                )
            )
            thinking_complete = True
            
        elif thinking_complete:
            # Continue streaming response
            response_buffer += current_chunk
            messages[-1] = ChatMessage(
                role="assistant",
                content=response_buffer
            )
            
        else:
            # Continue streaming thoughts
            thought_buffer += current_chunk
            messages[-1] = ChatMessage(
                role="assistant",
                content=thought_buffer,
                metadata={"title": "â³Thinking: *The thoughts produced by the Gemini2.0 Flash model are experimental"}
            )
        
        yield messages
```

Then, let's create the Gradio interface:

```python
with gr.Blocks() as demo:
    gr.Markdown("# Chat with Gemini 2.0 Flash and See its Thoughts ðŸ’­")
    
    chatbot = gr.Chatbot(
        label="Gemini2.0 'Thinking' Chatbot",
        render_markdown=True,
    )
    
    input_box = gr.Textbox(
        lines=1,
        label="Chat Message",
        placeholder="Type your message here and press Enter..."
    )
    
    # Set up event handlers
    msg_store = gr.State("")  # Store for preserving user message
    
    input_box.submit(
        lambda msg: (msg, msg, ""),  # Store message and clear input
        inputs=[input_box],
        outputs=[msg_store, input_box, input_box],
        queue=False
    ).then(
        user_message,  # Add user message to chat
        inputs=[msg_store, chatbot],
        outputs=[input_box, chatbot],
        queue=False
    ).then(
        stream_gemini_response,  # Generate and stream response
        inputs=[msg_store, chatbot],
        outputs=chatbot
    )

demo.launch()
```

This creates a chatbot that:

- Displays the model's thoughts in a collapsible section
- Streams the thoughts and final response in real-time
- Maintains a clean chat history

 That's it! You now have a chatbot that not only responds to users but also shows its thinking process, creating a more transparent and engaging interaction. See our finished Gemini 2.0 Flash Thinking demo [here](https://huggingface.co/spaces/ysharma/Gemini2-Flash-Thinking).


 ## Building with Citations 

The Gradio Chatbot can display citations from LLM responses, making it perfect for creating UIs that show source documentation and references. This guide will show you how to build a chatbot that displays Claude's citations in real-time.

### A real example using Anthropic's Citations API
Let's create a complete chatbot that shows both responses and their supporting citations. We'll use Anthropic's Claude API with citations enabled and Gradio for the UI.

We'll begin with imports and setting up the Anthropic client. Note that you'll need an `ANTHROPIC_API_KEY` environment variable set:

```python
import gradio as gr
import anthropic
import base64
from typing import List, Dict, Any

client = anthropic.Anthropic()
```

First, let's set up our message formatting functions that handle document preparation:

```python
def encode_pdf_to_base64(file_obj) -> str:
    """Convert uploaded PDF file to base64 string."""
    if file_obj is None:
        return None
    with open(file_obj.name, 'rb') as f:
        return base64.b64encode(f.read()).decode('utf-8')

def format_message_history(
    history: list, 
    enable_citations: bool,
    doc_type: str,
    text_input: str,
    pdf_file: str
) -> List[Dict]:
    """Convert Gradio chat history to Anthropic message format."""
    formatted_messages = []
    
    # Add previous messages
    for msg in history[:-1]:
        if msg["role"] == "user":
            formatted_messages.append({"role": "user", "content": msg["content"]})
    
    # Prepare the latest message with document
    latest_message = {"role": "user", "content": []}
    
    if enable_citations:
        if doc_type == "plain_text":
            latest_message["content"].append({
                "type": "document",
                "source": {
                    "type": "text",
                    "media_type": "text/plain",
                    "data": text_input.strip()
                },
                "title": "Text Document",
                "citations": {"enabled": True}
            })
        elif doc_type == "pdf" and pdf_file:
            pdf_data = encode_pdf_to_base64(pdf_file)
            if pdf_data:
                latest_message["content"].append({
                    "type": "document",
                    "source": {
                        "type": "base64",
                        "media_type": "application/pdf",
                        "data": pdf_data
                    },
                    "title": pdf_file.name,
                    "citations": {"enabled": True}
                })
    
    # Add the user's question
    latest_message["content"].append({"type": "text", "text": history[-1]["content"]})
    
    formatted_messages.append(latest_message)
    return formatted_messages
```

Then, let's create our bot response handler that processes citations:

```python
def bot_response(
    history: list,
    enable_citations: bool,
    doc_type: str,
    text_input: str,
    pdf_file: str
) -> List[Dict[str, Any]]:
    try:
        messages = format_message_history(history, enable_citations, doc_type, text_input, pdf_file)
        response = client.messages.create(model="claude-3-5-sonnet-20241022", max_tokens=1024, messages=messages)
        
        # Initialize main response and citations
        main_response = ""
        citations = []
        
        # Process each content block
        for block in response.content:
            if block.type == "text":
                main_response += block.text
                if enable_citations and hasattr(block, 'citations') and block.citations:
                    for citation in block.citations:
                        if citation.cited_text not in citations:
                            citations.append(citation.cited_text)
        
        # Add main response
        history.append({"role": "assistant", "content": main_response})
        
        # Add citations in a collapsible section
        if enable_citations and citations:
            history.append({
                "role": "assistant",
                "content": "\n".join([f"â€¢ {cite}" for cite in citations]),
                "metadata": {"title": "ðŸ“š Citations"}
            })
        
        return history
            
    except Exception as e:
        history.append({
            "role": "assistant",
            "content": "I apologize, but I encountered an error while processing your request."
        })
        return history
```

Finally, let's create the Gradio interface:

```python
with gr.Blocks() as demo:
    gr.Markdown("# Chat with Citations")
    
    with gr.Row(scale=1):
        with gr.Column(scale=4):
            chatbot = gr.Chatbot(bubble_full_width=False, show_label=False, scale=1)
            msg = gr.Textbox(placeholder="Enter your message here...", show_label=False, container=False)
            
        with gr.Column(scale=1):
            enable_citations = gr.Checkbox(label="Enable Citations", value=True, info="Toggle citation functionality" )
            doc_type_radio = gr.Radio( choices=["plain_text", "pdf"], value="plain_text", label="Document Type", info="Choose the type of document to use")
            text_input = gr.Textbox(label="Document Content", lines=10, info="Enter the text you want to reference")
            pdf_input = gr.File(label="Upload PDF", file_types=[".pdf"], file_count="single", visible=False)
    
    # Handle message submission
    msg.submit(
        user_message,
        [msg, chatbot, enable_citations, doc_type_radio, text_input, pdf_input],
        [msg, chatbot]
    ).then(
        bot_response,
        [chatbot, enable_citations, doc_type_radio, text_input, pdf_input],
        chatbot
    )

demo.launch()
```

This creates a chatbot that:
- Supports both plain text and PDF documents for Claude to cite from 
- Displays Citations in collapsible sections using our `metadata` feature
- Shows source quotes directly from the given documents

The citations feature works particularly well with the Gradio Chatbot's `metadata` support, allowing us to create collapsible sections that keep the chat interface clean while still providing easy access to source documentation.

That's it! You now have a chatbot that not only responds to users but also shows its sources, creating a more transparent and trustworthy interaction. See our finished Citations demo [here](https://huggingface.co/spaces/ysharma/anthropic-citations-with-gradio-metadata-key).

---

<!-- Source: guides/06_data-science-and-plots/03_filters-tables-and-stats.md -->
# Filters, Tables and Stats

Your dashboard will likely consist of more than just plots. Let's take a look at some of the other common components of a dashboard.

## Filters

Use any of the standard Gradio form components to filter your data. You can do this via event listeners or function-as-value syntax. Let's look at the event listener approach first:

$code_plot_guide_filters_events
$demo_plot_guide_filters_events

And this would be the function-as-value approach for the same demo.

$code_plot_guide_filters

## Tables and Stats

Add `gr.DataFrame` and `gr.Label` to your dashboard for some hard numbers.

$code_plot_guide_tables_stats
$demo_plot_guide_tables_stats

---

<!-- Source: guides/07_streaming/03_object-detection-from-video.md -->
# Streaming Object Detection from Video

Tags: VISION, STREAMING, VIDEO

In this guide we'll use the [RT-DETR](https://huggingface.co/docs/transformers/en/model_doc/rt_detr) model to detect objects in a user uploaded video. We'll stream the results from the server using the new video streaming features introduced in Gradio 5.0.

![video_object_detection_stream_latest](https://github.com/user-attachments/assets/4e27ac58-5ded-495d-9e0d-5e87e68b1355)

## Setting up the Model

First, we'll install the following requirements in our system:

```
opencv-python
torch
transformers>=4.43.0
spaces
```

Then, we'll download the model from the Hugging Face Hub:

```python
from transformers import RTDetrForObjectDetection, RTDetrImageProcessor

image_processor = RTDetrImageProcessor.from_pretrained("PekingU/rtdetr_r50vd")
model = RTDetrForObjectDetection.from_pretrained("PekingU/rtdetr_r50vd").to("cuda")
```
We're moving the model to the GPU. We'll be deploying our model to Hugging Face Spaces and running the inference in the [free ZeroGPU cluster](https://huggingface.co/zero-gpu-explorers). 


## The Inference Function

Our inference function will accept a video and a desired confidence threshold.
Object detection models identify many objects and assign a confidence score to each object. The lower the confidence, the higher the chance of a false positive. So we will let our users set the confidence threshold.

Our function will iterate over the frames in the video and run the RT-DETR model over each frame.
We will then draw the bounding boxes for each detected object in the frame and save the frame to a new output video.
The function will yield each output video in chunks of two seconds.

In order to keep inference times as low as possible on ZeroGPU (there is a time-based quota),
we will halve the original frames-per-second in the output video and resize the input frames to be half the original 
size before running the model.

The code for the inference function is below - we'll go over it piece by piece.

```python
import spaces
import cv2
from PIL import Image
import torch
import time
import numpy as np
import uuid

from draw_boxes import draw_bounding_boxes

SUBSAMPLE = 2

@spaces.GPU
def stream_object_detection(video, conf_threshold):
    cap = cv2.VideoCapture(video)

    # This means we will output mp4 videos
    video_codec = cv2.VideoWriter_fourcc(*"mp4v") # type: ignore
    fps = int(cap.get(cv2.CAP_PROP_FPS))

    desired_fps = fps // SUBSAMPLE
    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) // 2
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) // 2

    iterating, frame = cap.read()

    n_frames = 0

    # Use UUID to create a unique video file
    output_video_name = f"output_{uuid.uuid4()}.mp4"

    # Output Video
    output_video = cv2.VideoWriter(output_video_name, video_codec, desired_fps, (width, height)) # type: ignore
    batch = []

    while iterating:
        frame = cv2.resize( frame, (0,0), fx=0.5, fy=0.5)
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        if n_frames % SUBSAMPLE == 0:
            batch.append(frame)
        if len(batch) == 2 * desired_fps:
            inputs = image_processor(images=batch, return_tensors="pt").to("cuda")

            with torch.no_grad():
                outputs = model(**inputs)

            boxes = image_processor.post_process_object_detection(
                outputs,
                target_sizes=torch.tensor([(height, width)] * len(batch)),
                threshold=conf_threshold)
            
            for i, (array, box) in enumerate(zip(batch, boxes)):
                pil_image = draw_bounding_boxes(Image.fromarray(array), box, model, conf_threshold)
                frame = np.array(pil_image)
                # Convert RGB to BGR
                frame = frame[:, :, ::-1].copy()
                output_video.write(frame)

            batch = []
            output_video.release()
            yield output_video_name
            output_video_name = f"output_{uuid.uuid4()}.mp4"
            output_video = cv2.VideoWriter(output_video_name, video_codec, desired_fps, (width, height)) # type: ignore

        iterating, frame = cap.read()
        n_frames += 1
```

1. **Reading from the Video**

One of the industry standards for creating videos in python is OpenCV so we will use it in this app.

The `cap` variable is how we will read from the input video. Whenever we call `cap.read()`, we are reading the next frame in the video.

In order to stream video in Gradio, we need to yield a different video file for each "chunk" of the output video.
We create the next video file to write to with the `output_video = cv2.VideoWriter(output_video_name, video_codec, desired_fps, (width, height))` line. The `video_codec` is how we specify the type of video file. Only "mp4" and "ts" files are supported for video sreaming at the moment.


2. **The Inference Loop**

For each frame in the video, we will resize it to be half the size. OpenCV reads files in `BGR` format, so will convert to the expected `RGB` format of transfomers. That's what the first two lines of the while loop are doing. 

We take every other frame and add it to a `batch` list so that the output video is half the original FPS. When the batch covers two seconds of video, we will run the model. The two second threshold was chosen to keep the processing time of each batch small enough so that video is smoothly displayed in the server while not requiring too many separate forward passes. In order for video streaming to work properly in Gradio, the batch size should be at least 1 second. 

We run the forward pass of the model and then use the `post_process_object_detection` method of the model to scale the detected bounding boxes to the size of the input frame.

We make use of a custom function to draw the bounding boxes (source [here](https://huggingface.co/spaces/gradio/rt-detr-object-detection/blob/main/draw_boxes.py#L14)). We then have to convert from `RGB` to `BGR` before writing back to the output video.

Once we have finished processing the batch, we create a new output video file for the next batch.

## The Gradio Demo

The UI code is pretty similar to other kinds of Gradio apps. 
We'll use a standard two-column layout so that users can see the input and output videos side by side.

In order for streaming to work, we have to set `streaming=True` in the output video. Setting the video
to autoplay is not necessary but it's a better experience for users.

```python
import gradio as gr

with gr.Blocks() as app:
    gr.HTML(
        """
    <h1 style='text-align: center'>
    Video Object Detection with <a href='https://huggingface.co/PekingU/rtdetr_r101vd_coco_o365' target='_blank'>RT-DETR</a>
    </h1>
    """)
    with gr.Row():
        with gr.Column():
            video = gr.Video(label="Video Source")
            conf_threshold = gr.Slider(
                label="Confidence Threshold",
                minimum=0.0,
                maximum=1.0,
                step=0.05,
                value=0.30,
            )
        with gr.Column():
            output_video = gr.Video(label="Processed Video", streaming=True, autoplay=True)

    video.upload(
        fn=stream_object_detection,
        inputs=[video, conf_threshold],
        outputs=[output_video],
    )


```


## Conclusion

You can check out our demo hosted on Hugging Face Spaces [here](https://huggingface.co/spaces/gradio/rt-detr-object-detection). 

It is also embedded on this page below

$demo_rt-detr-object-detection

---

<!-- Source: guides/08_custom-components/03_configuration.md -->
# Configuring Your Custom Component

The custom components workflow focuses on [convention over configuration](https://en.wikipedia.org/wiki/Convention_over_configuration) to reduce the number of decisions you as a developer need to make when developing your custom component.
That being said, you can still configure some aspects of the custom component package and directory.
This guide will cover how.

## The Package Name

By default, all custom component packages are called `gradio_<component-name>` where `component-name` is the name of the component's python class in lowercase.

As an example, let's walkthrough changing the name of a component from `gradio_mytextbox` to `supertextbox`. 

1. Modify the `name` in the `pyproject.toml` file. 

```bash
[project]
name = "supertextbox"
```

2. Change all occurrences of `gradio_<component-name>` in `pyproject.toml` to `<component-name>`

```bash
[tool.hatch.build]
artifacts = ["/backend/supertextbox/templates", "*.pyi"]

[tool.hatch.build.targets.wheel]
packages = ["/backend/supertextbox"]
```

3. Rename the `gradio_<component-name>` directory in `backend/` to `<component-name>`

```bash
mv backend/gradio_mytextbox backend/supertextbox
```


Tip: Remember to change the import statement in `demo/app.py`!

## Top Level Python Exports

By default, only the custom component python class is a top level export. 
This means that when users type `from gradio_<component-name> import ...`, the only class that will be available is the custom component class.
To add more classes as top level exports, modify the `__all__` property in `__init__.py`

```python
from .mytextbox import MyTextbox
from .mytextbox import AdditionalClass, additional_function

__all__ = ['MyTextbox', 'AdditionalClass', 'additional_function']
```

## Python Dependencies

You can add python dependencies by modifying the `dependencies` key in `pyproject.toml`

```bash
dependencies = ["gradio", "numpy", "PIL"]
```


Tip: Remember to run `gradio cc install` when you add dependencies!

## Javascript Dependencies

You can add JavaScript dependencies by modifying the `"dependencies"` key in `frontend/package.json`

```json
"dependencies": {
    "@gradio/atoms": "0.2.0-beta.4",
    "@gradio/statustracker": "0.3.0-beta.6",
    "@gradio/utils": "0.2.0-beta.4",
    "your-npm-package": "<version>"
}
```

## Directory Structure

By default, the CLI will place the Python code in `backend` and the JavaScript code in `frontend`.
It is not recommended to change this structure since it makes it easy for a potential contributor to look at your source code and know where everything is.
However, if you did want to this is what you would have to do:

1. Place the Python code in the subdirectory of your choosing. Remember to modify the `[tool.hatch.build]` `[tool.hatch.build.targets.wheel]` in the `pyproject.toml` to match!

2. Place the JavaScript code in the subdirectory of your choosing.

2. Add the `FRONTEND_DIR` property on the component python class. It must be the relative path from the file where the class is defined to the location of the JavaScript directory.

```python
class SuperTextbox(Component):
    FRONTEND_DIR = "../../frontend/"
```

The JavaScript and Python directories must be under the same common directory!

## Conclusion


Sticking to the defaults will make it easy for others to understand and contribute to your custom component.
After all, the beauty of open source is that anyone can help improve your code!
But if you ever need to deviate from the defaults, you know how!

---

<!-- Source: guides/09_gradio-clients-and-lite/03_querying-gradio-apps-with-curl.md -->
# Querying Gradio Apps with Curl

Tags: CURL, API, SPACES

It is possible to use any Gradio app as an API using cURL, the command-line tool that is pre-installed on many operating systems. This is particularly useful if you are trying to query a Gradio app from an environment other than Python or Javascript (since specialized Gradio clients exist for both [Python](/guides/getting-started-with-the-python-client) and [Javascript](/guides/getting-started-with-the-js-client)).

As an example, consider this Gradio demo that translates text from English to French: https://abidlabs-en2fr.hf.space/.

Using `curl`, we can translate text programmatically.

Here's the code to do it:

```bash
$ curl -X POST https://abidlabs-en2fr.hf.space/call/predict -H "Content-Type: application/json" -d '{
  "data": ["Hello, my friend."] 
}'

>> {"event_id": $EVENT_ID}   
```

```bash
$ curl -N https://abidlabs-en2fr.hf.space/call/predict/$EVENT_ID

>> event: complete
>> data: ["Bonjour, mon ami."]
```


Note: making a prediction and getting a result requires two `curl` requests: a `POST` and a `GET`. The `POST` request returns an `EVENT_ID` and prints  it to the console, which is used in the second `GET` request to fetch the results. You can combine these into a single command using `awk` and `read` to parse the results of the first command and pipe into the second, like this:

```bash
$ curl -X POST https://abidlabs-en2fr.hf.space/call/predict -H "Content-Type: application/json" -d '{
  "data": ["Hello, my friend."] 
}' \
  | awk -F'"' '{ print $4}'  \
  | read EVENT_ID; curl -N https://abidlabs-en2fr.hf.space/call/predict/$EVENT_ID

>> event: complete
>> data: ["Bonjour, mon ami."]
```

In the rest of this Guide, we'll explain these two steps in more detail and provide additional examples of querying Gradio apps with `curl`.


**Prerequisites**: For this Guide, you do _not_ need to know how to build Gradio apps in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components.

## Installation

You generally don't need to install cURL, as it comes pre-installed on many operating systems. Run:

```bash
curl --version
```

to confirm that `curl` is installed. If it is not already installed, you can install it by visiting https://curl.se/download.html. 


## Step 0: Get the URL for your Gradio App 

To query a Gradio app, you'll need its full URL. This is usually just the URL that the Gradio app is hosted on, for example: https://bec81a83-5b5c-471e.gradio.live


**Hugging Face Spaces**

However, if you are querying a Gradio on Hugging Face Spaces, you will need to use the URL of the embedded Gradio app, not the URL of the Space webpage. For example:

```bash
âŒ Space URL: https://huggingface.co/spaces/abidlabs/en2fr
âœ… Gradio app URL: https://abidlabs-en2fr.hf.space/
```

You can get the Gradio app URL by clicking the "view API" link at the bottom of the page. Or, you can right-click on the page and then click on "View Frame Source" or the equivalent in your browser to view the URL of the embedded Gradio app.

While you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space,
and then use it to make as many requests as you'd like!

Note: to query private Spaces, you will need to pass in your Hugging Face (HF) token. You can get your HF token here: https://huggingface.co/settings/tokens. In this case, you will need to include an additional header in both of your `curl` calls that we'll discuss below:

```bash
-H "Authorization: Bearer $HF_TOKEN"
```

Now, we are ready to make the two `curl` requests.

## Step 1: Make a Prediction (POST)

The first of the two `curl` requests is `POST` request that submits the input payload to the Gradio app. 

The syntax of the `POST` request is as follows:

```bash
$ curl -X POST $URL/call/$API_NAME -H "Content-Type: application/json" -d '{
  "data": $PAYLOAD
}'
```

Here:

* `$URL` is the URL of the Gradio app as obtained in Step 0
* `$API_NAME` is the name of the API endpoint for the event that you are running. You can get the API endpoint names by clicking the "view API" link at the bottom of the page.
*  `$PAYLOAD` is a valid JSON data list containing the input payload, one element for each input component.

When you make this `POST` request successfully, you will get an event id that is printed to the terminal in this format:

```bash
>> {"event_id": $EVENT_ID}   
```

This `EVENT_ID` will be needed in the subsequent `curl` request to fetch the results of the prediction. 

Here are some examples of how to make the `POST` request

**Basic Example**

Revisiting the example at the beginning of the page, here is how to make the `POST` request for a simple Gradio application that takes in a single input text component:

```bash
$ curl -X POST https://abidlabs-en2fr.hf.space/call/predict -H "Content-Type: application/json" -d '{
  "data": ["Hello, my friend."] 
}'
```

**Multiple Input Components**

This [Gradio demo](https://huggingface.co/spaces/gradio/hello_world_3) accepts three inputs: a string corresponding to the `gr.Textbox`, a boolean value corresponding to the `gr.Checkbox`, and a numerical value corresponding to the `gr.Slider`. Here is the `POST` request:

```bash
curl -X POST https://gradio-hello-world-3.hf.space/call/predict -H "Content-Type: application/json" -d '{
  "data": ["Hello", true, 5]
}'
```

**Private Spaces**

As mentioned earlier, if you are making a request to a private Space, you will need to pass in a [Hugging Face token](https://huggingface.co/settings/tokens) that has read access to the Space. The request will look like this:

```bash
$ curl -X POST https://private-space.hf.space/call/predict -H "Content-Type: application/json" -H "Authorization: Bearer $HF_TOKEN" -d '{
  "data": ["Hello, my friend."] 
}'
```

**Files**

If you are using `curl` to query a Gradio application that requires file inputs, the files *need* to be provided as URLs, and The URL needs to be enclosed in a dictionary in this format:

```bash
{"path": $URL}
```

Here is an example `POST` request:

```bash
$ curl -X POST https://gradio-image-mod.hf.space/call/predict -H "Content-Type: application/json" -d '{
  "data": [{"path": "https://raw.githubusercontent.com/gradio-app/gradio/main/test/test_files/bus.png"}] 
}'
```


**Stateful Demos**

If your Gradio demo [persists user state](/guides/interface-state) across multiple interactions (e.g. is a chatbot), you can pass in a `session_hash` alongside the `data`. Requests with the same `session_hash` are assumed to be part of the same user session. Here's how that might look:

```bash
# These two requests will share a session

curl -X POST https://gradio-chatinterface-random-response.hf.space/call/chat -H "Content-Type: application/json" -d '{
  "data": ["Are you sentient?"],
  "session_hash": "randomsequence1234"
}'

curl -X POST https://gradio-chatinterface-random-response.hf.space/call/chat -H "Content-Type: application/json" -d '{
  "data": ["Really?"],
  "session_hash": "randomsequence1234"
}'

# This request will be treated as a new session

curl -X POST https://gradio-chatinterface-random-response.hf.space/call/chat -H "Content-Type: application/json" -d '{
  "data": ["Are you sentient?"],
  "session_hash": "newsequence5678"
}'
```



## Step 2: GET the result

Once you have received the `EVENT_ID` corresponding to your prediction, you can stream the results. Gradio stores these results  in a least-recently-used cache in the Gradio app. By default, the cache can store 2,000 results (across all users and endpoints of the app). 

To stream the results for your prediction, make a `GET` request with the following syntax:

```bash
$ curl -N $URL/call/$API_NAME/$EVENT_ID
```


Tip: If you are fetching results from a private Space, include a header with your HF token like this: `-H "Authorization: Bearer $HF_TOKEN"` in the `GET` request.

This should produce a stream of responses in this format:

```bash
event: ... 
data: ...
event: ... 
data: ...
...
```

Here: `event` can be one of the following:
* `generating`: indicating an intermediate result
* `complete`: indicating that the prediction is complete and the final result 
* `error`: indicating that the prediction was not completed successfully
* `heartbeat`: sent every 15 seconds to keep the request alive

The `data` is in the same format as the input payload: valid JSON data list containing the output result, one element for each output component.

Here are some examples of what results you should expect if a request is completed successfully:

**Basic Example**

Revisiting the example at the beginning of the page, we would expect the result to look like this:

```bash
event: complete
data: ["Bonjour, mon ami."]
```

**Multiple Outputs**

If your endpoint returns multiple values, they will appear as elements of the `data` list:

```bash
event: complete
data: ["Good morning Hello. It is 5 degrees today", -15.0]
```

**Streaming Example**

If your Gradio app [streams a sequence of values](/guides/streaming-outputs), then they will be streamed directly to your terminal, like this:

```bash
event: generating
data: ["Hello, w!"]
event: generating
data: ["Hello, wo!"]
event: generating
data: ["Hello, wor!"]
event: generating
data: ["Hello, worl!"]
event: generating
data: ["Hello, world!"]
event: complete
data: ["Hello, world!"]
```

**File Example**

If your Gradio app returns a file, the file will be represented as a dictionary in this format (including potentially some additional keys):

```python
{
    "orig_name": "example.jpg",
    "path": "/path/in/server.jpg",
    "url": "https:/example.com/example.jpg",
    "meta": {"_type": "gradio.FileData"}
}
```

In your terminal, it may appear like this:

```bash
event: complete
data: [{"path": "/tmp/gradio/359933dc8d6cfe1b022f35e2c639e6e42c97a003/image.webp", "url": "https://gradio-image-mod.hf.space/c/file=/tmp/gradio/359933dc8d6cfe1b022f35e2c639e6e42c97a003/image.webp", "size": null, "orig_name": "image.webp", "mime_type": null, "is_stream": false, "meta": {"_type": "gradio.FileData"}}]
```

## Authentication

What if your Gradio application has [authentication enabled](/guides/sharing-your-app#authentication)? In that case, you'll need to make an additional `POST` request with cURL to authenticate yourself before you make any queries. Here are the complete steps:

First, login with a `POST` request supplying a valid username and password:

```bash
curl -X POST $URL/login \
     -d "username=$USERNAME&password=$PASSWORD" \
     -c cookies.txt
```

If the credentials are correct, you'll get `{"success":true}` in response and the cookies will be saved in `cookies.txt`.

Next, you'll need to include these cookies when you make the original `POST` request, like this:

```bash
$ curl -X POST $URL/call/$API_NAME -b cookies.txt -H "Content-Type: application/json" -d '{
  "data": $PAYLOAD
}'
```

Finally, you'll need to `GET` the results, again supplying the cookies from the file:

```bash
curl -N $URL/call/$API_NAME/$EVENT_ID -b cookies.txt
```

---

<!-- Source: guides/10_mcp/03_building-an-mcp-client-with-gradio.md -->
# Using the Gradio Chatbot as an MCP Client

This guide will walk you through a Model Context Protocol (MCP) Client and Server implementation with Gradio. You'll build a Gradio Chatbot that uses Anthropic's Claude API to respond to user messages, but also, as an MCP Client, generates images (by connecting to an MCP Server, which is a separate Gradio app). 

<video src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/mcp-guides.mp4" style="width:100%" controls preload> </video>

## What is MCP?

The Model Context Protocol (MCP) standardizes how applications provide context to LLMs. It allows Claude to interact with external tools, like image generators, file systems, or APIs, etc.

## Prerequisites

- Python 3.10+
- An Anthropic API key
- Basic understanding of Python programming

## Setup

First, install the required packages:

```bash
pip install gradio anthropic mcp
```

Create a `.env` file in your project directory and add your Anthropic API key:

```
ANTHROPIC_API_KEY=your_api_key_here
```

## Part 1: Building the MCP Server

The server provides tools that Claude can use. In this example, we'll create a server that generates images through [a HuggingFace space](https://huggingface.co/spaces/ysharma/SanaSprint).

Create a file named `gradio_mcp_server.py`:

```python
from mcp.server.fastmcp import FastMCP
import json
import sys
import io
import time
from gradio_client import Client

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

mcp = FastMCP("huggingface_spaces_image_display")

@mcp.tool()
async def generate_image(prompt: str, width: int = 512, height: int = 512) -> str:
    """Generate an image using SanaSprint model.
    
    Args:
        prompt: Text prompt describing the image to generate
        width: Image width (default: 512)
        height: Image height (default: 512)
    """
    client = Client("https://ysharma-sanasprint.hf.space/")
    
    try:
        result = client.predict(
            prompt,
            "0.6B",
            0,
            True,
            width,
            height,
            4.0,
            2,
            api_name="/infer"
        )
        
        if isinstance(result, list) and len(result) >= 1:
            image_data = result[0]
            if isinstance(image_data, dict) and "url" in image_data:
                return json.dumps({
                    "type": "image",
                    "url": image_data["url"],
                    "message": f"Generated image for prompt: {prompt}"
                })
        
        return json.dumps({
            "type": "error",
            "message": "Failed to generate image"
        })
        
    except Exception as e:
        return json.dumps({
            "type": "error",
            "message": f"Error generating image: {str(e)}"
        })

if __name__ == "__main__":
    mcp.run(transport='stdio')
```

### What this server does:

1. It creates an MCP server that exposes a `generate_image` tool
2. The tool connects to the SanaSprint model hosted on HuggingFace Spaces
3. It handles the asynchronous nature of image generation by polling for results
4. When an image is ready, it returns the URL in a structured JSON format

## Part 2: Building the MCP Client with Gradio

Now let's create a Gradio chat interface as MCP Client that connects Claude to our MCP server.

Create a file named `app.py`:

```python
import asyncio
import os
import json
from typing import List, Dict, Any, Union
from contextlib import AsyncExitStack

import gradio as gr
from gradio.components.chatbot import ChatMessage
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from anthropic import Anthropic
from dotenv import load_dotenv

load_dotenv()

loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)

class MCPClientWrapper:
    def __init__(self):
        self.session = None
        self.exit_stack = None
        self.anthropic = Anthropic()
        self.tools = []
    
    def connect(self, server_path: str) -> str:
        return loop.run_until_complete(self._connect(server_path))
    
    async def _connect(self, server_path: str) -> str:
        if self.exit_stack:
            await self.exit_stack.aclose()
        
        self.exit_stack = AsyncExitStack()
        
        is_python = server_path.endswith('.py')
        command = "python" if is_python else "node"
        
        server_params = StdioServerParameters(
            command=command,
            args=[server_path],
            env={"PYTHONIOENCODING": "utf-8", "PYTHONUNBUFFERED": "1"}
        )
        
        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        self.stdio, self.write = stdio_transport
        
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))
        await self.session.initialize()
        
        response = await self.session.list_tools()
        self.tools = [{ 
            "name": tool.name,
            "description": tool.description,
            "input_schema": tool.inputSchema
        } for tool in response.tools]
        
        tool_names = [tool["name"] for tool in self.tools]
        return f"Connected to MCP server. Available tools: {', '.join(tool_names)}"
    
    def process_message(self, message: str, history: List[Union[Dict[str, Any], ChatMessage]]) -> tuple:
        if not self.session:
            return history + [
                {"role": "user", "content": message}, 
                {"role": "assistant", "content": "Please connect to an MCP server first."}
            ], gr.Textbox(value="")
        
        new_messages = loop.run_until_complete(self._process_query(message, history))
        return history + [{"role": "user", "content": message}] + new_messages, gr.Textbox(value="")
    
    async def _process_query(self, message: str, history: List[Union[Dict[str, Any], ChatMessage]]):
        claude_messages = []
        for msg in history:
            if isinstance(msg, ChatMessage):
                role, content = msg.role, msg.content
            else:
                role, content = msg.get("role"), msg.get("content")
            
            if role in ["user", "assistant", "system"]:
                claude_messages.append({"role": role, "content": content})
        
        claude_messages.append({"role": "user", "content": message})
        
        response = self.anthropic.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            messages=claude_messages,
            tools=self.tools
        )

        result_messages = []
        
        for content in response.content:
            if content.type == 'text':
                result_messages.append({
                    "role": "assistant", 
                    "content": content.text
                })
                
            elif content.type == 'tool_use':
                tool_name = content.name
                tool_args = content.input
                
                result_messages.append({
                    "role": "assistant",
                    "content": f"I'll use the {tool_name} tool to help answer your question.",
                    "metadata": {
                        "title": f"Using tool: {tool_name}",
                        "log": f"Parameters: {json.dumps(tool_args, ensure_ascii=True)}",
                        "status": "pending",
                        "id": f"tool_call_{tool_name}"
                    }
                })
                
                result_messages.append({
                    "role": "assistant",
                    "content": "```json\n" + json.dumps(tool_args, indent=2, ensure_ascii=True) + "\n```",
                    "metadata": {
                        "parent_id": f"tool_call_{tool_name}",
                        "id": f"params_{tool_name}",
                        "title": "Tool Parameters"
                    }
                })
                
                result = await self.session.call_tool(tool_name, tool_args)
                
                if result_messages and "metadata" in result_messages[-2]:
                    result_messages[-2]["metadata"]["status"] = "done"
                
                result_messages.append({
                    "role": "assistant",
                    "content": "Here are the results from the tool:",
                    "metadata": {
                        "title": f"Tool Result for {tool_name}",
                        "status": "done",
                        "id": f"result_{tool_name}"
                    }
                })
                
                result_content = result.content
                if isinstance(result_content, list):
                    result_content = "\n".join(str(item) for item in result_content)
                
                try:
                    result_json = json.loads(result_content)
                    if isinstance(result_json, dict) and "type" in result_json:
                        if result_json["type"] == "image" and "url" in result_json:
                            result_messages.append({
                                "role": "assistant",
                                "content": {"path": result_json["url"], "alt_text": result_json.get("message", "Generated image")},
                                "metadata": {
                                    "parent_id": f"result_{tool_name}",
                                    "id": f"image_{tool_name}",
                                    "title": "Generated Image"
                                }
                            })
                        else:
                            result_messages.append({
                                "role": "assistant",
                                "content": "```\n" + result_content + "\n```",
                                "metadata": {
                                    "parent_id": f"result_{tool_name}",
                                    "id": f"raw_result_{tool_name}",
                                    "title": "Raw Output"
                                }
                            })
                except:
                    result_messages.append({
                        "role": "assistant",
                        "content": "```\n" + result_content + "\n```",
                        "metadata": {
                            "parent_id": f"result_{tool_name}",
                            "id": f"raw_result_{tool_name}",
                            "title": "Raw Output"
                        }
                    })
                
                claude_messages.append({"role": "user", "content": f"Tool result for {tool_name}: {result_content}"})
                next_response = self.anthropic.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=1000,
                    messages=claude_messages,
                )
                
                if next_response.content and next_response.content[0].type == 'text':
                    result_messages.append({
                        "role": "assistant",
                        "content": next_response.content[0].text
                    })

        return result_messages

client = MCPClientWrapper()

def gradio_interface():
    with gr.Blocks(title="MCP Weather Client") as demo:
        gr.Markdown("# MCP Weather Assistant")
        gr.Markdown("Connect to your MCP weather server and chat with the assistant")
        
        with gr.Row(equal_height=True):
            with gr.Column(scale=4):
                server_path = gr.Textbox(
                    label="Server Script Path",
                    placeholder="Enter path to server script (e.g., weather.py)",
                    value="gradio_mcp_server.py"
                )
            with gr.Column(scale=1):
                connect_btn = gr.Button("Connect")
        
        status = gr.Textbox(label="Connection Status", interactive=False)
        
        chatbot = gr.Chatbot(
            value=[], 
            height=500,
            show_copy_button=True,
            avatar_images=("ðŸ‘¤", "ðŸ¤–")
        )
        
        with gr.Row(equal_height=True):
            msg = gr.Textbox(
                label="Your Question",
                placeholder="Ask about weather or alerts (e.g., What's the weather in New York?)",
                scale=4
            )
            clear_btn = gr.Button("Clear Chat", scale=1)
        
        connect_btn.click(client.connect, inputs=server_path, outputs=status)
        msg.submit(client.process_message, [msg, chatbot], [chatbot, msg])
        clear_btn.click(lambda: [], None, chatbot)
        
    return demo

if __name__ == "__main__":
    if not os.getenv("ANTHROPIC_API_KEY"):
        print("Warning: ANTHROPIC_API_KEY not found in environment. Please set it in your .env file.")
    
    interface = gradio_interface()
    interface.launch(debug=True)
```

### What this MCP Client does:

- Creates a friendly Gradio chat interface for user interaction
- Connects to the MCP server you specify
- Handles conversation history and message formatting
- Makes call to Claude API with tool definitions
- Processes tool usage requests from Claude
- Displays images and other tool outputs in the chat
- Sends tool results back to Claude for interpretation

## Running the Application

To run your MCP application:

- Start a terminal window and run the MCP Client:
   ```bash
   python app.py
   ```
- Open the Gradio interface at the URL shown (typically http://127.0.0.1:7860)
- In the Gradio interface, you'll see a field for the MCP Server path. It should default to `gradio_mcp_server.py`.
- Click "Connect" to establish the connection to the MCP server.
- You should see a message indicating the server connection was successful.

## Example Usage

Now you can chat with Claude and it will be able to generate images based on your descriptions.

Try prompts like:
- "Can you generate an image of a mountain landscape at sunset?"
- "Create an image of a cool tabby cat"
- "Generate a picture of a panda wearing sunglasses"

Claude will recognize these as image generation requests and automatically use the `generate_image` tool from your MCP server.


## How it Works

Here's the high-level flow of what happens during a chat session:

1. Your prompt enters the Gradio interface
2. The client forwards your prompt to Claude
3. Claude analyzes the prompt and decides to use the `generate_image` tool
4. The client sends the tool call to the MCP server
5. The server calls the external image generation API
6. The image URL is returned to the client
7. The client sends the image URL back to Claude
8. Claude provides a response that references the generated image
9. The Gradio chat interface displays both Claude's response and the image


## Next Steps

Now that you have a working MCP system, here are some ideas to extend it:

- Add more tools to your server
- Improve error handling 
- Add private Huggingface Spaces with authentication for secure tool access
- Create custom tools that connect to your own APIs or services
- Implement streaming responses for better user experience

## Conclusion

Congratulations! You've successfully built an MCP Client and Server that allows Claude to generate images based on text prompts. This is just the beginning of what you can do with Gradio and MCP. This guide enables you to build complex AI applications that can use Claude or any other powerful LLM to interact with virtually any external tool or service.

Read our other Guide on using [Gradio apps as MCP Servers](./building-mcp-server-with-gradio).

---

<!-- Source: guides/cn/01_getting-started/03_sharing-your-app.md -->
# åˆ†äº«æ‚¨çš„åº”ç”¨

å¦‚ä½•åˆ†äº«æ‚¨çš„ Gradio åº”ç”¨ï¼š

1. [ä½¿ç”¨ share å‚æ•°åˆ†äº«æ¼”ç¤º](#sharing-demos)
2. [åœ¨ HF Spaces ä¸Šæ‰˜ç®¡](#hosting-on-hf-spaces)
3. [åµŒå…¥æ‰˜ç®¡çš„ç©ºé—´](#embedding-hosted-spaces)
4. [ä½¿ç”¨ Web ç»„ä»¶åµŒå…¥](#embedding-with-web-components)
5. [ä½¿ç”¨ API é¡µé¢](#api-page)
6. [åœ¨é¡µé¢ä¸Šæ·»åŠ èº«ä»½éªŒè¯](#authentication)
7. [è®¿é—®ç½‘ç»œè¯·æ±‚](#accessing-the-network-request-directly)
8. [åœ¨ FastAPI ä¸­æŒ‚è½½](#mounting-within-another-fastapi-app)
9. [å®‰å…¨æ€§](#security-and-file-access)

## åˆ†äº«æ¼”ç¤º

é€šè¿‡åœ¨ `launch()` æ–¹æ³•ä¸­è®¾ç½® `share=True`ï¼Œå¯ä»¥è½»æ¾å…¬å¼€åˆ†äº« Gradio æ¼”ç¤ºã€‚å°±åƒè¿™æ ·ï¼š

```python
demo.launch(share=True)
```

è¿™å°†ç”Ÿæˆä¸€ä¸ªå…¬å¼€çš„å¯åˆ†äº«é“¾æŽ¥ï¼Œæ‚¨å¯ä»¥å°†å…¶å‘é€ç»™ä»»ä½•äººï¼å½“æ‚¨å‘é€æ­¤é“¾æŽ¥æ—¶ï¼Œå¯¹æ–¹ç”¨æˆ·å¯ä»¥åœ¨å…¶æµè§ˆå™¨ä¸­å°è¯•æ¨¡åž‹ã€‚å› ä¸ºå¤„ç†è¿‡ç¨‹å‘ç”Ÿåœ¨æ‚¨çš„è®¾å¤‡ä¸Šï¼ˆåªè¦æ‚¨çš„è®¾å¤‡ä¿æŒå¼€å¯ï¼ï¼‰ï¼Œæ‚¨ä¸å¿…æ‹…å¿ƒä»»ä½•æ‰“åŒ…ä¾èµ–é¡¹çš„é—®é¢˜ã€‚ä¸€ä¸ªåˆ†äº«é“¾æŽ¥é€šå¸¸çœ‹èµ·æ¥åƒè¿™æ ·ï¼š**XXXXX.gradio.app**ã€‚å°½ç®¡é“¾æŽ¥æ˜¯é€šè¿‡ Gradio URL æä¾›çš„ï¼Œä½†æˆ‘ä»¬åªæ˜¯æ‚¨æœ¬åœ°æœåŠ¡å™¨çš„ä»£ç†ï¼Œå¹¶ä¸ä¼šå­˜å‚¨é€šè¿‡æ‚¨çš„åº”ç”¨å‘é€çš„ä»»ä½•æ•°æ®ã€‚

ä½†è¯·è®°ä½ï¼Œè¿™äº›é“¾æŽ¥å¯ä»¥è¢«å…¬å¼€è®¿é—®ï¼Œè¿™æ„å‘³ç€ä»»ä½•äººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡åž‹è¿›è¡Œé¢„æµ‹ï¼å› æ­¤ï¼Œè¯·ç¡®ä¿ä¸è¦é€šè¿‡æ‚¨ç¼–å†™çš„å‡½æ•°å…¬å¼€ä»»ä½•æ•æ„Ÿä¿¡æ¯ï¼Œä¹Ÿä¸è¦å…è®¸åœ¨æ‚¨çš„è®¾å¤‡ä¸Šè¿›è¡Œä»»ä½•å…³é”®æ›´æ”¹ã€‚å¦‚æžœæ‚¨è®¾ç½® `share=False`ï¼ˆé»˜è®¤å€¼ï¼Œåœ¨ colab ç¬”è®°æœ¬ä¸­é™¤å¤–ï¼‰ï¼Œåˆ™åªåˆ›å»ºä¸€ä¸ªæœ¬åœ°é“¾æŽ¥ï¼Œå¯ä»¥é€šè¿‡[ç«¯å£è½¬å‘](https://www.ssh.com/ssh/tunneling/example)ä¸Žç‰¹å®šç”¨æˆ·å…±äº«ã€‚

<img style="width: 40%" src="/assets/guides/sharing.svg">

åˆ†äº«é“¾æŽ¥åœ¨ 72 å°æ—¶åŽè¿‡æœŸã€‚

## åœ¨ HF Spaces ä¸Šæ‰˜ç®¡

å¦‚æžœæ‚¨æƒ³åœ¨äº’è”ç½‘ä¸ŠèŽ·å¾—æ‚¨çš„ Gradio æ¼”ç¤ºçš„æ°¸ä¹…é“¾æŽ¥ï¼Œè¯·ä½¿ç”¨ Hugging Face Spacesã€‚ [Hugging Face Spaces](http://huggingface.co/spaces/) æä¾›äº†å…è´¹æ‰˜ç®¡æ‚¨çš„æœºå™¨å­¦ä¹ æ¨¡åž‹çš„åŸºç¡€è®¾æ–½ï¼

åœ¨æ‚¨åˆ›å»ºäº†ä¸€ä¸ªå…è´¹çš„ Hugging Face è´¦æˆ·åŽï¼Œæœ‰ä¸‰ç§æ–¹æ³•å¯ä»¥å°†æ‚¨çš„ Gradio åº”ç”¨éƒ¨ç½²åˆ° Hugging Face Spacesï¼š

1. ä»Žç»ˆç«¯ï¼šåœ¨åº”ç”¨ç›®å½•ä¸­è¿è¡Œ `gradio deploy`ã€‚CLI å°†æ”¶é›†ä¸€äº›åŸºæœ¬å…ƒæ•°æ®ï¼Œç„¶åŽå¯åŠ¨æ‚¨çš„åº”ç”¨ã€‚è¦æ›´æ–°æ‚¨çš„ç©ºé—´ï¼Œå¯ä»¥é‡æ–°è¿è¡Œæ­¤å‘½ä»¤æˆ–å¯ç”¨ Github Actions é€‰é¡¹ï¼Œåœ¨ `git push` æ—¶è‡ªåŠ¨æ›´æ–° Spacesã€‚
2. ä»Žæµè§ˆå™¨ï¼šå°†åŒ…å« Gradio æ¨¡åž‹å’Œæ‰€æœ‰ç›¸å…³æ–‡ä»¶çš„æ–‡ä»¶å¤¹æ‹–æ”¾åˆ° [æ­¤å¤„](https://huggingface.co/new-space)ã€‚
3. å°† Spaces ä¸Žæ‚¨çš„ Git å­˜å‚¨åº“è¿žæŽ¥ï¼ŒSpaces å°†ä»Žé‚£é‡Œæ‹‰å– Gradio åº”ç”¨ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [æ­¤æŒ‡å—å¦‚ä½•åœ¨ Hugging Face Spaces ä¸Šæ‰˜ç®¡](https://huggingface.co/blog/gradio-spaces)ã€‚

<video autoplay muted loop>
  <source src="/assets/guides/hf_demo.mp4" type="video/mp4" />
</video>

## åµŒå…¥æ‰˜ç®¡çš„ç©ºé—´

ä¸€æ—¦æ‚¨å°†åº”ç”¨æ‰˜ç®¡åœ¨ Hugging Face Spacesï¼ˆæˆ–æ‚¨è‡ªå·±çš„æœåŠ¡å™¨ä¸Šï¼‰ï¼Œæ‚¨å¯èƒ½å¸Œæœ›å°†æ¼”ç¤ºåµŒå…¥åˆ°ä¸åŒçš„ç½‘ç«™ä¸Šï¼Œä¾‹å¦‚æ‚¨çš„åšå®¢æˆ–ä¸ªäººä½œå“é›†ã€‚åµŒå…¥äº¤äº’å¼æ¼”ç¤ºä½¿äººä»¬å¯ä»¥åœ¨ä»–ä»¬çš„æµè§ˆå™¨ä¸­å°è¯•æ‚¨æž„å»ºçš„æœºå™¨å­¦ä¹ æ¨¡åž‹ï¼Œè€Œæ— éœ€ä¸‹è½½æˆ–å®‰è£…ä»»ä½•å†…å®¹ï¼æœ€å¥½çš„éƒ¨åˆ†æ˜¯ï¼Œæ‚¨ç”šè‡³å¯ä»¥å°†äº¤äº’å¼æ¼”ç¤ºåµŒå…¥åˆ°é™æ€ç½‘ç«™ä¸­ï¼Œä¾‹å¦‚ GitHub é¡µé¢ã€‚

æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥åµŒå…¥æ‚¨çš„ Gradio æ¼”ç¤ºã€‚æ‚¨å¯ä»¥åœ¨ Hugging Face Space é¡µé¢çš„â€œåµŒå…¥æ­¤ç©ºé—´â€ä¸‹æ‹‰é€‰é¡¹ä¸­ç›´æŽ¥æ‰¾åˆ°è¿™ä¸¤ä¸ªé€‰é¡¹çš„å¿«é€Ÿé“¾æŽ¥ï¼š

![åµŒå…¥æ­¤ç©ºé—´ä¸‹æ‹‰é€‰é¡¹](/assets/guides/embed_this_space.png)

### ä½¿ç”¨ Web ç»„ä»¶åµŒå…¥

ä¸Ž IFrames ç›¸æ¯”ï¼ŒWeb ç»„ä»¶é€šå¸¸ä¸ºç”¨æˆ·æä¾›æ›´å¥½çš„ä½“éªŒã€‚Web ç»„ä»¶è¿›è¡Œå»¶è¿ŸåŠ è½½ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä¸ä¼šå‡æ…¢æ‚¨ç½‘ç«™çš„åŠ è½½æ—¶é—´ï¼Œå¹¶ä¸”å®ƒä»¬ä¼šæ ¹æ® Gradio åº”ç”¨çš„å¤§å°è‡ªåŠ¨è°ƒæ•´å…¶é«˜åº¦ã€‚

è¦ä½¿ç”¨ Web ç»„ä»¶åµŒå…¥ï¼š

1.  é€šè¿‡åœ¨æ‚¨çš„ç½‘ç«™ä¸­æ·»åŠ ä»¥ä¸‹è„šæœ¬æ¥å¯¼å…¥ gradio JS åº“ï¼ˆåœ¨ URL ä¸­æ›¿æ¢{GRADIO_VERSION}ä¸ºæ‚¨ä½¿ç”¨çš„ Gradio åº“çš„ç‰ˆæœ¬ï¼‰ã€‚

        ```html

    &lt;script type="module"
    src="https://gradio.s3-us-west-2.amazonaws.com/{GRADIO_VERSION}/gradio.js">
    &lt;/script>
    ```

2.  åœ¨æ‚¨æƒ³æ”¾ç½®åº”ç”¨çš„ä½ç½®æ·»åŠ 
    `html
&lt;gradio-app src="https://$your_space_host.hf.space">&lt;/gradio-app>
    `
    å…ƒç´ ã€‚å°† `src=` å±žæ€§è®¾ç½®ä¸ºæ‚¨çš„ Space çš„åµŒå…¥ URLï¼Œæ‚¨å¯ä»¥åœ¨â€œåµŒå…¥æ­¤ç©ºé—´â€æŒ‰é’®ä¸­æ‰¾åˆ°ã€‚ä¾‹å¦‚ï¼š

        ```html

    &lt;gradio-app src="https://abidlabs-pytorch-image-classifier.hf.space">&lt;/gradio-app>
    ```

<script>
fetch("https://pypi.org/pypi/gradio/json"
).then(r => r.json()
).then(obj => {
    let v = obj.info.version;
    content = document.querySelector('.prose');
    content.innerHTML = content.innerHTML.replaceAll("{GRADIO_VERSION}", v);
});
</script>

æ‚¨å¯ä»¥åœ¨ <a href="https://www.gradio.app">Gradio é¦–é¡µ </a> ä¸ŠæŸ¥çœ‹ Web ç»„ä»¶çš„ç¤ºä¾‹ã€‚

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä¼ é€’ç»™ `<gradio-app>` æ ‡ç­¾çš„å±žæ€§æ¥è‡ªå®šä¹‰ Web ç»„ä»¶çš„å¤–è§‚å’Œè¡Œä¸ºï¼š

- `src`ï¼šå¦‚å‰æ‰€è¿°ï¼Œ`src` å±žæ€§é“¾æŽ¥åˆ°æ‚¨æƒ³è¦åµŒå…¥çš„æ‰˜ç®¡ Gradio æ¼”ç¤ºçš„ URL
- `space`ï¼šä¸€ä¸ªå¯é€‰çš„ç¼©å†™ï¼Œå¦‚æžœæ‚¨çš„ Gradio æ¼”ç¤ºæ‰˜ç®¡åœ¨ Hugging Face Space ä¸Šã€‚æŽ¥å— `username/space_name` è€Œä¸æ˜¯å®Œæ•´çš„ URLã€‚ç¤ºä¾‹ï¼š`gradio/Echocardiogram-Segmentation`ã€‚å¦‚æžœæä¾›äº†æ­¤å±žæ€§ï¼Œåˆ™ä¸éœ€è¦æä¾› `src`ã€‚
- `control_page_title`ï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼ŒæŒ‡å®šæ˜¯å¦å°† html æ ‡é¢˜è®¾ç½®ä¸º Gradio åº”ç”¨çš„æ ‡é¢˜ï¼ˆé»˜è®¤ä¸º `"false"`ï¼‰
- `initial_height`ï¼šåŠ è½½ Gradio åº”ç”¨æ—¶ Web ç»„ä»¶çš„åˆå§‹é«˜åº¦ï¼ˆé»˜è®¤ä¸º `"300px"`ï¼‰ã€‚è¯·æ³¨æ„ï¼Œæœ€ç»ˆé«˜åº¦æ˜¯æ ¹æ® Gradio åº”ç”¨çš„å¤§å°è®¾ç½®çš„ã€‚
- `container`ï¼šæ˜¯å¦æ˜¾ç¤ºè¾¹æ¡†æ¡†æž¶å’Œæœ‰å…³ Space æ‰˜ç®¡ä½ç½®çš„ä¿¡æ¯ï¼ˆé»˜è®¤ä¸º `"true"`ï¼‰
- `info`ï¼šæ˜¯å¦ä»…æ˜¾ç¤ºæœ‰å…³ Space æ‰˜ç®¡ä½ç½®çš„ä¿¡æ¯åœ¨åµŒå…¥çš„åº”ç”¨ç¨‹åºä¸‹æ–¹ï¼ˆé»˜è®¤ä¸º `"true"`ï¼‰
- `autoscroll`ï¼šåœ¨é¢„æµ‹å®ŒæˆåŽæ˜¯å¦è‡ªåŠ¨æ»šåŠ¨åˆ°è¾“å‡ºï¼ˆé»˜è®¤ä¸º `"false"`ï¼‰
- `eager`ï¼šåœ¨é¡µé¢åŠ è½½æ—¶æ˜¯å¦ç«‹å³åŠ è½½ Gradio åº”ç”¨ï¼ˆé»˜è®¤ä¸º `"false"`ï¼‰
- `theme_mode`ï¼šæ˜¯å¦ä½¿ç”¨ `dark`ï¼Œ`light` æˆ–é»˜è®¤çš„ `system` ä¸»é¢˜æ¨¡å¼ï¼ˆé»˜è®¤ä¸º `"system"`ï¼‰

ä»¥ä¸‹æ˜¯ä½¿ç”¨è¿™äº›å±žæ€§åˆ›å»ºä¸€ä¸ªæ‡’åŠ è½½ä¸”åˆå§‹é«˜åº¦ä¸º 0px çš„ Gradio åº”ç”¨çš„ç¤ºä¾‹ã€‚

```html
&lt;gradio-app space="gradio/Echocardiogram-Segmentation" eager="true"
initial_height="0px">&lt;/gradio-app>
```

_ æ³¨æ„ï¼šGradio çš„ CSS æ°¸è¿œä¸ä¼šå½±å“åµŒå…¥é¡µé¢ï¼Œä½†åµŒå…¥é¡µé¢å¯ä»¥å½±å“åµŒå…¥çš„ Gradio åº”ç”¨çš„æ ·å¼ã€‚è¯·ç¡®ä¿çˆ¶é¡µé¢ä¸­çš„ä»»ä½• CSS ä¸æ˜¯å¦‚æ­¤é€šç”¨ï¼Œä»¥è‡³äºŽå®ƒä¹Ÿå¯èƒ½é€‚ç”¨äºŽåµŒå…¥çš„ Gradio åº”ç”¨å¹¶å¯¼è‡´æ ·å¼ç ´è£‚ã€‚ä¾‹å¦‚ï¼Œå…ƒç´ é€‰æ‹©å™¨å¦‚ `header { ... }` å’Œ `footer { ... }` æœ€å¯èƒ½å¼•èµ·é—®é¢˜ã€‚_

### ä½¿ç”¨ IFrames åµŒå…¥

å¦‚æžœæ‚¨æ— æ³•å‘ç½‘ç«™æ·»åŠ  javascriptï¼ˆä¾‹å¦‚ï¼‰ï¼Œåˆ™å¯ä»¥æ”¹ä¸ºä½¿ç”¨ IFrames è¿›è¡ŒåµŒå…¥ï¼Œè¯·æ·»åŠ ä»¥ä¸‹å…ƒç´ ï¼š

```html
&lt;iframe src="https://$your_space_host.hf.space">&lt;/iframe>
```

åŒæ ·ï¼Œæ‚¨å¯ä»¥åœ¨â€œåµŒå…¥æ­¤ç©ºé—´â€æŒ‰é’®ä¸­æ‰¾åˆ°æ‚¨çš„ Space çš„åµŒå…¥ URL çš„ `src=` å±žæ€§ã€‚

æ³¨æ„ï¼šå¦‚æžœæ‚¨ä½¿ç”¨ IFramesï¼Œæ‚¨å¯èƒ½å¸Œæœ›æ·»åŠ ä¸€ä¸ªå›ºå®šçš„ `height` å±žæ€§ï¼Œå¹¶è®¾ç½® `style="border:0;"` ä»¥åŽ»é™¤è¾¹æ¡†ã€‚æ­¤å¤–ï¼Œå¦‚æžœæ‚¨çš„åº”ç”¨ç¨‹åºéœ€è¦è¯¸å¦‚è®¿é—®æ‘„åƒå¤´æˆ–éº¦å…‹é£Žä¹‹ç±»çš„æƒé™ï¼Œæ‚¨è¿˜éœ€è¦ä½¿ç”¨ `allow` å±žæ€§æä¾›å®ƒä»¬ã€‚

## API é¡µé¢

$demo_hello_world

å¦‚æžœæ‚¨ç‚¹å‡»å¹¶æ‰“å¼€ä¸Šé¢çš„ç©ºé—´ï¼Œæ‚¨ä¼šåœ¨åº”ç”¨çš„é¡µè„šçœ‹åˆ°ä¸€ä¸ªâ€œé€šè¿‡ API ä½¿ç”¨â€é“¾æŽ¥ã€‚

![é€šè¿‡ API ä½¿ç”¨](/assets/guides/use_via_api.png)

è¿™æ˜¯ä¸€ä¸ªæ–‡æ¡£é¡µé¢ï¼Œè®°å½•äº†ç”¨æˆ·å¯ä»¥ä½¿ç”¨çš„ REST API æ¥æŸ¥è¯¢â€œInterfaceâ€å‡½æ•°ã€‚`Blocks` åº”ç”¨ç¨‹åºä¹Ÿå¯ä»¥ç”Ÿæˆ API é¡µé¢ï¼Œä½†å¿…é¡»ä¸ºæ¯ä¸ªäº‹ä»¶ç›‘å¬å™¨æ˜¾å¼å‘½å APIï¼Œä¾‹å¦‚ï¼š

```python
btn.click(add, [num1, num2], output, api_name="addition")
```

è¿™å°†è®°å½•è‡ªåŠ¨ç”Ÿæˆçš„ API é¡µé¢çš„ç«¯ç‚¹ `/api/addition/`ã€‚

_æ³¨æ„_ï¼šå¯¹äºŽå¯ç”¨äº†[é˜Ÿåˆ—åŠŸèƒ½](https://gradio.app/key-features#queuing)çš„ Gradio åº”ç”¨ç¨‹åºï¼Œå¦‚æžœç”¨æˆ·å‘æ‚¨çš„ API ç«¯ç‚¹å‘å‡º POST è¯·æ±‚ï¼Œä»–ä»¬å¯ä»¥ç»•è¿‡é˜Ÿåˆ—ã€‚è¦ç¦ç”¨æ­¤è¡Œä¸ºï¼Œè¯·åœ¨ `queue()` æ–¹æ³•ä¸­è®¾ç½® `api_open=False`ã€‚

## é‰´æƒ

æ‚¨å¯èƒ½å¸Œæœ›åœ¨æ‚¨çš„åº”ç”¨ç¨‹åºå‰é¢æ”¾ç½®ä¸€ä¸ªé‰´æƒé¡µé¢ï¼Œä»¥é™åˆ¶è°å¯ä»¥æ‰“å¼€æ‚¨çš„åº”ç”¨ç¨‹åºã€‚ä½¿ç”¨ `launch()` æ–¹æ³•ä¸­çš„ `auth=` å…³é”®å­—å‚æ•°ï¼Œæ‚¨å¯ä»¥æä¾›ä¸€ä¸ªåŒ…å«ç”¨æˆ·åå’Œå¯†ç çš„å…ƒç»„ï¼Œæˆ–è€…ä¸€ä¸ªå¯æŽ¥å—çš„ç”¨æˆ·å / å¯†ç å…ƒç»„åˆ—è¡¨ï¼›ä»¥ä¸‹æ˜¯ä¸€ä¸ªä¸ºå•ä¸ªåä¸ºâ€œadminâ€çš„ç”¨æˆ·æä¾›åŸºäºŽå¯†ç çš„èº«ä»½éªŒè¯çš„ç¤ºä¾‹ï¼š

```python
demo.launch(auth=("admin", "pass1234"))
```

å¯¹äºŽæ›´å¤æ‚çš„èº«ä»½éªŒè¯å¤„ç†ï¼Œæ‚¨ç”šè‡³å¯ä»¥ä¼ é€’ä¸€ä¸ªä»¥ç”¨æˆ·åå’Œå¯†ç ä½œä¸ºå‚æ•°çš„å‡½æ•°ï¼Œå¹¶è¿”å›ž True ä»¥å…è®¸èº«ä»½éªŒè¯ï¼Œå¦åˆ™è¿”å›ž Falseã€‚è¿™å¯ç”¨äºŽè®¿é—®ç¬¬ä¸‰æ–¹èº«ä»½éªŒè¯æœåŠ¡ç­‰å…¶ä»–åŠŸèƒ½ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªæŽ¥å—ä»»ä½•ç”¨æˆ·åå’Œå¯†ç ç›¸åŒçš„ç™»å½•çš„å‡½æ•°ç¤ºä¾‹ï¼š

```python
def same_auth(username, password):
    return username == password
demo.launch(auth=same_auth)
```

ä¸ºäº†ä½¿èº«ä»½éªŒè¯æ­£å¸¸å·¥ä½œï¼Œå¿…é¡»åœ¨æµè§ˆå™¨ä¸­å¯ç”¨ç¬¬ä¸‰æ–¹ Cookieã€‚
é»˜è®¤æƒ…å†µä¸‹ï¼ŒSafariã€Chrome éšç§æ¨¡å¼ä¸ä¼šå¯ç”¨æ­¤åŠŸèƒ½ã€‚

## ç›´æŽ¥è®¿é—®ç½‘ç»œè¯·æ±‚

å½“ç”¨æˆ·å‘æ‚¨çš„åº”ç”¨ç¨‹åºè¿›è¡Œé¢„æµ‹æ—¶ï¼Œæ‚¨å¯èƒ½éœ€è¦åº•å±‚çš„ç½‘ç»œè¯·æ±‚ï¼Œä»¥èŽ·å–è¯·æ±‚æ ‡å¤´ï¼ˆä¾‹å¦‚ç”¨äºŽé«˜çº§èº«ä»½éªŒè¯ï¼‰ã€è®°å½•å®¢æˆ·ç«¯çš„ IP åœ°å€æˆ–å…¶ä»–åŽŸå› ã€‚Gradio æ”¯æŒä¸Ž FastAPI ç±»ä¼¼çš„æ–¹å¼ï¼šåªéœ€æ·»åŠ ä¸€ä¸ªç±»åž‹æç¤ºä¸º `gr.Request` çš„å‡½æ•°å‚æ•°ï¼ŒGradio å°†å°†ç½‘ç»œè¯·æ±‚ä½œä¸ºè¯¥å‚æ•°ä¼ é€’è¿›æ¥ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š

```python
import gradio as gr

def echo(name, request: gr.Request):
    if request:
        print("Request headers dictionary:", request.headers)
        print("IP address:", request.client.host)
    return name

io = gr.Interface(echo, "textbox", "textbox").launch()
```

æ³¨æ„ï¼šå¦‚æžœç›´æŽ¥è°ƒç”¨å‡½æ•°è€Œä¸æ˜¯é€šè¿‡ UIï¼ˆä¾‹å¦‚åœ¨ç¼“å­˜ç¤ºä¾‹æ—¶ï¼‰ï¼Œåˆ™ `request` å°†ä¸º `None`ã€‚æ‚¨åº”è¯¥æ˜Žç¡®å¤„ç†æ­¤æƒ…å†µï¼Œä»¥ç¡®ä¿æ‚¨çš„åº”ç”¨ç¨‹åºä¸ä¼šæŠ›å‡ºä»»ä½•é”™è¯¯ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æœ‰æ˜¾å¼æ£€æŸ¥ `if request`ã€‚

## åµŒå…¥åˆ°å¦ä¸€ä¸ª FastAPI åº”ç”¨ç¨‹åºä¸­

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½å·²ç»æœ‰ä¸€ä¸ªçŽ°æœ‰çš„ FastAPI åº”ç”¨ç¨‹åºï¼Œå¹¶ä¸”æ‚¨æƒ³è¦ä¸º Gradio æ¼”ç¤ºæ·»åŠ ä¸€ä¸ªè·¯å¾„ã€‚
æ‚¨å¯ä»¥ä½¿ç”¨ `gradio.mount_gradio_app()` æ¥è½»æ¾å®žçŽ°æ­¤ç›®çš„ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼š

$code_custom_path

è¯·æ³¨æ„ï¼Œæ­¤æ–¹æ³•è¿˜å…è®¸æ‚¨åœ¨è‡ªå®šä¹‰è·¯å¾„ä¸Šè¿è¡Œ Gradio åº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚ä¸Šé¢çš„ `http://localhost:8000/gradio`ï¼‰ã€‚

## å®‰å…¨æ€§å’Œæ–‡ä»¶è®¿é—®

ä¸Žä»–äººå…±äº« Gradio åº”ç”¨ç¨‹åºï¼ˆé€šè¿‡ Spacesã€æ‚¨è‡ªå·±çš„æœåŠ¡å™¨æˆ–ä¸´æ—¶å…±äº«é“¾æŽ¥è¿›è¡Œæ‰˜ç®¡ï¼‰å°†ä¸»æœºæœºå™¨ä¸Šçš„æŸäº›æ–‡ä»¶**æš´éœ²**ç»™æ‚¨çš„ Gradio åº”ç”¨ç¨‹åºçš„ç”¨æˆ·ã€‚

ç‰¹åˆ«æ˜¯ï¼ŒGradio åº”ç”¨ç¨‹åºå…è®¸ç”¨æˆ·è®¿é—®ä»¥ä¸‹ä¸‰ç±»æ–‡ä»¶ï¼š

- **ä¸Ž Gradio è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆæˆ–å­ç›®å½•ï¼‰ä¸­çš„æ–‡ä»¶ç›¸åŒã€‚** ä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨çš„ Gradio è„šæœ¬çš„è·¯å¾„æ˜¯ `/home/usr/scripts/project/app.py`ï¼Œå¹¶ä¸”æ‚¨ä»Ž `/home/usr/scripts/project/` å¯åŠ¨å®ƒï¼Œåˆ™å…±äº« Gradio åº”ç”¨ç¨‹åºçš„ç”¨æˆ·å°†èƒ½å¤Ÿè®¿é—® `/home/usr/scripts/project/` ä¸­çš„ä»»ä½•æ–‡ä»¶ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†æ‚¨å¯ä»¥åœ¨ Gradio åº”ç”¨ç¨‹åºä¸­è½»æ¾å¼•ç”¨è¿™äº›æ–‡ä»¶ï¼ˆä¾‹å¦‚åº”ç”¨ç¨‹åºçš„â€œç¤ºä¾‹â€ï¼‰ã€‚

- **Gradio åˆ›å»ºçš„ä¸´æ—¶æ–‡ä»¶ã€‚** è¿™äº›æ˜¯ç”± Gradio ä½œä¸ºè¿è¡Œæ‚¨çš„é¢„æµ‹å‡½æ•°çš„ä¸€éƒ¨åˆ†åˆ›å»ºçš„æ–‡ä»¶ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨çš„é¢„æµ‹å‡½æ•°è¿”å›žä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œåˆ™ Gradio å°†è¯¥è§†é¢‘ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ä¸­ï¼Œç„¶åŽå°†ä¸´æ—¶æ–‡ä»¶çš„è·¯å¾„å‘é€åˆ°å‰ç«¯ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½®çŽ¯å¢ƒå˜é‡ `GRADIO_TEMP_DIR` ä¸ºç»å¯¹è·¯å¾„ï¼ˆä¾‹å¦‚ `/home/usr/scripts/project/temp/`ï¼‰æ¥è‡ªå®šä¹‰ Gradio åˆ›å»ºçš„ä¸´æ—¶æ–‡ä»¶çš„ä½ç½®ã€‚

- **é€šè¿‡ `launch()` ä¸­çš„ `allowed_paths` å‚æ•°å…è®¸çš„æ–‡ä»¶ã€‚** æ­¤å‚æ•°å…è®¸æ‚¨ä¼ é€’ä¸€ä¸ªåŒ…å«å…¶ä»–ç›®å½•æˆ–ç¡®åˆ‡æ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨ï¼Œä»¥å…è®¸ç”¨æˆ·è®¿é—®å®ƒä»¬ã€‚ï¼ˆé»˜è®¤æƒ…å†µä¸‹ï¼Œæ­¤å‚æ•°ä¸ºç©ºåˆ—è¡¨ï¼‰ã€‚

Gradio**ä¸å…è®¸**è®¿é—®ä»¥ä¸‹å†…å®¹ï¼š

- **ç‚¹æ–‡ä»¶**ï¼ˆå…¶åç§°ä»¥ '.' å¼€å¤´çš„ä»»ä½•æ–‡ä»¶ï¼‰æˆ–å…¶åç§°ä»¥ '.' å¼€å¤´çš„ä»»ä½•ç›®å½•ä¸­çš„ä»»ä½•æ–‡ä»¶ã€‚

- **é€šè¿‡ `launch()` ä¸­çš„ `blocked_paths` å‚æ•°å…è®¸çš„æ–‡ä»¶ã€‚** æ‚¨å¯ä»¥å°†å…¶ä»–ç›®å½•æˆ–ç¡®åˆ‡æ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨ä¼ é€’ç»™ `launch()` ä¸­çš„ `blocked_paths` å‚æ•°ã€‚æ­¤å‚æ•°ä¼˜å…ˆäºŽ Gradio é»˜è®¤æˆ– `allowed_paths` å…è®¸çš„æ–‡ä»¶ã€‚

- **ä¸»æœºæœºå™¨ä¸Šçš„ä»»ä½•å…¶ä»–è·¯å¾„**ã€‚ç”¨æˆ·ä¸åº”èƒ½å¤Ÿè®¿é—®ä¸»æœºä¸Šçš„å…¶ä»–ä»»æ„è·¯å¾„ã€‚

è¯·ç¡®ä¿æ‚¨æ­£åœ¨è¿è¡Œæœ€æ–°ç‰ˆæœ¬çš„ `gradio`ï¼Œä»¥ä½¿è¿™äº›å®‰å…¨è®¾ç½®ç”Ÿæ•ˆã€‚

---

<!-- Source: guides/cn/02_building-interfaces/03_more-on-examples.md -->
# æ›´å¤šç¤ºä¾‹ (More on Examples)

æœ¬æŒ‡å—ä»‹ç»äº†æœ‰å…³ç¤ºä¾‹çš„æ›´å¤šå†…å®¹ï¼šä»Žç›®å½•ä¸­åŠ è½½ç¤ºä¾‹ï¼Œæä¾›éƒ¨åˆ†ç¤ºä¾‹å’Œç¼“å­˜ã€‚å¦‚æžœä½ å¯¹ç¤ºä¾‹è¿˜ä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹ [å…³é”®ç‰¹æ€§](../key-features/#example-inputs) æŒ‡å—ä¸­çš„ä»‹ç»ã€‚

## æä¾›ç¤ºä¾‹ (Providing Examples)

æ­£å¦‚ [å…³é”®ç‰¹æ€§](../key-features/#example-inputs) æŒ‡å—ä¸­æ‰€ä»‹ç»çš„ï¼Œå‘æŽ¥å£æ·»åŠ ç¤ºä¾‹å°±åƒæä¾›ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ç»™ `examples` å…³é”®å­—å‚æ•°ä¸€æ ·ç®€å•ã€‚
æ¯ä¸ªå­åˆ—è¡¨éƒ½æ˜¯ä¸€ä¸ªæ•°æ®æ ·æœ¬ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ å¯¹åº”äºŽé¢„æµ‹å‡½æ•°çš„ä¸€ä¸ªè¾“å…¥ã€‚
è¾“å…¥å¿…é¡»æŒ‰ç…§ä¸Žé¢„æµ‹å‡½æ•°æœŸæœ›çš„é¡ºåºæŽ’åºã€‚

å¦‚æžœä½ çš„æŽ¥å£åªæœ‰ä¸€ä¸ªè¾“å…¥ç»„ä»¶ï¼Œé‚£ä¹ˆå¯ä»¥å°†ç¤ºä¾‹æä¾›ä¸ºå¸¸è§„åˆ—è¡¨ï¼Œè€Œä¸æ˜¯åˆ—è¡¨çš„åˆ—è¡¨ã€‚

### ä»Žç›®å½•åŠ è½½ç¤ºä¾‹ (Loading Examples from a Directory)

ä½ è¿˜å¯ä»¥æŒ‡å®šä¸€ä¸ªåŒ…å«ç¤ºä¾‹çš„ç›®å½•è·¯å¾„ã€‚å¦‚æžœä½ çš„æŽ¥å£åªæŽ¥å—å•ä¸ªæ–‡ä»¶ç±»åž‹çš„è¾“å…¥ï¼ˆä¾‹å¦‚å›¾åƒåˆ†ç±»å™¨ï¼‰ï¼Œä½ åªéœ€å°†ç›®å½•æ–‡ä»¶è·¯å¾„ä¼ é€’ç»™ `examples=` å‚æ•°ï¼Œ`Interface` å°†åŠ è½½ç›®å½•ä¸­çš„å›¾åƒä½œä¸ºç¤ºä¾‹ã€‚
å¯¹äºŽå¤šä¸ªè¾“å…¥ï¼Œè¯¥ç›®å½•å¿…é¡»åŒ…å«ä¸€ä¸ªå¸¦æœ‰ç¤ºä¾‹å€¼çš„ log.csv æ–‡ä»¶ã€‚
åœ¨è®¡ç®—å™¨æ¼”ç¤ºçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½® `examples='/demo/calculator/examples'` ï¼Œåœ¨è¯¥ç›®å½•ä¸­åŒ…å«ä»¥ä¸‹ `log.csv` æ–‡ä»¶ï¼š
contain a log.csv file with the example values.
In the context of the calculator demo, we can set `examples='/demo/calculator/examples'` and in that directory we include the following `log.csv` file:

```csv
num,operation,num2
5,"add",3
4,"divide",2
5,"multiply",3
```

å½“æµè§ˆæ ‡è®°æ•°æ®æ—¶ï¼Œè¿™å°†éžå¸¸æœ‰ç”¨ã€‚åªéœ€æŒ‡å‘æ ‡è®°ç›®å½•ï¼Œ`Interface` å°†ä»Žæ ‡è®°æ•°æ®åŠ è½½ç¤ºä¾‹ã€‚

### æä¾›éƒ¨åˆ†ç¤ºä¾‹

æœ‰æ—¶ä½ çš„åº”ç”¨ç¨‹åºæœ‰è®¸å¤šè¾“å…¥ç»„ä»¶ï¼Œä½†ä½ åªæƒ³ä¸ºå…¶ä¸­çš„ä¸€éƒ¨åˆ†æä¾›ç¤ºä¾‹ã€‚ä¸ºäº†åœ¨ç¤ºä¾‹ä¸­æŽ’é™¤æŸäº›è¾“å…¥ï¼Œå¯¹äºŽé‚£äº›ç‰¹å®šè¾“å…¥å¯¹åº”çš„æ‰€æœ‰æ•°æ®æ ·æœ¬éƒ½ä¼ é€’ `None`ã€‚

## ç¤ºä¾‹ç¼“å­˜ (Caching examples)

ä½ å¯èƒ½å¸Œæœ›ä¸ºç”¨æˆ·æä¾›ä¸€äº›æ¨¡åž‹çš„ç¼“å­˜ç¤ºä¾‹ï¼Œä»¥ä¾¿ä»–ä»¬å¯ä»¥å¿«é€Ÿå°è¯•ï¼Œä»¥é˜²æ‚¨çš„æ¨¡åž‹è¿è¡Œæ—¶é—´è¾ƒé•¿ã€‚
å¦‚æžœ `cache_examples=True` ï¼Œå½“ä½ è°ƒç”¨ `launch()` æ–¹æ³•æ—¶ï¼Œ`Interface` å°†è¿è¡Œæ‰€æœ‰ç¤ºä¾‹ï¼Œå¹¶ä¿å­˜è¾“å‡ºã€‚è¿™äº›æ•°æ®å°†ä¿å­˜åœ¨ä¸€ä¸ªåä¸º `gradio_cached_examples` çš„ç›®å½•ä¸­ã€‚

æ¯å½“ç”¨æˆ·ç‚¹å‡»ç¤ºä¾‹æ—¶ï¼Œè¾“å‡ºå°†è‡ªåŠ¨å¡«å……åˆ°åº”ç”¨ç¨‹åºä¸­ï¼Œä½¿ç”¨æ¥è‡ªè¯¥ç¼“å­˜ç›®å½•çš„æ•°æ®ï¼Œè€Œä¸æ˜¯å®žé™…è¿è¡Œå‡½æ•°ã€‚è¿™å¯¹äºŽç”¨æˆ·å¯ä»¥å¿«é€Ÿå°è¯•æ‚¨çš„æ¨¡åž‹è€Œä¸å¢žåŠ ä»»ä½•è´Ÿè½½æ˜¯éžå¸¸æœ‰ç”¨çš„ï¼

è¯·è®°ä½ä¸€æ—¦ç”Ÿæˆäº†ç¼“å­˜ï¼Œå®ƒå°†ä¸ä¼šåœ¨ä»¥åŽçš„å¯åŠ¨ä¸­æ›´æ–°ã€‚å¦‚æžœç¤ºä¾‹æˆ–å‡½æ•°é€»è¾‘å‘ç”Ÿæ›´æ”¹ï¼Œè¯·åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤¹ä»¥æ¸…é™¤ç¼“å­˜å¹¶ä½¿ç”¨å¦ä¸€ä¸ª `launch()` é‡æ–°æž„å»ºå®ƒã€‚

---

<!-- Source: guides/cn/03_building-with-blocks/03_state-in-blocks.md -->
# åˆ†å—çŠ¶æ€ (State in Blocks)

æˆ‘ä»¬å·²ç»ä»‹ç»äº†[æŽ¥å£çŠ¶æ€](https://gradio.app/interface-state)ï¼Œè¿™ç¯‡æŒ‡å—å°†ä»‹ç»åˆ†å—çŠ¶æ€ï¼Œå®ƒçš„å·¥ä½œåŽŸç†å¤§è‡´ç›¸åŒã€‚

## å…¨å±€çŠ¶æ€ (Global State)

åˆ†å—ä¸­çš„å…¨å±€çŠ¶æ€ä¸ŽæŽ¥å£ä¸­çš„å…¨å±€çŠ¶æ€ç›¸åŒã€‚åœ¨å‡½æ•°è°ƒç”¨å¤–åˆ›å»ºçš„ä»»ä½•å˜é‡éƒ½æ˜¯åœ¨æ‰€æœ‰ç”¨æˆ·ä¹‹é—´å…±äº«çš„å¼•ç”¨ã€‚

## ä¼šè¯çŠ¶æ€ (Session State)

Gradio åœ¨åˆ†å—åº”ç”¨ç¨‹åºä¸­åŒæ ·æ”¯æŒä¼šè¯**çŠ¶æ€**ï¼Œå³åœ¨é¡µé¢ä¼šè¯ä¸­è·¨å¤šæ¬¡æäº¤ä¿æŒçš„æ•°æ®ã€‚éœ€è¦å†æ¬¡å¼ºè°ƒï¼Œä¼šè¯æ•°æ®*ä¸ä¼š*åœ¨æ¨¡åž‹çš„ä¸åŒç”¨æˆ·ä¹‹é—´å…±äº«ã€‚è¦åœ¨ä¼šè¯çŠ¶æ€ä¸­å­˜å‚¨æ•°æ®ï¼Œéœ€è¦å®Œæˆä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1. åˆ›å»ºä¸€ä¸ª `gr.State()` å¯¹è±¡ã€‚å¦‚æžœæ­¤å¯çŠ¶æ€å¯¹è±¡æœ‰ä¸€ä¸ªé»˜è®¤å€¼ï¼Œè¯·å°†å…¶ä¼ é€’ç»™æž„é€ å‡½æ•°ã€‚
2. åœ¨äº‹ä»¶ç›‘å¬å™¨ä¸­ï¼Œå°† `State` å¯¹è±¡ä½œä¸ºè¾“å…¥å’Œè¾“å‡ºã€‚
3. åœ¨äº‹ä»¶ç›‘å¬å™¨å‡½æ•°ä¸­ï¼Œå°†å˜é‡æ·»åŠ åˆ°è¾“å…¥å‚æ•°å’Œè¿”å›žå€¼ä¸­ã€‚

è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªçŒœè¯æ¸¸æˆçš„ä¾‹å­ã€‚

$code_hangman
$demo_hangman

è®©æˆ‘ä»¬çœ‹çœ‹åœ¨è¿™ä¸ªæ¸¸æˆä¸­å¦‚ä½•å®Œæˆä¸Šè¿°çš„ 3 ä¸ªæ­¥éª¤ï¼š

1. æˆ‘ä»¬å°†å·²ä½¿ç”¨çš„å­—æ¯å­˜å‚¨åœ¨ `used_letters_var` ä¸­ã€‚åœ¨ `State` çš„æž„é€ å‡½æ•°ä¸­ï¼Œå°†å…¶åˆå§‹å€¼è®¾ç½®ä¸ºç©ºåˆ—è¡¨`[]`ã€‚
2. åœ¨ `btn.click()` ä¸­ï¼Œæˆ‘ä»¬åœ¨è¾“å…¥å’Œè¾“å‡ºä¸­éƒ½å¼•ç”¨äº† `used_letters_var`ã€‚
3. åœ¨ `guess_letter` ä¸­ï¼Œæˆ‘ä»¬å°†æ­¤ `State` çš„å€¼ä¼ é€’ç»™ `used_letters`ï¼Œç„¶åŽåœ¨è¿”å›žè¯­å¥ä¸­è¿”å›žæ›´æ–°åŽçš„è¯¥ `State` çš„å€¼ã€‚

å¯¹äºŽæ›´å¤æ‚çš„åº”ç”¨ç¨‹åºï¼Œæ‚¨å¯èƒ½ä¼šåœ¨ä¸€ä¸ªå•ç‹¬çš„åˆ†å—åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨è®¸å¤šå­˜å‚¨ä¼šè¯çŠ¶æ€çš„ `State` å˜é‡ã€‚

åœ¨[æ–‡æ¡£](https://gradio.app/docs/state)ä¸­äº†è§£æ›´å¤šå…³äºŽ `State` çš„ä¿¡æ¯ã€‚

---

<!-- Source: guides/02_building-interfaces/04_reactive-interfaces.md -->
# Reactive Interfaces

Finally, we cover how to get Gradio demos to refresh automatically or continuously stream data.

## Live Interfaces

You can make interfaces automatically refresh by setting `live=True` in the interface. Now the interface will recalculate as soon as the user input changes.

$code_calculator_live
$demo_calculator_live

Note there is no submit button, because the interface resubmits automatically on change.

## Streaming Components

Some components have a "streaming" mode, such as `Audio` component in microphone mode, or the `Image` component in webcam mode. Streaming means data is sent continuously to the backend and the `Interface` function is continuously being rerun.

The difference between `gr.Audio(source='microphone')` and `gr.Audio(source='microphone', streaming=True)`, when both are used in `gr.Interface(live=True)`, is that the first `Component` will automatically submit data and run the `Interface` function when the user stops recording, whereas the second `Component` will continuously send data and run the `Interface` function _during_ recording.

Here is example code of streaming images from the webcam.

$code_stream_frames

Streaming can also be done in an output component. A `gr.Audio(streaming=True)` output component can take a stream of audio data yielded piece-wise by a generator function and combines them into a single audio file. For a detailed example, see our guide on performing [automatic speech recognition](/guides/real-time-speech-recognition) with Gradio.

---

<!-- Source: guides/03_building-with-blocks/04_dynamic-apps-with-render-decorator.md -->
# Dynamic Apps with the Render Decorator

The components and event listeners you define in a Blocks so far have been fixed - once the demo was launched, new components and listeners could not be added, and existing one could not be removed. 

The `@gr.render` decorator introduces the ability to dynamically change this. Let's take a look. 

## Dynamic Number of Components

In the example below, we will create a variable number of Textboxes. When the user edits the input Textbox, we create a Textbox for each letter in the input. Try it out below:

$code_render_split_simple
$demo_render_split_simple

See how we can now create a variable number of Textboxes using our custom logic - in this case, a simple `for` loop. The `@gr.render` decorator enables this with the following steps:

1. Create a function and attach the @gr.render decorator to it.
2. Add the input components to the `inputs=` argument of @gr.render, and create a corresponding argument in your function for each component. This function will automatically re-run on any change to a component.
3. Add all components inside the function that you want to render based on the inputs.

Now whenever the inputs change, the function re-runs, and replaces the components created from the previous function run with the latest run. Pretty straightforward! Let's add a little more complexity to this app:

$code_render_split
$demo_render_split

By default, `@gr.render` re-runs are triggered by the `.load` listener to the app and the `.change` listener to any input component provided. We can override this by explicitly setting the triggers in the decorator, as we have in this app to only trigger on `input_text.submit` instead. 
If you are setting custom triggers, and you also want an automatic render at the start of the app, make sure to add `demo.load` to your list of triggers.

## Dynamic Event Listeners

If you're creating components, you probably want to attach event listeners to them as well. Let's take a look at an example that takes in a variable number of Textbox as input, and merges all the text into a single box.

$code_render_merge_simple
$demo_render_merge_simple

Let's take a look at what's happening here:

1. The state variable `text_count` is keeping track of the number of Textboxes to create. By clicking on the Add button, we increase `text_count` which triggers the render decorator.
2. Note that in every single Textbox we create in the render function, we explicitly set a `key=` argument. This key allows us to preserve the value of this Component between re-renders. If you type in a value in a textbox, and then click the Add button, all the Textboxes re-render, but their values aren't cleared because the `key=` maintains the the value of a Component across a render.
3. We've stored the Textboxes created in a list, and provide this list as input to the merge button event listener. Note that **all event listeners that use Components created inside a render function must also be defined inside that render function**. The event listener can still reference Components outside the render function, as we do here by referencing `merge_btn` and `output` which are both defined outside the render function.

Just as with Components, whenever a function re-renders, the event listeners created from the previous render are cleared and the new event listeners from the latest run are attached. 

This allows us to create highly customizable and complex interactions! 

## Closer Look at `keys=` parameter

The `key=` argument is used to let Gradio know that the same component is being generated when your render function re-runs. This does two things:

1. The same element in the browser is re-used from the previous render for this Component. This gives browser performance gains - as there's no need to destroy and rebuild a component on a render - and preserves any browser attributes that the Component may have had. If your Component is nested within layout items like `gr.Row`, make sure they are keyed as well because the keys of the parents must also match.
2. Properties that may be changed by the user or by other event listeners are preserved. By default, only the "value" of Component is preserved, but you can specify any list of properties to preserve using the `preserved_by_key=` kwarg.

See the example below:

$code_render_preserve_key
$demo_render_preserve_key

You'll see in this example, when you change the `number_of_boxes` slider, there's a new re-render to update the number of box rows. If you click the "Change Label" buttons, they change the `label` and `info` properties of the corresponding textbox. You can also enter text in any textbox to change its value. If you change number of boxes after this, the re-renders "reset" the `info`, but the `label` and any entered `value` is still preserved.

Note you can also key any event listener, e.g. `button.click(key=...)` if the same listener is being recreated with the same inputs and outputs across renders. This gives performance benefits, and also prevents errors from occurring if an event was triggered in a previous render, then a re-render occurs, and then the previous event finishes processing. By keying your listener, Gradio knows where to send the data properly. 

## Putting it Together

Let's look at two examples that use all the features above. First, try out the to-do list app below: 

$code_todo_list
$demo_todo_list

Note that almost the entire app is inside a single `gr.render` that reacts to the tasks `gr.State` variable. This variable is a nested list, which presents some complexity. If you design a `gr.render` to react to a list or dict structure, ensure you do the following:

1. Any event listener that modifies a state variable in a manner that should trigger a re-render must set the state variable as an output. This lets Gradio know to check if the variable has changed behind the scenes. 
2. In a `gr.render`, if a variable in a loop is used inside an event listener function, that variable should be "frozen" via setting it to itself as a default argument in the function header. See how we have `task=task` in both `mark_done` and `delete`. This freezes the variable to its "loop-time" value.

Let's take a look at one last example that uses everything we learned. Below is an audio mixer. Provide multiple audio tracks and mix them together.

$code_audio_mixer
$demo_audio_mixer

Two things to note in this app:
1. Here we provide `key=` to all the components! We need to do this so that if we add another track after setting the values for an existing track, our input values to the existing track do not get reset on re-render.
2. When there are lots of components of different types and arbitrary counts passed to an event listener, it is easier to use the set and dictionary notation for inputs rather than list notation. Above, we make one large set of all the input `gr.Audio` and `gr.Slider` components when we pass the inputs to the `merge` function. In the function body we query the component values as a dict.

The `gr.render` expands gradio capabilities extensively - see what you can make out of it!

---

<!-- Source: guides/04_additional-features/04_alerts.md -->
# Alerts

You may wish to display alerts to the user. To do so, raise a `gr.Error("custom message")` in your function to halt the execution of your function and display an error message to the user.

You can also issue `gr.Warning("custom message")` or `gr.Info("custom message")` by having them as standalone lines in your function, which will immediately display modals while continuing the execution of your function. The only difference between `gr.Info()` and `gr.Warning()` is the color of the alert. 

```python
def start_process(name):
    gr.Info("Starting process")
    if name is None:
        gr.Warning("Name is empty")
    ...
    if success == False:
        raise gr.Error("Process failed")
```

Tip: Note that `gr.Error()` is an exception that has to be raised, while `gr.Warning()` and `gr.Info()` are functions that are called directly.

---

<!-- Source: guides/05_chatbots/04_creating-a-custom-chatbot-with-blocks.md -->
# How to Create a Custom Chatbot with Gradio Blocks

Tags: NLP, TEXT, CHAT
Related spaces: https://huggingface.co/spaces/gradio/chatbot_streaming, https://huggingface.co/spaces/project-baize/Baize-7B,

## Introduction

**Important Note**: if you are getting started, we recommend using the `gr.ChatInterface` to create chatbots -- its a high-level abstraction that makes it possible to create beautiful chatbot applications fast, often with a single line of code. [Read more about it here](/guides/creating-a-chatbot-fast).

This tutorial will show how to make chatbot UIs from scratch with Gradio's low-level Blocks API. This will give you full control over your Chatbot UI. You'll start by first creating a a simple chatbot to display text, a second one to stream text responses, and finally a chatbot that can handle media files as well. The chatbot interface that we create will look something like this:

$demo_chatbot_streaming

**Prerequisite**: We'll be using the `gradio.Blocks` class to build our Chatbot demo.
You can [read the Guide to Blocks first](https://gradio.app/blocks-and-event-listeners) if you are not already familiar with it. Also please make sure you are using the **latest version** version of Gradio: `pip install --upgrade gradio`.

## A Simple Chatbot Demo

Let's start with recreating the simple demo above. As you may have noticed, our bot simply randomly responds "How are you?", "Today is a great day", or "I'm very hungry" to any input. Here's the code to create this with Gradio:

$code_chatbot_simple

There are three Gradio components here:

- A `Chatbot`, whose value stores the entire history of the conversation, as a list of response pairs between the user and bot.
- A `Textbox` where the user can type their message, and then hit enter/submit to trigger the chatbot response
- A `ClearButton` button to clear the Textbox and entire Chatbot history

We have a single function, `respond()`, which takes in the entire history of the chatbot, appends a random message, waits 1 second, and then returns the updated chat history. The `respond()` function also clears the textbox when it returns.

Of course, in practice, you would replace `respond()` with your own more complex function, which might call a pretrained model or an API, to generate a response.

$demo_chatbot_simple

Tip: For better type hinting and auto-completion in your IDE, you can use the `gr.ChatMessage` dataclass:

```python
from gradio import ChatMessage

def chat_function(message, history):
    history.append(ChatMessage(role="user", content=message))
    history.append(ChatMessage(role="assistant", content="Hello, how can I help you?"))
    return history
```

## Add Streaming to your Chatbot

There are several ways we can improve the user experience of the chatbot above. First, we can stream responses so the user doesn't have to wait as long for a message to be generated. Second, we can have the user message appear immediately in the chat history, while the chatbot's response is being generated. Here's the code to achieve that:

$code_chatbot_streaming

You'll notice that when a user submits their message, we now _chain_ two event events with `.then()`:

1. The first method `user()` updates the chatbot with the user message and clears the input field. Because we want this to happen instantly, we set `queue=False`, which would skip any queue had it been enabled. The chatbot's history is appended with `{"role": "user", "content": user_message}`.

2. The second method, `bot()` updates the chatbot history with the bot's response. Finally, we construct the message character by character and `yield` the intermediate outputs as they are being constructed. Gradio automatically turns any function with the `yield` keyword [into a streaming output interface](/guides/key-features/#iterative-outputs).


Of course, in practice, you would replace `bot()` with your own more complex function, which might call a pretrained model or an API, to generate a response.


## Adding Markdown, Images, Audio, or Videos

The `gr.Chatbot` component supports a subset of markdown including bold, italics, and code. For example, we could write a function that responds to a user's message, with a bold **That's cool!**, like this:

```py
def bot(history):
    response = {"role": "assistant", "content": "**That's cool!**"}
    history.append(response)
    return history
```

In addition, it can handle media files, such as images, audio, and video. You can use the `MultimodalTextbox` component to easily upload all types of media files to your chatbot. You can customize the `MultimodalTextbox` further by passing in the `sources` parameter, which is a list of sources to enable. To pass in a media file, we must pass in the file a dictionary with a `path` key pointing to a local file and an `alt_text` key. The `alt_text` is optional, so you can also just pass in a tuple with a single element `{"path": "filepath"}`, like this:

```python
def add_message(history, message):
    for x in message["files"]:
        history.append({"role": "user", "content": {"path": x}})
    if message["text"] is not None:
        history.append({"role": "user", "content": message["text"]})
    return history, gr.MultimodalTextbox(value=None, interactive=False, file_types=["image"], sources=["upload", "microphone"])
```

Putting this together, we can create a _multimodal_ chatbot with a multimodal textbox for a user to submit text and media files. The rest of the code looks pretty much the same as before:

$code_chatbot_multimodal
$demo_chatbot_multimodal

And you're done! That's all the code you need to build an interface for your chatbot model. Finally, we'll end our Guide with some links to Chatbots that are running on Spaces so that you can get an idea of what else is possible:

- [project-baize/Baize-7B](https://huggingface.co/spaces/project-baize/Baize-7B): A stylized chatbot that allows you to stop generation as well as regenerate responses.
- [MAGAer13/mPLUG-Owl](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl): A multimodal chatbot that allows you to upvote and downvote responses.

---

<!-- Source: guides/06_data-science-and-plots/04_connecting-to-a-database.md -->
# Connecting to a Database

The data you wish to visualize may be stored in a database. Let's use SQLAlchemy to quickly extract database content into pandas Dataframe format so we can use it in gradio.

First install `pip install sqlalchemy` and then let's see some examples.

## SQLite

```python
from sqlalchemy import create_engine
import pandas as pd

engine = create_engine('sqlite:///your_database.db')

with gr.Blocks() as demo:
    gr.LinePlot(pd.read_sql_query("SELECT time, price from flight_info;", engine), x="time", y="price")
```

Let's see a a more interactive plot involving filters that modify your SQL query:

```python
from sqlalchemy import create_engine
import pandas as pd

engine = create_engine('sqlite:///your_database.db')

with gr.Blocks() as demo:
    origin = gr.Dropdown(["DFW", "DAL", "HOU"], value="DFW", label="Origin")

    gr.LinePlot(lambda origin: pd.read_sql_query(f"SELECT time, price from flight_info WHERE origin = {origin};", engine), inputs=origin, x="time", y="price")
```

## Postgres, mySQL, and other databases

If you're using a different database format, all you have to do is swap out the engine, e.g.

```python
engine = create_engine('postgresql://username:password@host:port/database_name')
```

```python
engine = create_engine('mysql://username:password@host:port/database_name')
```

```python
engine = create_engine('oracle://username:password@host:port/database_name')
```

---

<!-- Source: guides/07_streaming/04_conversational-chatbot.md -->
# Building Conversational Chatbots with Gradio

Tags: AUDIO, STREAMING, CHATBOTS

## Introduction

The next generation of AI user interfaces is moving towards audio-native experiences. Users will be able to speak to chatbots and receive spoken responses in return. Several models have been built under this paradigm, including GPT-4o and [mini omni](https://github.com/gpt-omni/mini-omni).

In this guide, we'll walk you through building your own conversational chat application using mini omni as an example. You can see a demo of the finished app below:

<video src="https://github.com/user-attachments/assets/db36f4db-7535-49f1-a2dd-bd36c487ebdf" controls
height="600" width="600" style="display: block; margin: auto;" autoplay="true" loop="true">
</video>

## Application Overview

Our application will enable the following user experience:

1. Users click a button to start recording their message
2. The app detects when the user has finished speaking and stops recording
3. The user's audio is passed to the omni model, which streams back a response
4. After omni mini finishes speaking, the user's microphone is reactivated
5. All previous spoken audio, from both the user and omni, is displayed in a chatbot component

Let's dive into the implementation details.

## Processing User Audio

We'll stream the user's audio from their microphone to the server and determine if the user has stopped speaking on each new chunk of audio.

Here's our `process_audio` function:

```python
import numpy as np
from utils import determine_pause

def process_audio(audio: tuple, state: AppState):
    if state.stream is None:
        state.stream = audio[1]
        state.sampling_rate = audio[0]
    else:
        state.stream = np.concatenate((state.stream, audio[1]))

    pause_detected = determine_pause(state.stream, state.sampling_rate, state)
    state.pause_detected = pause_detected

    if state.pause_detected and state.started_talking:
        return gr.Audio(recording=False), state
    return None, state
```

This function takes two inputs:
1. The current audio chunk (a tuple of `(sampling_rate, numpy array of audio)`)
2. The current application state

We'll use the following `AppState` dataclass to manage our application state:

```python
from dataclasses import dataclass

@dataclass
class AppState:
    stream: np.ndarray | None = None
    sampling_rate: int = 0
    pause_detected: bool = False
    stopped: bool = False
    conversation: list = []
```

The function concatenates new audio chunks to the existing stream and checks if the user has stopped speaking. If a pause is detected, it returns an update to stop recording. Otherwise, it returns `None` to indicate no changes.

The implementation of the `determine_pause` function is specific to the omni-mini project and can be found [here](https://huggingface.co/spaces/gradio/omni-mini/blob/eb027808c7bfe5179b46d9352e3fa1813a45f7c3/app.py#L98).

## Generating the Response

After processing the user's audio, we need to generate and stream the chatbot's response. Here's our `response` function:

```python
import io
import tempfile
from pydub import AudioSegment

def response(state: AppState):
    if not state.pause_detected and not state.started_talking:
        return None, AppState()
    
    audio_buffer = io.BytesIO()

    segment = AudioSegment(
        state.stream.tobytes(),
        frame_rate=state.sampling_rate,
        sample_width=state.stream.dtype.itemsize,
        channels=(1 if len(state.stream.shape) == 1 else state.stream.shape[1]),
    )
    segment.export(audio_buffer, format="wav")

    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        f.write(audio_buffer.getvalue())
    
    state.conversation.append({"role": "user",
                                "content": {"path": f.name,
                                "mime_type": "audio/wav"}})
    
    output_buffer = b""

    for mp3_bytes in speaking(audio_buffer.getvalue()):
        output_buffer += mp3_bytes
        yield mp3_bytes, state

    with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f:
        f.write(output_buffer)
    
    state.conversation.append({"role": "assistant",
                    "content": {"path": f.name,
                                "mime_type": "audio/mp3"}})
    yield None, AppState(conversation=state.conversation)
```

This function:
1. Converts the user's audio to a WAV file
2. Adds the user's message to the conversation history
3. Generates and streams the chatbot's response using the `speaking` function
4. Saves the chatbot's response as an MP3 file
5. Adds the chatbot's response to the conversation history

Note: The implementation of the `speaking` function is specific to the omni-mini project and can be found [here](https://huggingface.co/spaces/gradio/omni-mini/blob/main/app.py#L116).

## Building the Gradio App

Now let's put it all together using Gradio's Blocks API:

```python
import gradio as gr

def start_recording_user(state: AppState):
    if not state.stopped:
        return gr.Audio(recording=True)

with gr.Blocks() as demo:
    with gr.Row():
        with gr.Column():
            input_audio = gr.Audio(
                label="Input Audio", sources="microphone", type="numpy"
            )
        with gr.Column():
            chatbot = gr.Chatbot(label="Conversation")
            output_audio = gr.Audio(label="Output Audio", streaming=True, autoplay=True)
    state = gr.State(value=AppState())

    stream = input_audio.stream(
        process_audio,
        [input_audio, state],
        [input_audio, state],
        stream_every=0.5,
        time_limit=30,
    )
    respond = input_audio.stop_recording(
        response,
        [state],
        [output_audio, state]
    )
    respond.then(lambda s: s.conversation, [state], [chatbot])

    restart = output_audio.stop(
        start_recording_user,
        [state],
        [input_audio]
    )
    cancel = gr.Button("Stop Conversation", variant="stop")
    cancel.click(lambda: (AppState(stopped=True), gr.Audio(recording=False)), None,
                [state, input_audio], cancels=[respond, restart])

if __name__ == "__main__":
    demo.launch()
```

This setup creates a user interface with:
- An input audio component for recording user messages
- A chatbot component to display the conversation history
- An output audio component for the chatbot's responses
- A button to stop and reset the conversation

The app streams user audio in 0.5-second chunks, processes it, generates responses, and updates the conversation history accordingly.

## Conclusion

This guide demonstrates how to build a conversational chatbot application using Gradio and the mini omni model. You can adapt this framework to create various audio-based chatbot demos. To see the full application in action, visit the Hugging Face Spaces demo: https://huggingface.co/spaces/gradio/omni-mini

Feel free to experiment with different models, audio processing techniques, or user interface designs to create your own unique conversational AI experiences!

---

<!-- Source: guides/08_custom-components/04_backend.md -->
# The Backend ðŸ

This guide will cover everything you need to know to implement your custom component's backend processing.

## Which Class to Inherit From

All components inherit from one of three classes `Component`, `FormComponent`, or `BlockContext`.
You need to inherit from one so that your component behaves like all other gradio components.
When you start from a template with `gradio cc create --template`, you don't need to worry about which one to choose since the template uses the correct one. 
For completeness, and in the event that you need to make your own component from scratch, we explain what each class is for.

* `FormComponent`: Use this when you want your component to be grouped together in the same `Form` layout with other `FormComponents`. The `Slider`, `Textbox`, and `Number` components are all `FormComponents`.
* `BlockContext`: Use this when you want to place other components "inside" your component. This enabled `with MyComponent() as component:` syntax.
* `Component`: Use this for all other cases.

Tip: If your component supports streaming output, inherit from the `StreamingOutput` class.

Tip: If you inherit from `BlockContext`, you also need to set the metaclass to be `ComponentMeta`. See example below.

```python
from gradio.blocks import BlockContext
from gradio.component_meta import ComponentMeta




@document()
class Row(BlockContext, metaclass=ComponentMeta):
    pass
```

## The methods you need to implement

When you inherit from any of these classes, the following methods must be implemented.
Otherwise the Python interpreter will raise an error when you instantiate your component!

### `preprocess` and `postprocess`

Explained in the [Key Concepts](./key-component-concepts#the-value-and-how-it-is-preprocessed-postprocessed) guide. 
They handle the conversion from the data sent by the frontend to the format expected by the python function.

```python
    def preprocess(self, x: Any) -> Any:
        """
        Convert from the web-friendly (typically JSON) value in the frontend to the format expected by the python function.
        """
        return x

    def postprocess(self, y):
        """
        Convert from the data returned by the python function to the web-friendly (typically JSON) value expected by the frontend.
        """
        return y
```

### `process_example`

Takes in the original Python value and returns the modified value that should be displayed in the examples preview in the app. 
If not provided, the `.postprocess()` method is used instead. Let's look at the following example from the `SimpleDropdown` component.

```python
def process_example(self, input_data):
    return next((c[0] for c in self.choices if c[1] == input_data), None)
```

Since `self.choices` is a list of tuples corresponding to (`display_name`, `value`), this converts the value that a user provides to the display value (or if the value is not present in `self.choices`, it is converted to `None`).


### `api_info`

A JSON-schema representation of the value that the `preprocess` expects. 
This powers api usage via the gradio clients. 
You do **not** need to implement this yourself if you components specifies a `data_model`. 
The `data_model` in the following section.

```python
def api_info(self) -> dict[str, list[str]]:
    """
    A JSON-schema representation of the value that the `preprocess` expects and the `postprocess` returns.
    """
    pass
```

### `example_payload`

An example payload for your component, e.g. something that can be passed into the `.preprocess()` method
of your component. The example input is displayed in the `View API` page of a Gradio app that uses your custom component. 
Must be JSON-serializable. If your component expects a file, it is best to use a publicly accessible URL.

```python
def example_payload(self) -> Any:
    """
    The example inputs for this component for API usage. Must be JSON-serializable.
    """
    pass
```

### `example_value`

An example value for your component, e.g. something that can be passed into the `.postprocess()` method
of your component. This is used as the example value in the default app that is created in custom component development.

```python
def example_payload(self) -> Any:
    """
    The example inputs for this component for API usage. Must be JSON-serializable.
    """
    pass
```

### `flag`

Write the component's value to a format that can be stored in the `csv` or `json` file used for flagging.
You do **not** need to implement this yourself if you components specifies a `data_model`. 
The `data_model` in the following section.

```python
def flag(self, x: Any | GradioDataModel, flag_dir: str | Path = "") -> str:
    pass
```

### `read_from_flag`
Convert from the format stored in the `csv` or `json` file used for flagging to the component's python `value`.
You do **not** need to implement this yourself if you components specifies a `data_model`. 
The `data_model` in the following section.

```python
def read_from_flag(
    self,
    x: Any,
) -> GradioDataModel | Any:
    """
    Convert the data from the csv or jsonl file into the component state.
    """
    return x
```

## The `data_model`

The `data_model` is how you define the expected data format your component's value will be stored in the frontend.
It specifies the data format your `preprocess` method expects and the format the `postprocess` method returns.
It is not necessary to define a `data_model` for your component but it greatly simplifies the process of creating a custom component.
If you define a custom component you only need to implement four methods - `preprocess`, `postprocess`, `example_payload`, and `example_value`!

You define a `data_model` by defining a [pydantic model](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage) that inherits from either `GradioModel` or `GradioRootModel`.

This is best explained with an example. Let's look at the core `Video` component, which stores the video data as a JSON object with two keys `video` and `subtitles` which point to separate files.

```python
from gradio.data_classes import FileData, GradioModel

class VideoData(GradioModel):
    video: FileData
    subtitles: Optional[FileData] = None

class Video(Component):
    data_model = VideoData
```

By adding these four lines of code, your component automatically implements the methods needed for API usage, the flagging methods, and example caching methods!
It also has the added benefit of self-documenting your code.
Anyone who reads your component code will know exactly the data it expects.

Tip: If your component expects files to be uploaded from the frontend, your must use the `FileData` model! It will be explained in the following section. 

Tip: Read the pydantic docs [here](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage).

The difference between a `GradioModel` and a `GradioRootModel` is that the `RootModel` will not serialize the data to a dictionary.
For example, the `Names` model will serialize the data to `{'names': ['freddy', 'pete']}` whereas the `NamesRoot` model will serialize it to `['freddy', 'pete']`.

```python
from typing import List

class Names(GradioModel):
    names: List[str]

class NamesRoot(GradioRootModel):
    root: List[str]
```

Even if your component does not expect a "complex" JSON data structure it can be beneficial to define a `GradioRootModel` so that you don't have to worry about implementing the API and flagging methods.

Tip: Use classes from the Python typing library to type your models. e.g. `List` instead of `list`.

## Handling Files

If your component expects uploaded files as input, or returns saved files to the frontend, you **MUST** use the `FileData` to type the files in your `data_model`.

When you use the `FileData`:

* Gradio knows that it should allow serving this file to the frontend. Gradio automatically blocks requests to serve arbitrary files in the computer running the server.

* Gradio will automatically place the file in a cache so that duplicate copies of the file don't get saved.

* The client libraries will automatically know that they should upload input files prior to sending the request. They will also automatically download files.

If you do not use the `FileData`, your component will not work as expected!


## Adding Event Triggers To Your Component

The events triggers for your component are defined in the `EVENTS` class attribute.
This is a list that contains the string names of the events.
Adding an event to this list will automatically add a method with that same name to your component!

You can import the `Events` enum from `gradio.events` to access commonly used events in the core gradio components.

For example, the following code will define `text_submit`, `file_upload` and `change` methods in the `MyComponent` class.

```python
from gradio.events import Events
from gradio.components import FormComponent

class MyComponent(FormComponent):

    EVENTS = [
        "text_submit",
        "file_upload",
        Events.change
    ]
```


Tip: Don't forget to also handle these events in the JavaScript code!

## Conclusion

---

<!-- Source: guides/09_gradio-clients-and-lite/04_gradio-and-llm-agents.md -->
# Gradio & LLM Agents ðŸ¤

Large Language Models (LLMs) are very impressive but they can be made even more powerful if we could give them skills to accomplish specialized tasks.

The [gradio_tools](https://github.com/freddyaboulton/gradio-tools) library can turn any [Gradio](https://github.com/gradio-app/gradio) application into a [tool](https://python.langchain.com/en/latest/modules/agents/tools.html) that an [agent](https://docs.langchain.com/docs/components/agents/agent) can use to complete its task. For example, an LLM could use a Gradio tool to transcribe a voice recording it finds online and then summarize it for you. Or it could use a different Gradio tool to apply OCR to a document on your Google Drive and then answer questions about it.

This guide will show how you can use `gradio_tools` to grant your LLM Agent access to the cutting edge Gradio applications hosted in the world. Although `gradio_tools` are compatible with more than one agent framework, we will focus on [Langchain Agents](https://docs.langchain.com/docs/components/agents/) in this guide.

## Some background

### What are agents?

A [LangChain agent](https://docs.langchain.com/docs/components/agents/agent) is a Large Language Model (LLM) that takes user input and reports an output based on using one of many tools at its disposal.

### What is Gradio?

[Gradio](https://github.com/gradio-app/gradio) is the defacto standard framework for building Machine Learning Web Applications and sharing them with the world - all with just python! ðŸ

## gradio_tools - An end-to-end example

To get started with `gradio_tools`, all you need to do is import and initialize your tools and pass them to the langchain agent!

In the following example, we import the `StableDiffusionPromptGeneratorTool` to create a good prompt for stable diffusion, the
`StableDiffusionTool` to create an image with our improved prompt, the `ImageCaptioningTool` to caption the generated image, and
the `TextToVideoTool` to create a video from a prompt.

We then tell our agent to create an image of a dog riding a skateboard, but to please improve our prompt ahead of time. We also ask
it to caption the generated image and create a video for it. The agent can decide which tool to use without us explicitly telling it.

```python
import os

if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEY must be set")

from langchain.agents import initialize_agent
from langchain.llms import OpenAI
from gradio_tools import (StableDiffusionTool, ImageCaptioningTool, StableDiffusionPromptGeneratorTool,
                          TextToVideoTool)

from langchain.memory import ConversationBufferMemory

llm = OpenAI(temperature=0)
memory = ConversationBufferMemory(memory_key="chat_history")
tools = [StableDiffusionTool().langchain, ImageCaptioningTool().langchain,
         StableDiffusionPromptGeneratorTool().langchain, TextToVideoTool().langchain]


agent = initialize_agent(tools, llm, memory=memory, agent="conversational-react-description", verbose=True)
output = agent.run(input=("Please create a photo of a dog riding a skateboard "
                          "but improve my prompt prior to using an image generator."
                          "Please caption the generated image and create a video for it using the improved prompt."))
```

You'll note that we are using some pre-built tools that come with `gradio_tools`. Please see this [doc](https://github.com/freddyaboulton/gradio-tools#gradio-tools-gradio--llm-agents) for a complete list of the tools that come with `gradio_tools`.
If you would like to use a tool that's not currently in `gradio_tools`, it is very easy to add your own. That's what the next section will cover.

## gradio_tools - creating your own tool

The core abstraction is the `GradioTool`, which lets you define a new tool for your LLM as long as you implement a standard interface:

```python
class GradioTool(BaseTool):

    def __init__(self, name: str, description: str, src: str) -> None:

    @abstractmethod
    def create_job(self, query: str) -> Job:
        pass

    @abstractmethod
    def postprocess(self, output: Tuple[Any] | Any) -> str:
        pass
```

The requirements are:

1. The name for your tool
2. The description for your tool. This is crucial! Agents decide which tool to use based on their description. Be precise and be sure to include example of what the input and the output of the tool should look like.
3. The url or space id, e.g. `freddyaboulton/calculator`, of the Gradio application. Based on this value, `gradio_tool` will create a [gradio client](https://github.com/gradio-app/gradio/blob/main/client/python/README.md) instance to query the upstream application via API. Be sure to click the link and learn more about the gradio client library if you are not familiar with it.
4. create_job - Given a string, this method should parse that string and return a job from the client. Most times, this is as simple as passing the string to the `submit` function of the client. More info on creating jobs [here](https://github.com/gradio-app/gradio/blob/main/client/python/README.md#making-a-prediction)
5. postprocess - Given the result of the job, convert it to a string the LLM can display to the user.
6. _Optional_ - Some libraries, e.g. [MiniChain](https://github.com/srush/MiniChain/tree/main), may need some info about the underlying gradio input and output types used by the tool. By default, this will return gr.Textbox() but
   if you'd like to provide more accurate info, implement the `_block_input(self, gr)` and `_block_output(self, gr)` methods of the tool. The `gr` variable is the gradio module (the result of `import gradio as gr`). It will be
   automatically imported by the `GradiTool` parent class and passed to the `_block_input` and `_block_output` methods.

And that's it!

Once you have created your tool, open a pull request to the `gradio_tools` repo! We welcome all contributions.

## Example tool - Stable Diffusion

Here is the code for the StableDiffusion tool as an example:

```python
from gradio_tool import GradioTool
import os

class StableDiffusionTool(GradioTool):
    """Tool for calling stable diffusion from llm"""

    def __init__(
        self,
        name="StableDiffusion",
        description=(
            "An image generator. Use this to generate images based on "
            "text input. Input should be a description of what the image should "
            "look like. The output will be a path to an image file."
        ),
        src="gradio-client-demos/stable-diffusion",
        token=None,
    ) -> None:
        super().__init__(name, description, src, token)

    def create_job(self, query: str) -> Job:
        return self.client.submit(query, "", 9, fn_index=1)

    def postprocess(self, output: str) -> str:
        return [os.path.join(output, i) for i in os.listdir(output) if not i.endswith("json")][0]

    def _block_input(self, gr) -> "gr.components.Component":
        return gr.Textbox()

    def _block_output(self, gr) -> "gr.components.Component":
        return gr.Image()
```

Some notes on this implementation:

1. All instances of `GradioTool` have an attribute called `client` that is a pointed to the underlying [gradio client](https://github.com/gradio-app/gradio/tree/main/client/python#gradio_client-use-a-gradio-app-as-an-api----in-3-lines-of-python). That is what you should use
   in the `create_job` method.
2. `create_job` just passes the query string to the `submit` function of the client with some other parameters hardcoded, i.e. the negative prompt string and the guidance scale. We could modify our tool to also accept these values from the input string in a subsequent version.
3. The `postprocess` method simply returns the first image from the gallery of images created by the stable diffusion space. We use the `os` module to get the full path of the image.

## Conclusion

You now know how to extend the abilities of your LLM with the 1000s of gradio spaces running in the wild!
Again, we welcome any contributions to the [gradio_tools](https://github.com/freddyaboulton/gradio-tools) library.
We're excited to see the tools you all build!

---

<!-- Source: guides/10_mcp/04_using-docs-mcp.md -->
# Using the Gradio Docs MCP Server

Tags: MCP, TOOL, LLM, SERVER, DOCS

In this guide, we will describe how to use the official Gradio Docs MCP Server.

### Prerequisites

You will need an LLM application that supports tool calling using the MCP protocol, such as Claude Desktop, Cursor, or Cline (these are known as "MCP Clients").

## Why an MCP Server?

If you're using LLMs in your workflow, adding this server will augment them with just the right context on gradio - which makes your experience a lot faster and smoother. 

<video src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/mcp-docs.mp4" style="width:100%" controls preload> </video>

The server is running on Spaces and was launched entirely using Gradio, you can see all the code [here](https://huggingface.co/spaces/gradio/docs-mcp). For more on building an mcp server with gradio, see the [previous guide](./building-an-mcp-client-with-gradio). 

## Installing in the Clients 

For clients that support streamable HTTP (e.g. Cursor, Windsurf, Cline), simply add the following configuration to your MCP config:

```json
{
  "mcpServers": {
    "gradio": {
      "url": "https://gradio-docs-mcp.hf.space/gradio_api/mcp/"
    }
  }
}
```

We've included step-by-step instructions for Cursor below, but you can consult the docs for Windsurf [here](https://docs.windsurf.com/windsurf/mcp), and Cline [here](https://docs.cline.bot/mcp-servers/configuring-mcp-servers) which are similar to set up. 



### Cursor 

1. Make sure you're using the latest version of Cursor, and go to Cursor > Settings > Cursor Settings > MCP 
2. Click on '+ Add new global MCP server' 
3. Copy paste this json into the file that opens and then save it. 
```json
{
  "mcpServers": {
    "gradio": {
      "url": "https://gradio-docs-mcp.hf.space/gradio_api/mcp/"
    }
  }
}
```
4. That's it! You should see the tools load and the status go green in the settings page. You may have to click the refresh icon or wait a few seconds. 

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/cursor-mcp.png)

### Claude Desktop

1. Since Claude Desktop only supports stdio, you will need to [install Node.js](https://nodejs.org/en/download/) to get this to work. 
2. Make sure you're using the latest version of Claude Desktop, and go to Claude > Settings > Developer > Edit Config 
3. Open the file with your favorite editor and copy paste this json, then save the file. 
```json
{
  "mcpServers": {
    "gradio": {
      "command": "npx",
      "args": [
        "mcp-remote",
        "https://gradio-docs-mcp.hf.space/gradio_api/mcp/"
      ]
    }
  }
}
```
4. Quit and re-open Claude Desktop, and you should be good to go. You should see it loaded in the Search and Tools icon or on the developer settings page. 
 
![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/claude-desktop-mcp.gif)

## Tools 

There are currently only two tools in the server: `gradio_docs_mcp_load_gradio_docs` and `gradio_docs_mcp_search_gradio_docs`. 

1. `gradio_docs_mcp_load_gradio_docs`: This tool takes no arguments and will load an /llms.txt style summary of Gradio's latest, full documentation. Very useful context the LLM can parse before answering questions or generating code. 

2. `gradio_docs_mcp_search_gradio_docs`: This tool takes a query as an argument and will run embedding search on Gradio's docs, guides, and demos to return the most useful context for the LLM to parse.

---

<!-- Source: guides/cn/02_building-interfaces/04_four-kinds-of-interfaces.md -->
# Gradio ç•Œé¢çš„ 4 ç§ç±»åž‹

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸€ç›´å‡è®¾æž„å»º Gradio æ¼”ç¤ºéœ€è¦åŒæ—¶å…·å¤‡è¾“å…¥å’Œè¾“å‡ºã€‚ä½†å¯¹äºŽæœºå™¨å­¦ä¹ æ¼”ç¤ºæ¥è¯´ï¼Œå¹¶ä¸æ€»æ˜¯å¦‚æ­¤ï¼šä¾‹å¦‚ï¼Œ*æ— æ¡ä»¶å›¾åƒç”Ÿæˆæ¨¡åž‹*ä¸éœ€è¦ä»»ä½•è¾“å…¥ï¼Œä½†ä¼šç”Ÿæˆä¸€å¼ å›¾åƒä½œä¸ºè¾“å‡ºã€‚

äº‹å®žè¯æ˜Žï¼Œ`gradio.Interface` ç±»å®žé™…ä¸Šå¯ä»¥å¤„ç† 4 ç§ä¸åŒç±»åž‹çš„æ¼”ç¤ºï¼š

1. **Standard demos æ ‡å‡†æ¼”ç¤º**ï¼šåŒæ—¶å…·æœ‰ç‹¬ç«‹çš„è¾“å…¥å’Œè¾“å‡ºï¼ˆä¾‹å¦‚å›¾åƒåˆ†ç±»å™¨æˆ–è¯­éŸ³è½¬æ–‡æœ¬æ¨¡åž‹ï¼‰
2. **Output-only demos ä»…è¾“å‡ºæ¼”ç¤º**ï¼šä¸æŽ¥å—ä»»ä½•è¾“å…¥ï¼Œä½†ä¼šäº§ç”Ÿè¾“å‡ºï¼ˆä¾‹å¦‚æ— æ¡ä»¶å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼‰
3. **Input-only demos ä»…è¾“å…¥æ¼”ç¤º**ï¼šä¸äº§ç”Ÿä»»ä½•è¾“å‡ºï¼Œä½†ä¼šæŽ¥å—æŸç§å½¢å¼çš„è¾“å…¥ï¼ˆä¾‹å¦‚ä¿å­˜æ‚¨ä¸Šä¼ åˆ°å¤–éƒ¨æŒä¹…æ•°æ®åº“çš„å›¾åƒçš„æ¼”ç¤ºï¼‰
4. **Unified demos ç»Ÿä¸€æ¼”ç¤º**ï¼šåŒæ—¶å…·æœ‰è¾“å…¥å’Œè¾“å‡ºç»„ä»¶ï¼Œä½†è¿™äº›ç»„ä»¶æ˜¯*ç›¸åŒçš„*ã€‚è¿™æ„å‘³ç€ç”Ÿæˆçš„è¾“å‡ºå°†è¦†ç›–è¾“å…¥ï¼ˆä¾‹å¦‚æ–‡æœ¬è‡ªåŠ¨å®Œæˆæ¨¡åž‹ï¼‰

æ ¹æ®æ¼”ç¤ºç±»åž‹çš„ä¸åŒï¼Œç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰ä¼šæœ‰ç•¥å¾®ä¸åŒçš„å¤–è§‚ï¼š

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/interfaces4.png)

æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨ `Interface` ç±»æž„å»ºæ¯ç§ç±»åž‹çš„æ¼”ç¤ºï¼Œä»¥åŠç¤ºä¾‹ï¼š

## æ ‡å‡†æ¼”ç¤º (Standard demos)

è¦åˆ›å»ºå…·æœ‰è¾“å…¥å’Œè¾“å‡ºç»„ä»¶çš„æ¼”ç¤ºï¼Œåªéœ€åœ¨ `Interface()` ä¸­è®¾ç½® `inputs` å’Œ `outputs` å‚æ•°çš„å€¼ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•å›¾åƒæ»¤é•œçš„ç¤ºä¾‹æ¼”ç¤ºï¼š

$code_sepia_filter
$demo_sepia_filter

## ä»…è¾“å‡ºæ¼”ç¤º (Output-only demos)

é‚£ä¹ˆä»…åŒ…å«è¾“å‡ºçš„æ¼”ç¤ºå‘¢ï¼Ÿä¸ºäº†æž„å»ºè¿™æ ·çš„æ¼”ç¤ºï¼Œåªéœ€å°† `Interface()` ä¸­çš„ `inputs` å‚æ•°å€¼è®¾ç½®ä¸º `None`ã€‚ä»¥ä¸‹æ˜¯æ¨¡æ‹Ÿå›¾åƒç”Ÿæˆæ¨¡åž‹çš„ç¤ºä¾‹æ¼”ç¤ºï¼š

$code_fake_gan_no_input
$demo_fake_gan_no_input

## ä»…è¾“å…¥æ¼”ç¤º (Input-only demos)

åŒæ ·åœ°ï¼Œè¦åˆ›å»ºä»…åŒ…å«è¾“å…¥çš„æ¼”ç¤ºï¼Œå°† `Interface()` ä¸­çš„ `outputs` å‚æ•°å€¼è®¾ç½®ä¸º `None`ã€‚ä»¥ä¸‹æ˜¯å°†ä»»ä½•ä¸Šä¼ çš„å›¾åƒä¿å­˜åˆ°ç£ç›˜çš„ç¤ºä¾‹æ¼”ç¤ºï¼š

$code_save_file_no_output
$demo_save_file_no_output

## ç»Ÿä¸€æ¼”ç¤º (Unified demos)

è¿™ç§æ¼”ç¤ºå°†å•ä¸ªç»„ä»¶åŒæ—¶ä½œä¸ºè¾“å…¥å’Œè¾“å‡ºã€‚åªéœ€å°† `Interface()` ä¸­çš„ `inputs` å’Œ `outputs` å‚æ•°å€¼è®¾ç½®ä¸ºç›¸åŒçš„ç»„ä»¶å³å¯åˆ›å»ºæ­¤æ¼”ç¤ºã€‚ä»¥ä¸‹æ˜¯æ–‡æœ¬ç”Ÿæˆæ¨¡åž‹çš„ç¤ºä¾‹æ¼”ç¤ºï¼š

$code_unified_demo_text_generation
$demo_unified_demo_text_generation

---

<!-- Source: guides/cn/03_building-with-blocks/04_custom-CSS-and-JS.md -->
# è‡ªå®šä¹‰çš„ JS å’Œ CSS

æœ¬æŒ‡å—ä»‹ç»äº†å¦‚ä½•æ›´çµæ´»åœ°ä¸º Blocks æ·»åŠ æ ·å¼ï¼Œå¹¶æ·»åŠ  JavaScript ä»£ç åˆ°äº‹ä»¶ç›‘å¬å™¨ä¸­ã€‚

**è­¦å‘Š**ï¼šåœ¨è‡ªå®šä¹‰çš„ JS å’Œ CSS ä¸­ä½¿ç”¨æŸ¥è¯¢é€‰æ‹©å™¨ä¸èƒ½ä¿è¯èƒ½åœ¨æ‰€æœ‰ Gradio ç‰ˆæœ¬ä¸­æ­£å¸¸å·¥ä½œï¼Œå› ä¸º Gradio çš„ HTML DOM å¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚æˆ‘ä»¬å»ºè®®è°¨æ…Žä½¿ç”¨æŸ¥è¯¢é€‰æ‹©å™¨ã€‚

## è‡ªå®šä¹‰çš„ CSS

Gradio ä¸»é¢˜æ˜¯è‡ªå®šä¹‰åº”ç”¨ç¨‹åºå¤–è§‚å’Œæ„Ÿè§‰çš„æœ€ç®€å•æ–¹å¼ã€‚æ‚¨å¯ä»¥ä»Žå„ç§ä¸»é¢˜ä¸­è¿›è¡Œé€‰æ‹©ï¼Œæˆ–è€…åˆ›å»ºè‡ªå·±çš„ä¸»é¢˜ã€‚è¦å®žçŽ°è¿™ä¸€ç‚¹ï¼Œè¯·å°† `theme=` kwarg ä¼ é€’ç»™ `Blocks` æž„é€ å‡½æ•°ã€‚ä¾‹å¦‚ï¼š

```python
with gr.Blocks(theme=gr.themes.Glass()):
    ...
```

Gradio è‡ªå¸¦ä¸€å¥—é¢„æž„å»ºçš„ä¸»é¢˜ï¼Œæ‚¨å¯ä»¥ä»Ž `gr.themes.*` ä¸­åŠ è½½è¿™äº›ä¸»é¢˜ã€‚æ‚¨å¯ä»¥æ‰©å±•è¿™äº›ä¸»é¢˜ï¼Œæˆ–è€…ä»Žå¤´å¼€å§‹åˆ›å»ºè‡ªå·±çš„ä¸»é¢˜ - æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ä¸»é¢˜æŒ‡å—](/theming-guide)ã€‚

è¦å¢žåŠ é™„åŠ çš„æ ·å¼èƒ½åŠ›ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `css=` kwarg å°†ä»»ä½• CSS ä¼ é€’ç»™æ‚¨çš„åº”ç”¨ç¨‹åºã€‚

Gradio åº”ç”¨ç¨‹åºçš„åŸºç±»æ˜¯ `gradio-container`ï¼Œå› æ­¤ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œç”¨äºŽæ›´æ”¹ Gradio åº”ç”¨ç¨‹åºçš„èƒŒæ™¯é¢œè‰²ï¼š

```python
with gr.Blocks(css=".gradio-container {background-color: red}") as demo:
    ...
```

å¦‚æžœæ‚¨æƒ³åœ¨æ‚¨çš„ CSS ä¸­å¼•ç”¨å¤–éƒ¨æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ `"file="` ä½œä¸ºæ–‡ä»¶è·¯å¾„çš„å‰ç¼€ï¼ˆå¯ä»¥æ˜¯ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ï¼‰ï¼Œä¾‹å¦‚ï¼š

```python
with gr.Blocks(css=".gradio-container {background: url('file=clouds.jpg')}") as demo:
    ...
```

æ‚¨è¿˜å¯ä»¥å°† CSS æ–‡ä»¶çš„æ–‡ä»¶è·¯å¾„ä¼ é€’ç»™ `css` å‚æ•°ã€‚

## `elem_id` å’Œ `elem_classes` å‚æ•°

æ‚¨å¯ä»¥ä½¿ç”¨ `elem_id` æ¥ä¸ºä»»ä½•ç»„ä»¶æ·»åŠ  HTML å…ƒç´  `id`ï¼Œå¹¶ä½¿ç”¨ `elem_classes` æ·»åŠ ä¸€ä¸ªç±»æˆ–ç±»åˆ—è¡¨ã€‚è¿™å°†ä½¿æ‚¨èƒ½å¤Ÿæ›´è½»æ¾åœ°ä½¿ç”¨ CSS é€‰æ‹©å…ƒç´ ã€‚è¿™ç§æ–¹æ³•æ›´æœ‰å¯èƒ½åœ¨ Gradio ç‰ˆæœ¬ä¹‹é—´ä¿æŒç¨³å®šï¼Œå› ä¸ºå†…ç½®çš„ç±»åæˆ– id å¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼ˆä½†æ­£å¦‚ä¸Šé¢çš„è­¦å‘Šä¸­æ‰€æåˆ°çš„ï¼Œå¦‚æžœæ‚¨ä½¿ç”¨è‡ªå®šä¹‰ CSSï¼Œæˆ‘ä»¬ä¸èƒ½ä¿è¯åœ¨ Gradio ç‰ˆæœ¬ä¹‹é—´å®Œå…¨å…¼å®¹ï¼Œå› ä¸º DOM å…ƒç´ æœ¬èº«å¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼‰ã€‚

```python
css = """
#warning {background-color: #FFCCCB}
.feedback textarea {font-size: 24px !important}
"""

with gr.Blocks(css=css) as demo:
    box1 = gr.Textbox(value="Good Job", elem_classes="feedback")
    box2 = gr.Textbox(value="Failure", elem_id="warning", elem_classes="feedback")
```

CSS `#warning` è§„åˆ™é›†ä»…é’ˆå¯¹ç¬¬äºŒä¸ªæ–‡æœ¬æ¡†ï¼Œè€Œ `.feedback` è§„åˆ™é›†å°†åŒæ—¶ä½œç”¨äºŽä¸¤ä¸ªæ–‡æœ¬æ¡†ã€‚è¯·æ³¨æ„ï¼Œåœ¨é’ˆå¯¹ç±»æ—¶ï¼Œæ‚¨å¯èƒ½éœ€è¦ä½¿ç”¨ `!important` é€‰æ‹©å™¨æ¥è¦†ç›–é»˜è®¤çš„ Gradio æ ·å¼ã€‚

## è‡ªå®šä¹‰çš„ JS

äº‹ä»¶ç›‘å¬å™¨å…·æœ‰ `_js` å‚æ•°ï¼Œå¯ä»¥æŽ¥å— JavaScript å‡½æ•°ä½œä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶åƒ Python äº‹ä»¶ç›‘å¬å™¨å‡½æ•°ä¸€æ ·å¤„ç†å®ƒã€‚æ‚¨å¯ä»¥ä¼ é€’ JavaScript å‡½æ•°å’Œ Python å‡½æ•°ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…ˆè¿è¡Œ JavaScript å‡½æ•°ï¼‰ï¼Œæˆ–è€…ä»…ä¼ é€’ JavaScriptï¼ˆå¹¶å°† Python çš„ `fn` è®¾ç½®ä¸º `None`ï¼‰ã€‚è¯·æŸ¥çœ‹ä¸‹é¢çš„ä»£ç ï¼š

$code_blocks_js_methods
$demo_blocks_js_methods

---

<!-- Source: guides/02_building-interfaces/05_four-kinds-of-interfaces.md -->
# The 4 Kinds of Gradio Interfaces

So far, we've always assumed that in order to build an Gradio demo, you need both inputs and outputs. But this isn't always the case for machine learning demos: for example, _unconditional image generation models_ don't take any input but produce an image as the output.

It turns out that the `gradio.Interface` class can actually handle 4 different kinds of demos:

1. **Standard demos**: which have both separate inputs and outputs (e.g. an image classifier or speech-to-text model)
2. **Output-only demos**: which don't take any input but produce on output (e.g. an unconditional image generation model)
3. **Input-only demos**: which don't produce any output but do take in some sort of input (e.g. a demo that saves images that you upload to a persistent external database)
4. **Unified demos**: which have both input and output components, but the input and output components _are the same_. This means that the output produced overrides the input (e.g. a text autocomplete model)

Depending on the kind of demo, the user interface (UI) looks slightly different:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/interfaces4.png)

Let's see how to build each kind of demo using the `Interface` class, along with examples:

## Standard demos

To create a demo that has both the input and the output components, you simply need to set the values of the `inputs` and `outputs` parameter in `Interface()`. Here's an example demo of a simple image filter:

$code_sepia_filter
$demo_sepia_filter

## Output-only demos

What about demos that only contain outputs? In order to build such a demo, you simply set the value of the `inputs` parameter in `Interface()` to `None`. Here's an example demo of a mock image generation model:

$code_fake_gan_no_input
$demo_fake_gan_no_input

## Input-only demos

Similarly, to create a demo that only contains inputs, set the value of `outputs` parameter in `Interface()` to be `None`. Here's an example demo that saves any uploaded image to disk:

$code_save_file_no_output
$demo_save_file_no_output

## Unified demos

A demo that has a single component as both the input and the output. It can simply be created by setting the values of the `inputs` and `outputs` parameter as the same component. Here's an example demo of a text generation model:

$code_unified_demo_text_generation
$demo_unified_demo_text_generation

It may be the case that none of the 4 cases fulfill your exact needs. In this case, you need to use the `gr.Blocks()` approach!

---

<!-- Source: guides/04_additional-features/05_progress-bars.md -->
# Progress Bars

Gradio supports the ability to create custom Progress Bars so that you have customizability and control over the progress update that you show to the user. In order to enable this, simply add an argument to your method that has a default value of a `gr.Progress` instance. Then you can update the progress levels by calling this instance directly with a float between 0 and 1, or using the `tqdm()` method of the `Progress` instance to track progress over an iterable, as shown below.

$code_progress_simple
$demo_progress_simple

If you use the `tqdm` library, you can even report progress updates automatically from any `tqdm.tqdm` that already exists within your function by setting the default argument as `gr.Progress(track_tqdm=True)`!

---

<!-- Source: guides/05_chatbots/05_chatbot-specific-events.md -->
# Chatbot-Specific Events

Tags: LLM, CHAT

Users expect modern chatbot UIs to let them easily interact with individual chat messages: for example, users might want to retry message generations, undo messages, or click on a like/dislike button to upvote or downvote a generated message.

Thankfully, the Gradio Chatbot exposes several events, such as `.retry`, `.undo`, `.like`, and `.clear`, to let you build this functionality into your application. As an application developer, you can attach functions to any of these event, allowing you to run arbitrary Python functions e.g. when a user interacts with a message.

In this demo, we'll build a UI that implements these events. You can see our finished demo deployed on Hugging Face spaces here:

$demo_chatbot_retry_undo_like

Tip: `gr.ChatInterface` automatically uses the `retry` and `.undo` events so it's best to start there in order get a fully working application quickly.


## The UI

First, we'll build the UI without handling these events and build from there. 
We'll use the Hugging Face InferenceClient in order to get started without setting up
any API keys.

This is what the first draft of our application looks like:

```python
from huggingface_hub import InferenceClient
import gradio as gr

client = InferenceClient()

def respond(
    prompt: str,
    history,
):
    if not history:
        history = [{"role": "system", "content": "You are a friendly chatbot"}]
    history.append({"role": "user", "content": prompt})

    yield history

    response = {"role": "assistant", "content": ""}
    for message in client.chat_completion( # type: ignore
        history,
        temperature=0.95,
        top_p=0.9,
        max_tokens=512,
        stream=True,
        model="openai/gpt-oss-20b"
    ):
        response["content"] += message.choices[0].delta.content or "" if message.choices else ""
        yield history + [response]


with gr.Blocks() as demo:
    gr.Markdown("# Chat with GPT-OSS 20b ðŸ¤—")
    chatbot = gr.Chatbot(
        label="Agent",
        avatar_images=(
            None,
            "https://em-content.zobj.net/source/twitter/376/hugging-face_1f917.png",
        ),
    )
    prompt = gr.Textbox(max_lines=1, label="Chat Message")
    prompt.submit(respond, [prompt, chatbot], [chatbot])
    prompt.submit(lambda: "", None, [prompt])

if __name__ == "__main__":
    demo.launch()
```

## The Undo Event

Our undo event will populate the textbox with the previous user message and also remove all subsequent assistant responses.

In order to know the index of the last user message, we can pass `gr.UndoData` to our event handler function like so:

```python
def handle_undo(history, undo_data: gr.UndoData):
    return history[:undo_data.index], history[undo_data.index]['content'][0]["text"]
```

We then pass this function to the `undo` event!

```python
    chatbot.undo(handle_undo, chatbot, [chatbot, prompt])
```

You'll notice that every bot response will now have an "undo icon" you can use to undo the response - 

![undo_event](https://github.com/user-attachments/assets/180b5302-bc4a-4c3e-903c-f14ec2adcaa6)

Tip: You can also access the content of the user message with `undo_data.value`

## The Retry Event

The retry event will work similarly. We'll use `gr.RetryData` to get the index of the previous user message and remove all the subsequent messages from the history. Then we'll use the `respond` function to generate a new response. We could also get the previous prompt via the `value` property of `gr.RetryData`.

```python
def handle_retry(history, retry_data: gr.RetryData):
    new_history = history[:retry_data.index]
    previous_prompt = history[retry_data.index]['content'][0]["text"]
    yield from respond(previous_prompt, new_history)
...

chatbot.retry(handle_retry, chatbot, chatbot)
```

You'll see that the bot messages have a "retry" icon now -

![retry_event](https://github.com/user-attachments/assets/cec386a7-c4cd-4fb3-a2d7-78fd806ceac6)

Tip: The Hugging Face inference API caches responses, so in this demo, the retry button will not generate a new response.

## The Like Event

By now you should hopefully be seeing the pattern!
To let users like a message, we'll add a `.like` event to our chatbot.
We'll pass it a function that accepts a `gr.LikeData` object.
In this case, we'll just print the message that was either liked or disliked.

```python
def handle_like(data: gr.LikeData):
    if data.liked:
        print("You upvoted this response: ", data.value)
    else:
        print("You downvoted this response: ", data.value)

chatbot.like(handle_like, None, None)
```

## The Edit Event

Same idea with the edit listener! with `gr.Chatbot(editable=True)`, you can capture user edits. The `gr.EditData` object tells us the index of the message edited and the new text of the mssage. Below, we use this object to edit the history, and delete any subsequent messages. 

```python
def handle_edit(history, edit_data: gr.EditData):
    new_history = history[:edit_data.index]
    new_history[-1]['content'] = [{"text": edit_data.value, "type": "text"}]
    return new_history

...

chatbot.edit(handle_edit, chatbot, chatbot)
```

## The Clear Event

As a bonus, we'll also cover the `.clear()` event, which is triggered when the user clicks the clear icon to clear all messages. As a developer, you can attach additional events that should happen when this icon is clicked, e.g. to handle clearing of additional chatbot state:

```python
from uuid import uuid4
import gradio as gr


def clear():
    print("Cleared uuid")
    return uuid4()


def chat_fn(user_input, history, uuid):
    return f"{user_input} with uuid {uuid}"


with gr.Blocks() as demo:
    uuid_state = gr.State(
        uuid4
    )
    chatbot = gr.Chatbot()
    chatbot.clear(clear, outputs=[uuid_state])

    gr.ChatInterface(
        chat_fn,
        additional_inputs=[uuid_state],
        chatbot=chatbot,
    )

demo.launch()
```

In this example, the `clear` function, bound to the `chatbot.clear` event, returns a new UUID into our session state, when the chat history is cleared via the trash icon. This can be seen in the `chat_fn` function, which references the UUID saved in our session state.

This example also shows that you can use these events with `gr.ChatInterface` by passing in a custom `gr.Chatbot` object.

## Conclusion

That's it! You now know how you can implement the retry, undo, like, and clear events for the Chatbot.

---

<!-- Source: guides/07_streaming/05_real-time-speech-recognition.md -->
# Real Time Speech Recognition

Tags: ASR, SPEECH, STREAMING

## Introduction

Automatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).

Using `gradio`, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.

This tutorial will show how to take a pretrained speech-to-text model and deploy it with a Gradio interface. We will start with a **_full-context_** model, in which the user speaks the entire audio before the prediction runs. Then we will adapt the demo to make it **_streaming_**, meaning that the audio model will convert speech as you speak. 

### Prerequisites

Make sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:

- Transformers (for this, `pip install torch transformers torchaudio`)

Make sure you have at least one of these installed so that you can follow along the tutorial. You will also need `ffmpeg` [installed on your system](https://www.ffmpeg.org/download.html), if you do not already have it, to process files from the microphone.

Here's how to build a real time speech recognition (ASR) app:

1. [Set up the Transformers ASR Model](#1-set-up-the-transformers-asr-model)
2. [Create a Full-Context ASR Demo with Transformers](#2-create-a-full-context-asr-demo-with-transformers)
3. [Create a Streaming ASR Demo with Transformers](#3-create-a-streaming-asr-demo-with-transformers)

## 1. Set up the Transformers ASR Model

First, you will need to have an ASR model that you have either trained yourself or you will need to download a pretrained model. In this tutorial, we will start by using a pretrained ASR model from the model, `whisper`.

Here is the code to load `whisper` from Hugging Face `transformers`.

```python
from transformers import pipeline

p = pipeline("automatic-speech-recognition", model="openai/whisper-base.en")
```

That's it!

## 2. Create a Full-Context ASR Demo with Transformers

We will start by creating a _full-context_ ASR demo, in which the user speaks the full audio before using the ASR model to run inference. This is very easy with Gradio -- we simply create a function around the `pipeline` object above.

We will use `gradio`'s built in `Audio` component, configured to take input from the user's microphone and return a filepath for the recorded audio. The output component will be a plain `Textbox`.

$code_asr
$demo_asr

The `transcribe` function takes a single parameter, `audio`, which is a numpy array of the audio the user recorded. The `pipeline` object expects this in float32 format, so we convert it first to float32, and then extract the transcribed text.

## 3. Create a Streaming ASR Demo with Transformers

To make this a *streaming* demo, we need to make these changes:

1. Set `streaming=True` in the `Audio` component
2. Set `live=True` in the `Interface`
3. Add a `state` to the interface to store the recorded audio of a user

Tip: You can also set `time_limit` and `stream_every` parameters in the interface. The `time_limit` caps the amount of time each user's stream can take. The default is 30 seconds so users won't be able to stream audio for more than 30 seconds. The `stream_every` parameter controls how frequently data is sent to your function. By default it is 0.5 seconds.

Take a look below.

$code_stream_asr

Notice that we now have a state variable because we need to track all the audio history. `transcribe` gets called whenever there is a new small chunk of audio, but we also need to keep track of all the audio spoken so far in the state. As the interface runs, the `transcribe` function gets called, with a record of all the previously spoken audio in the `stream` and the new chunk of audio as `new_chunk`. We return the new full audio to be stored back in its current state, and we also return the transcription. Here, we naively append the audio together and call the `transcriber` object on the entire audio. You can imagine more efficient ways of handling this, such as re-processing only the last 5 seconds of audio whenever a new chunk of audio is received. 

$demo_stream_asr

Now the ASR model will run inference as you speak!

---

<!-- Source: guides/08_custom-components/05_frontend.md -->
# The Frontend ðŸŒâ­ï¸

This guide will cover everything you need to know to implement your custom component's frontend.

Tip: Gradio components use Svelte. Writing Svelte is fun! If you're not familiar with it, we recommend checking out their interactive [guide](https://learn.svelte.dev/tutorial/welcome-to-svelte).

## The directory structure 

The frontend code should have, at minimum, three files:

* `Index.svelte`: This is the main export and where your component's layout and logic should live.
* `Example.svelte`: This is where the example view of the component is defined.

Feel free to add additional files and subdirectories. 
If you want to export any additional modules, remember to modify the `package.json` file

```json
"exports": {
    ".": "./Index.svelte",
    "./example": "./Example.svelte",
    "./package.json": "./package.json"
},
```

## The Index.svelte file

Your component should expose the following props that will be passed down from the parent Gradio application.

```typescript
import type { LoadingStatus } from "@gradio/statustracker";
import type { Gradio } from "@gradio/utils";

export let gradio: Gradio<{
    event_1: never;
    event_2: never;
}>;

export let elem_id = "";
export let elem_classes: string[] = [];
export let scale: number | null = null;
export let min_width: number | undefined = undefined;
export let loading_status: LoadingStatus | undefined = undefined;
export let mode: "static" | "interactive";
```

* `elem_id` and `elem_classes` allow Gradio app developers to target your component with custom CSS and JavaScript from the Python `Blocks` class.

* `scale` and `min_width` allow Gradio app developers to control how much space your component takes up in the UI.

* `loading_status` is used to display a loading status over the component when it is the output of an event.

* `mode` is how the parent Gradio app tells your component whether the `interactive` or `static` version should be displayed.

* `gradio`: The `gradio` object is created by the parent Gradio app. It stores some application-level configuration that will be useful in your component, like internationalization. You must use it to dispatch events from your component.

A minimal `Index.svelte` file would look like:

```svelte
<script lang="ts">
	import type { LoadingStatus } from "@gradio/statustracker";
    import { Block } from "@gradio/atoms";
	import { StatusTracker } from "@gradio/statustracker";
	import type { Gradio } from "@gradio/utils";

	export let gradio: Gradio<{
		event_1: never;
		event_2: never;
	}>;

    export let value = "";
	export let elem_id = "";
	export let elem_classes: string[] = [];
	export let scale: number | null = null;
	export let min_width: number | undefined = undefined;
	export let loading_status: LoadingStatus | undefined = undefined;
    export let mode: "static" | "interactive";
</script>

<Block
	visible={true}
	{elem_id}
	{elem_classes}
	{scale}
	{min_width}
	allow_overflow={false}
	padding={true}
>
	{#if loading_status}
		<StatusTracker
			autoscroll={gradio.autoscroll}
			i18n={gradio.i18n}
			{...loading_status}
		/>
	{/if}
    <p>{value}</p>
</Block>
```

## The Example.svelte file

The `Example.svelte` file should expose the following props:

```typescript
    export let value: string;
    export let type: "gallery" | "table";
    export let selected = false;
    export let index: number;
```

* `value`: The example value that should be displayed.

* `type`: This is a variable that can be either `"gallery"` or `"table"` depending on how the examples are displayed. The `"gallery"` form is used when the examples correspond to a single input component, while the `"table"` form is used when a user has multiple input components, and the examples need to populate all of them. 

* `selected`: You can also adjust how the examples are displayed if a user "selects" a particular example by using the selected variable.

* `index`: The current index of the selected value.

* Any additional props your "non-example" component takes!

This is the `Example.svelte` file for the code `Radio` component:

```svelte
<script lang="ts">
	export let value: string;
	export let type: "gallery" | "table";
	export let selected = false;
</script>

<div
	class:table={type === "table"}
	class:gallery={type === "gallery"}
	class:selected
>
	{value}
</div>

<style>
	.gallery {
		padding: var(--size-1) var(--size-2);
	}
</style>
```

## Handling Files

If your component deals with files, these files **should** be uploaded to the backend server. 
The `@gradio/client` npm package provides the `upload` and `prepare_files` utility functions to help you do this.

The `prepare_files` function will convert the browser's `File` datatype to gradio's internal `FileData` type.
You should use the `FileData` data in your component to keep track of uploaded files.

The `upload` function will upload an array of `FileData` values to the server.

Here's an example of loading files from an `<input>` element when its value changes.


```svelte
<script lang="ts">
    import { upload, prepare_files, type FileData } from "@gradio/client";
    export let root;
    export let value;
    let uploaded_files;

    async function handle_upload(file_data: FileData[]): Promise<void> {
        await tick();
        uploaded_files = await upload(file_data, root);
    }

    async function loadFiles(files: FileList): Promise<void> {
        let _files: File[] = Array.from(files);
        if (!files.length) {
            return;
        }
        if (file_count === "single") {
            _files = [files[0]];
        }
        let file_data = await prepare_files(_files);
        await handle_upload(file_data);
    }

    async function loadFilesFromUpload(e: Event): Promise<void> {
		const target = e.target;

		if (!target.files) return;
		await loadFiles(target.files);
	}
</script>

<input
    type="file"
    on:change={loadFilesFromUpload}
    multiple={true}
/>
```

The component exposes a prop named `root`. 
This is passed down by the parent gradio app and it represents the base url that the files will be uploaded to and fetched from.

For WASM support, you should get the upload function from the `Context` and pass that as the third parameter of the `upload` function.

```typescript
<script lang="ts">
    import { getContext } from "svelte";
    const upload_fn = getContext<typeof upload_files>("upload_files");

    async function handle_upload(file_data: FileData[]): Promise<void> {
        await tick();
        await upload(file_data, root, upload_fn);
    }
</script>
```

## Leveraging Existing Gradio Components

Most of Gradio's frontend components are published on [npm](https://www.npmjs.com/), the javascript package repository.
This means that you can use them to save yourself time while incorporating common patterns in your component, like uploading files.
For example, the `@gradio/upload` package has `Upload` and `ModifyUpload` components for properly uploading files to the Gradio server. 
Here is how you can use them to create a user interface to upload and display PDF files.

```svelte
<script>
	import { type FileData, Upload, ModifyUpload } from "@gradio/upload";
	import { Empty, UploadText, BlockLabel } from "@gradio/atoms";
</script>

<BlockLabel Icon={File} label={label || "PDF"} />
{#if value === null && interactive}
    <Upload
        filetype="application/pdf"
        on:load={handle_load}
        {root}
        >
        <UploadText type="file" i18n={gradio.i18n} />
    </Upload>
{:else if value !== null}
    {#if interactive}
        <ModifyUpload i18n={gradio.i18n} on:clear={handle_clear}/>
    {/if}
    <iframe title={value.orig_name || "PDF"} src={value.data} height="{height}px" width="100%"></iframe>
{:else}
    <Empty size="large"> <File/> </Empty>	
{/if}
```

You can also combine existing Gradio components to create entirely unique experiences.
Like rendering a gallery of chatbot conversations. 
The possibilities are endless, please read the documentation on our javascript packages [here](https://gradio.app/main/docs/js).
We'll be adding more packages and documentation over the coming weeks!

## Matching Gradio Core's Design System

You can explore our component library via Storybook. You'll be able to interact with our components and see them in their various states.

For those interested in design customization, we provide the CSS variables consisting of our color palette, radii, spacing, and the icons we use - so you can easily match up your custom component with the style of our core components. This Storybook will be regularly updated with any new additions or changes.

[Storybook Link](https://gradio.app/main/docs/js/storybook)

## Custom configuration

If you want to make use of the vast vite ecosystem, you can use the `gradio.config.js` file to configure your component's build process. This allows you to make use of tools like tailwindcss, mdsvex, and more.

Currently, it is possible to configure the following:

Vite options:
- `plugins`: A list of vite plugins to use.

Svelte options:
- `preprocess`: A list of svelte preprocessors to use.
- `extensions`: A list of file extensions to compile to `.svelte` files.
- `build.target`: The target to build for, this may be necessary to support newer javascript features. See the [esbuild docs](https://esbuild.github.io/api/#target) for more information.

The `gradio.config.js` file should be placed in the root of your component's `frontend` directory. A default config file is created for you when you create a new component. But you can also create your own config file, if one doesn't exist, and use it to customize your component's build process.

### Example for a Vite plugin

Custom components can use Vite plugins to customize the build process. Check out the [Vite Docs](https://vitejs.dev/guide/using-plugins.html) for more information. 

Here we configure [TailwindCSS](https://tailwindcss.com), a utility-first CSS framework. Setup is easiest using the version 4 prerelease. 

```
npm install tailwindcss@next @tailwindcss/vite@next
```

In `gradio.config.js`:

```typescript
import tailwindcss from "@tailwindcss/vite";
export default {
    plugins: [tailwindcss()]
};
```

Then create a `style.css` file with the following content:

```css
@import "tailwindcss";
```

Import this file into `Index.svelte`. Note, that you need to import the css file containing `@import` and cannot just use a `<style>` tag and use `@import` there. 

```svelte
<script lang="ts">
[...]
import "./style.css";
[...]
</script>
```

### Example for Svelte options

In `gradio.config.js` you can also specify a some Svelte options to apply to the Svelte compilation. In this example we will add support for [`mdsvex`](https://mdsvex.pngwn.io), a Markdown preprocessor for Svelte. 

In order to do this we will need to add a [Svelte Preprocessor](https://svelte.dev/docs/svelte-compiler#preprocess) to the `svelte` object in `gradio.config.js` and configure the [`extensions`](https://github.com/sveltejs/vite-plugin-svelte/blob/HEAD/docs/config.md#config-file) field. Other options are not currently supported.

First, install the `mdsvex` plugin:

```bash
npm install mdsvex
```

Then add the following to `gradio.config.js`:

```typescript
import { mdsvex } from "mdsvex";

export default {
    svelte: {
        preprocess: [
            mdsvex()
        ],
        extensions: [".svelte", ".svx"]
    }
};
```

Now we can create `mdsvex` documents in our component's `frontend` directory and they will be compiled to `.svelte` files.

```md
<!-- HelloWorld.svx -->

<script lang="ts">
    import { Block } from "@gradio/atoms";

    export let title = "Hello World";
</script>

<Block label="Hello World">

# {title}

This is a markdown file.

</Block>
```

We can then use the `HelloWorld.svx` file in our components:

```svelte
<script lang="ts">
    import HelloWorld from "./HelloWorld.svx";
</script>

<HelloWorld />
```

## Conclusion

You now know how to create delightful frontends for your components!

---

<!-- Source: guides/10_mcp/05_building-chatgpt-apps-with-gradio.md -->
# Building ChatGPT Apps with Gradio and Apps SDK

[Apps in ChatGPT](https://openai.com/index/introducing-apps-in-chatgpt/) are a great way to let users try your machine learning models or other kinds of apps entirely by chatting in familiar chat application. OpenAI has released the [Apps SDK](https://developers.openai.com/apps-sdk/quickstart) for developers to build complete applications, but you can use Gradio to build ChatGPT apps very quickly, based off of your Gradio MCP server. We will also see how Gradio's built-in [share links](https://www.gradio.app/guides/sharing-your-app#sharing-demos) make it especially easy to iterate on your ChatGPT app!

### Introduction

Building a ChatGPT app requires doing two things:

* Building a Gradio MCP server with at least one tool exposed. If you're not already familiar with building a Gradio MCP server, we recommend reading [this guide first](https://www.gradio.app/guides/building-mcp-server-with-gradio).

* Building a custom UI with HTML, JavaScript, and CSS that will be displayed when your tool is called, an exposing that as an MCP resource. 

We will walk through the steps in more detail below.

### Prerequisites

* You will need to enable "developer mode" in ChatGPT under Settings â†’ Apps & Connectors â†’ Advanced settings in ChatGPT. This currently requires a paid ChatGPT account.
* You need to have `gradio>=6.0` installed with the `mcp` add-on:

```bash
pip install --upgrade gradio[mcp]
```

Now, let's walk through two examples of how you can build build ChatGPT apps with Gradio. 

### Example 1: Letter Counter App

The first example is an ChatGPT app that counts the occurrence of letters in a word and displays a card with the word and specified letters highlighted, like this:

<video src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/letter-counter-app-recording.mp4" controls></video>


So how do we build this? You can find the complete code for the letter counter app [in a single file here](https://github.com/gradio-app/gradio/blob/main/demo/mcp_letter_counter_app/run.py), or follow the steps below:

1. Start by writing your Python function. In our case, the function is simply a letter counter:

```py
def letter_counter(word: str, letter: str) -> int:
    """
    Count the number of letters in a word or phrase.

    Parameters:
        word (str): The word or phrase to count the letters of.
        letter (str): The letter to count the occurrences of.
    """
    return word.count(letter)
```

2. Then, wrap your Python function with a Gradio UI, something along these lines:

```py
with gr.Blocks() as demo:
    with gr.Row():
        with gr.Column():
            word = gr.Textbox(label="Word")
            letter = gr.Textbox(label="Letter")
            btn = gr.Button("Count Letters")
        with gr.Column():
            count = gr.Number(label="Count")

    btn.click(letter_counter, inputs=[word, letter], outputs=count)
```

3. Now, launch your Gradio app with the MCP server enabled, i.e. with `mcp_server=True`

```py
    demo.launch(mcp_server=True)
```

As covered in [earlier guides](https://www.gradio.app/guides/building-mcp-server-with-gradio), you will now be able to test the tool using any MCP Client, such as the MCP Inspector tool. Test it and confirm that it behaves as you expect.

4. Create a UI for your ChatGPT app and expose it as a resource. This part requires writing some frontend code and may be unfamiliar at first, but a few examples will help you create an app that works well for your use case. In our case, we'll create a card with HTML, Javascript, and CSS. Inside the card, we'll display the word presented by the user, highlighting each occurrence of the specified letter. Note that we access the user's tool input using `window.openai?.toolInput?.word` and `window.openai?.toolInput?.letter`. The `window.openai` object is automatically inserted by ChatGPT with the data from the user's tool call. This is what the complete function looks like:

```py
@gr.mcp.resource("ui://widget/app.html", mime_type="text/html+skybridge")
def app_html():
    visual = """
    <div id="letter-card-container"></div>
    <script>
        const container = document.getElementById('letter-card-container');

        function render() {
            const word = window.openai?.toolInput?.word || "strawberry";
            const letter = window.openai?.toolInput?.letter || "r";

            let letterHTML = '';
            for (let i = 0; i < word.length; i++) {
                const char = word[i];
                const color = char.toLowerCase() === letter.toLowerCase() ? '#b8860b' : '#000000';
                letterHTML += `<span style="color: ${color};">${char}</span>`;
            }

            container.innerHTML = `
                <div style="
                    background: linear-gradient(135deg, #f5f5dc 0%, #e8e4d0 100%);
                    background-image:
                        repeating-linear-gradient(45deg, transparent, transparent 2px, rgba(139, 121, 94, 0.03) 2px, rgba(139, 121, 94, 0.03) 4px),
                        linear-gradient(135deg, #f5f5dc 0%, #e8e4d0 100%);
                    border-radius: 16px;
                    padding: 40px;
                    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1), 0 1px 3px rgba(0, 0, 0, 0.08);
                    max-width: 600px;
                    margin: 20px auto;
                    font-family: 'Georgia', serif;
                    text-align: center;
                ">
                    <div style="
                        font-size: 48px;
                        font-weight: bold;
                        letter-spacing: 8px;
                        line-height: 1.5;
                    ">
                        ${letterHTML}
                    </div>
                </div>
            `;
        }
        render();
        window.addEventListener("openai:set_globals", (event) => {
            if (event.detail?.globals?.toolInput) {
                render();
            }
        }, { passive: true });
    </script>
    """
    return visual
```

Note that we've provided a URI for the `gr.mcp.resource` at `ui://widget/app.html`. This is arbitrary, but we'll need to use the same URI later on. We also need to specify the mimetype of the resource to be `mime_type="text/html+skybridge"`. Finally, note that we attached an event listener in the JavaScript for "openai:set_globals", which is generally a good practice as it allows the widget to update whenever a new tool call is triggered. 

5. Create an event in your Gradio app corresponding to the resource function. This is necessary because your Gradio app only picks up MCP tools, resources, prompts, etc. if they are associated with a Gradio event. Typically, the convention is to simply display the code for your MCP resource in a `gr.Code` component, e.g. like this:

```py
    html = gr.Code(language="html", max_lines=20)
    
    # ... the rest of your Gradio app

    btn.click(app_html, outputs=html)
```

6. Add `_meta` attributes to your MCP tool. We need to connect the MCP tool that we created to the UI that we created for our app. We can do this by adding this decorator to our MCP tool function:

```py
@gr.mcp.tool(
    _meta={
        "openai/outputTemplate": "ui://widget/app.html",
        "openai/resultCanProduceWidget": True,
        "openai/widgetAccessible": True,
    }
)
```

The key thing to observe is that the `"openai/outputTemplate"` must match the URI of the MCP resource that we created earlier.

7. Relaunch your Gradio app with `share=True`. This will make it very easy to test within ChatGPT. Note the MCP server URL that is printed to your terminal, e.g. `https://2e879c6066d729b11b.gradio.live/gradio_api/mcp/`.

```py
    demo.launch(share=True, mcp_server=True)
```

This will print a public URL that your Gradio app will be running on.

8. Now, navigate to ChatGPT (https://chat.com/). As mentioned earlier, you need to enable "developer mode" in ChatGPT under Settings â†’ Apps & Connectors â†’ Advanced settings in ChatGPT. Then, navigate to Settings â†’ Apps & Connectors and click the "Create" button. Give your connector a name, a description (optional), and paste in the MCP server URL that was printed to your terminal. Choose "No authentication" and create.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/letter-counter-setup.png)

And that's it! Once the Connector has been created, you can start prompting it by saying something like, "Use @letter-counter to count the number of r's in Gradio."

### Example 2: An Image Brightener

Next, let's see a more complex ChatGPT app for image enhancement. The ChatGPT app includes a "Brighten" button that lets the user call the tool directly from the app UI.

<video src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/mcp-image-app.mp4" controls></video>

Here's the complete code for this app:

$code_mcp_image_app

We won't break down the code in as much detail since many of the pieces are the same. But note the following differences from the earlier example:

* Calling tools from the widget: The app uses `window.openai.callTool()` to invoke the MCP tool directly from a button click, without requiring ChatGPT to call it:

```javascript
const result = await window.openai.callTool('power_law_image', {
    input_path: imageEl.src
});
```

* Parsing tool call results: The result from `callTool()` contains a `content` array that needs to be parsed to extract data:

```javascript
function extractImageUrl(data) {
    if (data?.content) {
        for (const item of data.content) {
            if (item.type === 'text' && item.text?.startsWith('Image URL: ')) {
                return item.text.substring('Image URL: '.length).trim();
            }
        }
    }
}
```

* Updating UI based on tool results: After calling the tool, the app immediately updates the displayed image with the new result:

```javascript
const newUrl = extractImageUrl(result);
if (newUrl) imageEl.src = newUrl;
```

With these examples, you've seen how to build both simple reactive widgets and more advanced interactive apps that can call tools directly from the UI. By combining Gradio's MCP server capabilities with the OpenAI Apps SDK, it's time to start create richer ChatGPT integrations that enhance the conversational experience with custom visualizations and user interactions!

---

<!-- Source: guides/cn/03_building-with-blocks/05_using-blocks-like-functions.md -->
# ä½¿ç”¨ Gradio å—åƒå‡½æ•°ä¸€æ ·

Tags: TRANSLATION, HUB, SPACES

**å…ˆå†³æ¡ä»¶**: æœ¬æŒ‡å—æ˜¯åœ¨å—ä»‹ç»çš„åŸºç¡€ä¸Šæž„å»ºçš„ã€‚è¯·ç¡®ä¿[å…ˆé˜…è¯»è¯¥æŒ‡å—](https://gradio.app/blocks-and-event-listeners)ã€‚

## ä»‹ç»

ä½ çŸ¥é“å—ï¼Œé™¤äº†ä½œä¸ºä¸€ä¸ªå…¨æ ˆæœºå™¨å­¦ä¹ æ¼”ç¤ºï¼ŒGradio å—åº”ç”¨å…¶å®žä¹Ÿæ˜¯ä¸€ä¸ªæ™®é€šçš„ Python å‡½æ•°ï¼ï¼Ÿ

è¿™æ„å‘³ç€å¦‚æžœä½ æœ‰ä¸€ä¸ªåä¸º `demo` çš„ Gradio å—ï¼ˆæˆ–ç•Œé¢ï¼‰åº”ç”¨ï¼Œä½ å¯ä»¥åƒä½¿ç”¨ä»»ä½• Python å‡½æ•°ä¸€æ ·ä½¿ç”¨ `demo`ã€‚

æ‰€ä»¥ï¼Œåƒ `output = demo("Hello", "friend")` è¿™æ ·çš„æ“ä½œä¼šåœ¨è¾“å…¥ä¸º "Hello" å’Œ "friend" çš„æƒ…å†µä¸‹è¿è¡Œ `demo` ä¸­å®šä¹‰çš„ç¬¬ä¸€ä¸ªäº‹ä»¶ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨å˜é‡ `output` ä¸­ã€‚

å¦‚æžœä»¥ä¸Šå†…å®¹è®©ä½ æ‰“çžŒç¡ ðŸ¥±ï¼Œè¯·å¿è€ä¸€ä¸‹ï¼é€šè¿‡å°†åº”ç”¨ç¨‹åºåƒå‡½æ•°ä¸€æ ·ä½¿ç”¨ï¼Œä½ å¯ä»¥è½»æ¾åœ°ç»„åˆ Gradio åº”ç”¨ã€‚
æŽ¥ä¸‹æ¥çš„éƒ¨åˆ†å°†å±•ç¤ºå¦‚ä½•å®žçŽ°ã€‚

## å°†å—è§†ä¸ºå‡½æ•°

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘ä¸ºå¾·æ–‡æ–‡æœ¬çš„æ¼”ç¤ºå—ã€‚

$code_english_translator

æˆ‘å·²ç»å°†å®ƒæ‰˜ç®¡åœ¨ Hugging Face Spaces ä¸Šçš„ [gradio/english_translator](https://huggingface.co/spaces/gradio/english_translator)ã€‚

ä½ ä¹Ÿå¯ä»¥åœ¨ä¸‹é¢çœ‹åˆ°æ¼”ç¤ºï¼š

$demo_english_translator

çŽ°åœ¨ï¼Œå‡è®¾ä½ æœ‰ä¸€ä¸ªç”Ÿæˆè‹±æ–‡æ–‡æœ¬çš„åº”ç”¨ç¨‹åºï¼Œä½†ä½ è¿˜æƒ³é¢å¤–ç”Ÿæˆå¾·æ–‡æ–‡æœ¬ã€‚

ä½ å¯ä»¥é€‰æ‹©ï¼š

1. å°†æˆ‘çš„è‹±å¾·ç¿»è¯‘çš„æºä»£ç å¤åˆ¶ç²˜è´´åˆ°ä½ çš„åº”ç”¨ç¨‹åºä¸­ã€‚

2. åœ¨ä½ çš„åº”ç”¨ç¨‹åºä¸­åŠ è½½æˆ‘çš„è‹±å¾·ç¿»è¯‘ï¼Œå¹¶å°†å…¶å½“ä½œæ™®é€šçš„ Python å‡½æ•°å¤„ç†ã€‚

é€‰é¡¹ 1 ä»ŽæŠ€æœ¯ä¸Šè®²æ€»æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å®ƒç»å¸¸å¼•å…¥ä¸å¿…è¦çš„å¤æ‚æ€§ã€‚

é€‰é¡¹ 2 å…è®¸ä½ å€Ÿç”¨æ‰€éœ€çš„åŠŸèƒ½ï¼Œè€Œä¸ä¼šè¿‡äºŽç´§å¯†åœ°è€¦åˆæˆ‘ä»¬çš„åº”ç”¨ç¨‹åºã€‚

ä½ åªéœ€è¦åœ¨æºæ–‡ä»¶ä¸­è°ƒç”¨ `Blocks.load` ç±»æ–¹æ³•å³å¯ã€‚
ä¹‹åŽï¼Œä½ å°±å¯ä»¥åƒä½¿ç”¨æ™®é€šçš„ Python å‡½æ•°ä¸€æ ·ä½¿ç”¨æˆ‘çš„ç¿»è¯‘åº”ç”¨ç¨‹åºäº†ï¼

ä¸‹é¢çš„ä»£ç ç‰‡æ®µå’Œæ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `Blocks.load`ã€‚

è¯·æ³¨æ„ï¼Œå˜é‡ `english_translator` æ˜¯æˆ‘çš„è‹±å¾·ç¿»è¯‘åº”ç”¨ç¨‹åºï¼Œä½†å®ƒåœ¨ `generate_text` ä¸­åƒæ™®é€šå‡½æ•°ä¸€æ ·ä½¿ç”¨ã€‚

$code_generate_english_german

$demo_generate_english_german

## å¦‚ä½•æŽ§åˆ¶ä½¿ç”¨åº”ç”¨ç¨‹åºä¸­çš„å“ªä¸ªå‡½æ•°

å¦‚æžœä½ æ­£åœ¨åŠ è½½çš„åº”ç”¨ç¨‹åºå®šä¹‰äº†å¤šä¸ªå‡½æ•°ï¼Œä½ å¯ä»¥ä½¿ç”¨ `fn_index` å’Œ `api_name` å‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„å‡½æ•°ã€‚

åœ¨è‹±å¾·æ¼”ç¤ºçš„ä»£ç ä¸­ï¼Œä½ ä¼šçœ‹åˆ°ä»¥ä¸‹ä»£ç è¡Œï¼š

translate_btn.click(translate, inputs=english, outputs=german, api_name="translate-to-german")

è¿™ä¸ª `api_name` åœ¨æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºä¸­ç»™è¿™ä¸ªå‡½æ•°ä¸€ä¸ªå”¯ä¸€çš„åç§°ã€‚ä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªåç§°å‘Šè¯‰ Gradio ä½ æƒ³ä½¿ç”¨
ä¸Šæ¸¸ç©ºé—´ä¸­çš„å“ªä¸ªå‡½æ•°ï¼š

english_generator(text, api_name="translate-to-german")[0]["generated_text"]

ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ `fn_index` å‚æ•°ã€‚
å‡è®¾æˆ‘çš„åº”ç”¨ç¨‹åºè¿˜å®šä¹‰äº†ä¸€ä¸ªè‹±è¯­åˆ°è¥¿ç­ç‰™è¯­çš„ç¿»è¯‘å‡½æ•°ã€‚
ä¸ºäº†åœ¨æˆ‘ä»¬çš„æ–‡æœ¬ç”Ÿæˆåº”ç”¨ç¨‹åºä¸­ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š

english_generator(text, fn_index=1)[0]["generated_text"]

Gradio ç©ºé—´ä¸­çš„å‡½æ•°æ˜¯ä»Žé›¶å¼€å§‹ç´¢å¼•çš„ï¼Œæ‰€ä»¥è¥¿ç­ç‰™è¯­ç¿»è¯‘å™¨å°†æ˜¯æˆ‘çš„ç©ºé—´ä¸­çš„ç¬¬äºŒä¸ªå‡½æ•°ï¼Œ
å› æ­¤ä½ ä¼šä½¿ç”¨ç´¢å¼• 1ã€‚

## ç»“è¯­

æˆ‘ä»¬å±•ç¤ºäº†å°†å—åº”ç”¨è§†ä¸ºæ™®é€š Python å‡½æ•°çš„æ–¹æ³•ï¼Œè¿™æœ‰åŠ©äºŽåœ¨ä¸åŒçš„åº”ç”¨ç¨‹åºä¹‹é—´ç»„åˆåŠŸèƒ½ã€‚
ä»»ä½•å—åº”ç”¨ç¨‹åºéƒ½å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå‡½æ•°ï¼Œä½†ä¸€ä¸ªå¼ºå¤§çš„æ¨¡å¼æ˜¯åœ¨å°†å…¶è§†ä¸ºå‡½æ•°ä¹‹å‰ï¼Œ
åœ¨[è‡ªå·±çš„åº”ç”¨ç¨‹åºä¸­åŠ è½½](https://huggingface.co/spaces)æ‰˜ç®¡åœ¨[Hugging Face Spaces](https://huggingface.co/spaces)ä¸Šçš„åº”ç”¨ç¨‹åºã€‚
æ‚¨ä¹Ÿå¯ä»¥åŠ è½½æ‰˜ç®¡åœ¨[Hugging Face Model Hub](https://huggingface.co/models)ä¸Šçš„æ¨¡åž‹â€”â€”æœ‰å…³ç¤ºä¾‹ï¼Œè¯·å‚é˜…[ä½¿ç”¨ Hugging Face é›†æˆ](/using_hugging_face_integrations)æŒ‡å—ã€‚

### å¼€å§‹æž„å»ºï¼âš’ï¸

## Parting Remarks

æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°† Blocks åº”ç”¨ç¨‹åºè§†ä¸ºå¸¸è§„ Python å‡½æ•°ï¼Œä»¥ä¾¿åœ¨ä¸åŒçš„åº”ç”¨ç¨‹åºä¹‹é—´ç»„åˆåŠŸèƒ½ã€‚
ä»»ä½• Blocks åº”ç”¨ç¨‹åºéƒ½å¯ä»¥è¢«è§†ä¸ºå‡½æ•°ï¼Œä½†æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ¨¡å¼æ˜¯åœ¨å°†å…¶è§†ä¸ºè‡ªå·±åº”ç”¨ç¨‹åºçš„å‡½æ•°ä¹‹å‰ï¼Œå…ˆ`åŠ è½½`æ‰˜ç®¡åœ¨[Hugging Face Spaces](https://huggingface.co/spaces)ä¸Šçš„åº”ç”¨ç¨‹åºã€‚
æ‚¨è¿˜å¯ä»¥åŠ è½½æ‰˜ç®¡åœ¨[Hugging Face Model Hub](https://huggingface.co/models)ä¸Šçš„æ¨¡åž‹-è¯·å‚è§[ä½¿ç”¨ Hugging Face é›†æˆæŒ‡å—](/using_hugging_face_integrations)ä¸­çš„ç¤ºä¾‹ã€‚

### Happy building! âš’ï¸

---

<!-- Source: guides/03_building-with-blocks/06_custom_HTML_components.md -->
# Custom Components with `gr.HTML`

If you wish to create custom HTML in your app, use the `gr.HTML` component. Here's a basic "HTML-only" example:

```python
gr.HTML(value="<h1>Hello World!</h1>")
```

You can also use html-templates to organize your HTML. Take a look at the example below:

```python
gr.HTML(value="John", html_template"<h1>Hello, {{value}}!</h1><p>${value.length} letters</p>")
```

"John" becomes `value` when injected into the template, resulting in:

```html
<h1>Hello, John!</h1><p>4 letters</p>
```

Notice how we support two types of templating syntaxes: `${}` for custom JavaScript expressions, and `{{}}` for Handlebars templating. You can use either or both in your templates - `${}` allows for completely custom JS logic, while Handlebars provides structured templating for loops and conditionals.

Let's look at another example for displaying a list of items:

```python
gr.HTML(value=["apple", "banana", "cherry"], html_templates="""
    <h1>${value.length} fruits:</h1>
    <ul>
      {{#each value}}
        <li>{{this}}</li>
      {{/each}}
    </ul>
""")
```

By default, the content of gr.HTML will have some CSS styles applied to match the Gradio theme. You can disable this with `apply_default_css=False`. You can also provide your own CSS styles via the `css_template` argument as shown in the next example.

Let's build a simple star rating component using `gr.HTML`, and then extend it with more features.

$code_star_rating_simple
$demo_star_rating_simple

Note how we used the `css_template` argument to add custom CSS that styles the HTML inside the `gr.HTML` component.

Let's see how the template automatically updates when we update the value.

$code_star_rating_templates
$demo_star_rating_templates

We may wish to pass additional props beyond just `value` to the `html_template`. Simply add these props to your templates and pass them as kwargs to the `gr.HTML` component. For example, lets add `size` and `max_stars` props to the star rating component.

$code_star_rating_props
$demo_star_rating_props

Note how both `html_template` and `css_template` can format these extra props. Note also how any of these props can be updated via Gradio event listeners.

## Triggering Events and Custom Input Components

The `gr.HTML` component can also be used to create custom input components by triggering events. You will provide `js_on_load`, javascript code that runs when the component loads. The code has access to the `trigger` function to trigger events that Gradio can listen to, and the object `props` which has access to all the props of the component, including `value`.

$code_star_rating_events
$demo_star_rating_events

Take a look at the `js_on_load` code above. We add click event listeners to each star image to update the value via `props.value` when a star is clicked. This also re-renders the template to show the updated value. We also add a click event listener to the submit button that triggers the `submit` event. In our app, we listen to this trigger to run a function that outputs the `value` of the star rating.

You can update any other props of the component via `props.<prop_name>`, and trigger events via `trigger('<event_name>')`. The trigger event can also be send event data, e.g.

```js
trigger('event_name', { key: value, count: 123 });
```

This event data will be accessible the Python event listener functions via gr.EventData.

```python
def handle_event(evt: gr.EventData):
    print(evt.key)
    print(evt.count)

star_rating.event(fn=handle_event, inputs=[], outputs=[])
```

Keep in mind that event listeners attached in `js_on_load` are only attached once when the component is first rendered. If your component creates new elements dynamically that need event listeners, attach the event listener to a parent element that exists when the component loads, and check for the target. For example:

```js
element.addEventListener('click', (e) =>
    if (e.target && e.target.matches('.child-element')) {
        props.value = e.target.dataset.value;
    }
);
```

## Component Classes

If you are reusing the same HTML component in multiple places, you can create a custom component class by subclassing `gr.HTML` and setting default values for the templates and other arguments. Here's an example of creating a reusable StarRating component.

$code_star_rating_component
$demo_star_rating_component

Note: Gradio requires all components to accept certain arguments, such as `render`. You do not need
to handle these arguments, but you do need to accept them in your component constructor and pass
them to the parent `gr.HTML` class. Otherwise, your component may not behave correctly. The easiest
way is to add `**kwargs` to your `__init__` method and pass it to `super().__init__()`, just like in the code example above.

We've created several custom HTML components as reusable components as examples you can reference in [this directory](https://github.com/gradio-app/gradio/tree/main/gradio/components/custom_html_components).

### API / MCP support

To make your custom HTML component work with Gradio's built-in support for API and MCP (Model Context Protocol) usage, you need to define how its data should be serialized. There are two ways to do this:

**Option 1: Define an `api_info()` method**

Add an `api_info()` method that returns a JSON schema dictionary describing your component's data format. This is what we do in the StarRating class above.

**Option 2: Define a Pydantic data model**

For more complex data structures, you can define a Pydantic model that inherits from `GradioModel` or `GradioRootModel`:

```python
from gradio.data_classes import GradioModel, GradioRootModel

class MyComponentData(GradioModel):
    items: List[str]
    count: int

class MyComponent(gr.HTML):
    data_model = MyComponentData
```

Use `GradioModel` when your data is a dictionary with named fields, or `GradioRootModel` when your data is a simple type (string, list, etc.) that doesn't need to be wrapped in a dictionary. By defining a `data_model`, your component automatically implements API methods.

## Security Considerations

Keep in mind that using `gr.HTML` to create custom components involves injecting raw HTML and JavaScript into your Gradio app. Be cautious about using untrusted user input into `html_template` and `js_on_load`, as this could lead to cross-site scripting (XSS) vulnerabilities. 

You should also expect that any Python event listeners that take your `gr.HTML` component as input could have any arbitrary value passed to them, not just the values you expect the frontend to be able to set for `value`. Sanitize and validate user input appropriately in public applications.

## Next Steps

Check out some examples of custom components that you can build in [this directory](https://github.com/gradio-app/gradio/tree/main/gradio/components/custom_html_components).

---

<!-- Source: guides/04_additional-features/06_batch-functions.md -->
# Batch functions

Gradio supports the ability to pass _batch_ functions. Batch functions are just
functions which take in a list of inputs and return a list of predictions.

For example, here is a batched function that takes in two lists of inputs (a list of
words and a list of ints), and returns a list of trimmed words as output:

```py
import time

def trim_words(words, lens):
    trimmed_words = []
    time.sleep(5)
    for w, l in zip(words, lens):
        trimmed_words.append(w[:int(l)])
    return [trimmed_words]
```

The advantage of using batched functions is that if you enable queuing, the Gradio server can automatically _batch_ incoming requests and process them in parallel,
potentially speeding up your demo. Here's what the Gradio code looks like (notice the `batch=True` and `max_batch_size=16`)

With the `gr.Interface` class:

```python
demo = gr.Interface(
    fn=trim_words, 
    inputs=["textbox", "number"], 
    outputs=["output"],
    batch=True, 
    max_batch_size=16
)

demo.launch()
```

With the `gr.Blocks` class:

```py
import gradio as gr

with gr.Blocks() as demo:
    with gr.Row():
        word = gr.Textbox(label="word")
        leng = gr.Number(label="leng")
        output = gr.Textbox(label="Output")
    with gr.Row():
        run = gr.Button()

    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)

demo.launch()
```

In the example above, 16 requests could be processed in parallel (for a total inference time of 5 seconds), instead of each request being processed separately (for a total
inference time of 80 seconds). Many Hugging Face `transformers` and `diffusers` models work very naturally with Gradio's batch mode: here's [an example demo using diffusers to
generate images in batches](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)

---

<!-- Source: guides/05_chatbots/06_creating-a-discord-bot-from-a-gradio-app.md -->
# ðŸš€ Creating Discord Bots with Gradio ðŸš€

Tags: CHAT, DEPLOY, DISCORD

You can make your Gradio app available as a Discord bot to let users in your Discord server interact with it directly. 

## How does it work?

The Discord bot will listen to messages mentioning it in channels. When it receives a message (which can include text as well as files), it will send it to your Gradio app via Gradio's built-in API. Your bot will reply with the response it receives from the API. 

Because Gradio's API is very flexible, you can create Discord bots that support text, images, audio, streaming, chat history, and a wide variety of other features very easily. 

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/Screen%20Recording%202024-12-18%20at%204.26.55%E2%80%AFPM.gif)

## Prerequisites

* Install the latest version of `gradio` and the `discord.py` libraries:

```
pip install --upgrade gradio discord.py~=2.0
```

* Have a running Gradio app. This app can be running locally or on Hugging Face Spaces. In this example, we will be using the [Gradio Playground Space](https://huggingface.co/spaces/abidlabs/gradio-playground-bot), which takes in an image and/or text and generates the code to generate the corresponding Gradio app.

Now, we are ready to get started!


### 1. Create a Discord application

First, go to the [Discord apps dashboard](https://discord.com/developers/applications). Look for the "New Application" button and click it. Give your application a name, and then click "Create".

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/discord-4.png)

On the resulting screen, you will see basic information about your application. Under the Settings section, click on the "Bot" option. You can update your bot's username if you would like.

Then click on the "Reset Token" button. A new token will be generated. Copy it as we will need it for the next step.

Scroll down to the section that says "Privileged Gateway Intents". Your bot will need certain permissions to work correctly. In this tutorial, we will only be using the "Message Content Intent" so click the toggle to enable this intent. Save the changes.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/discord-3.png)



### 2. Write a Discord bot

Let's start by writing a very simple Discord bot, just to make sure that everything is working. Write the following Python code in a file called `bot.py`, pasting the discord bot token from the previous step:

```python
# bot.py
import discord

TOKEN = #PASTE YOUR DISCORD BOT TOKEN HERE

client = discord.Client()

@client.event
async def on_ready():
    print(f'{client.user} has connected to Discord!')

client.run(TOKEN)
```

Now, run this file: `python bot.py`, which should run and print a message like:

```text
We have logged in as GradioPlaygroundBot#1451
```

If that is working, we are ready to add Gradio-specific code. We will be using the [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) to query the Gradio Playground Space mentioned above. Here's the updated `bot.py` file:

```python
import discord
from gradio_client import Client, handle_file
import httpx
import os

TOKEN = #PASTE YOUR DISCORD BOT TOKEN HERE

intents = discord.Intents.default()
intents.message_content = True

client = discord.Client(intents=intents)
gradio_client = Client("abidlabs/gradio-playground-bot")

def download_image(attachment):
    response = httpx.get(attachment.url)
    image_path = f"./images/{attachment.filename}"
    os.makedirs("./images", exist_ok=True)
    with open(image_path, "wb") as f:
        f.write(response.content)
    return image_path

@client.event
async def on_ready():
    print(f'We have logged in as {client.user}')

@client.event
async def on_message(message):
    # Ignore messages from the bot itself
    if message.author == client.user:
        return

    # Check if the bot is mentioned in the message and reply
    if client.user in message.mentions:
        # Extract the message content without the bot mention
        clean_message = message.content.replace(f"<@{client.user.id}>", "").strip()

        # Handle images (only the first image is used)
        files = []
        if message.attachments:
            for attachment in message.attachments:
                if any(attachment.filename.lower().endswith(ext) for ext in ['png', 'jpg', 'jpeg', 'gif', 'webp']):
                    image_path = download_image(attachment)
                    files.append(handle_file(image_path))
                    break
        
        # Stream the responses to the channel
        for response in gradio_client.submit(
            message={"text": clean_message, "files": files},
        ):
            await message.channel.send(response[-1])

client.run(TOKEN)
```

### 3. Add the bot to your Discord Server

Now we are ready to install the bot on our server. Go back to the [Discord apps dashboard](https://discord.com/developers/applications). Under the Settings section, click on the "OAuth2" option. Scroll down to the "OAuth2 URL Generator" box and select the "bot" checkbox:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/discord-2.png)



Then in "Bot Permissions" box that pops up underneath, enable the following permissions:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/discord-1.png)


Copy the generated URL that appears underneath, which should look something like:

```text
https://discord.com/oauth2/authorize?client_id=1319011745452265575&permissions=377957238784&integration_type=0&scope=bot
```

Paste it into your browser, which should allow you to add the Discord bot to any Discord server that you manage.


### 4. That's it!

Now you can mention your bot from any channel in your Discord server, optionally attach an image, and it will respond with generated Gradio app code!

The bot will:
1. Listen for mentions
2. Process any attached images
3. Send the text and images to your Gradio app
4. Stream the responses back to the Discord channel

 This is just a basic example - you can extend it to handle more types of files, add error handling, or integrate with different Gradio apps.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/Screen%20Recording%202024-12-18%20at%204.26.55%E2%80%AFPM.gif)

If you build a Discord bot from a Gradio app, feel free to share it on X and tag [the Gradio account](https://x.com/Gradio), and we are happy to help you amplify!

---

<!-- Source: guides/07_streaming/06_automatic-voice-detection.md -->
# Multimodal Gradio App Powered by Groq with Automatic Speech Detection

Tags: AUDIO, STREAMING, CHATBOTS, VOICE

## Introduction
Modern voice applications should feel natural and responsive, moving beyond the traditional "click-to-record" pattern. By combining Groq's fast inference capabilities with automatic speech detection, we can create a more intuitive interaction model where users can simply start talking whenever they want to engage with the AI.

> Credits: VAD and Gradio code inspired by [WillHeld's Diva-audio-chat](https://huggingface.co/spaces/WillHeld/diva-audio-chat/tree/main).

In this tutorial, you will learn how to create a multimodal Gradio and Groq app that has automatic speech detection. You can also watch the full video tutorial which includes a demo of the application:

<iframe width="560" height="315" src="https://www.youtube.com/embed/azXaioGdm2Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

## Background

Many voice apps currently work by the user clicking record, speaking, then stopping the recording. While this can be a powerful demo, the most natural mode of interaction with voice requires the app to dynamically detect when the user is speaking, so they can talk back and forth without having to continually click a record button. 

Creating a natural interaction with voice and text requires a dynamic and low-latency response. Thus, we need both automatic voice detection and fast inference. With @ricky0123/vad-web powering speech detection and Groq powering the LLM, both of these requirements are met. Groq provides a lightning fast response, and Gradio allows for easy creation of impressively functional apps.

This tutorial shows you how to build a calorie tracking app where you speak to an AI that automatically detects when you start and stop your response, and provides its own text response back to guide you with questions that allow it to give a calorie estimate of your last meal.

## Key Components

- **Gradio**: Provides the web interface and audio handling capabilities
- **@ricky0123/vad-web**: Handles voice activity detection
- **Groq**: Powers fast LLM inference for natural conversations
- **Whisper**: Transcribes speech to text

### Setting Up the Environment

First, letâ€™s install and import our essential libraries and set up a client for using the Groq API. Hereâ€™s how to do it:

`requirements.txt`
```
gradio
groq
numpy
soundfile
librosa
spaces
xxhash
datasets
```

`app.py`
```python
import groq
import gradio as gr
import soundfile as sf
from dataclasses import dataclass, field
import os

# Initialize Groq client securely
api_key = os.environ.get("GROQ_API_KEY")
if not api_key:
    raise ValueError("Please set the GROQ_API_KEY environment variable.")
client = groq.Client(api_key=api_key)
```

Here, weâ€™re pulling in key libraries to interact with the Groq API, build a sleek UI with Gradio, and handle audio data. Weâ€™re accessing the Groq API key securely with a key stored in an environment variable, which is a security best practice for avoiding leaking the API key.

---

### State Management for Seamless Conversations

We need a way to keep track of our conversation history, so the chatbot remembers past interactions, and manage other states like whether recording is currently active. To do this, letâ€™s create an `AppState` class:

```python
@dataclass
class AppState:
    conversation: list = field(default_factory=list)
    stopped: bool = False
    model_outs: Any = None
```

Our `AppState` class is a handy tool for managing conversation history and tracking whether recording is on or off. Each instance will have its own fresh list of conversations, making sure chat history is isolated to each session. 

---

### Transcribing Audio with Whisper on Groq

Next, weâ€™ll create a function to transcribe the userâ€™s audio input into text using Whisper, a powerful transcription model hosted on Groq. This transcription will also help us determine whether thereâ€™s meaningful speech in the input. Hereâ€™s how:

```python
def transcribe_audio(client, file_name):
    if file_name is None:
        return None

    try:
        with open(file_name, "rb") as audio_file:
            response = client.audio.transcriptions.with_raw_response.create(
                model="whisper-large-v3-turbo",
                file=("audio.wav", audio_file),
                response_format="verbose_json",
            )
            completion = process_whisper_response(response.parse())
            return completion
    except Exception as e:
        print(f"Error in transcription: {e}")
        return f"Error in transcription: {str(e)}"
```

This function opens the audio file and sends it to Groqâ€™s Whisper model for transcription, requesting detailed JSON output. verbose_json is needed to get information to determine if speech was included in the audio. We also handle any potential errors so our app doesnâ€™t fully crash if thereâ€™s an issue with the API request. 

```python
def process_whisper_response(completion):
    """
    Process Whisper transcription response and return text or null based on no_speech_prob
    
    Args:
        completion: Whisper transcription response object
        
    Returns:
        str or None: Transcribed text if no_speech_prob <= 0.7, otherwise None
    """
    if completion.segments and len(completion.segments) > 0:
        no_speech_prob = completion.segments[0].get('no_speech_prob', 0)
        print("No speech prob:", no_speech_prob)

        if no_speech_prob > 0.7:
            return None
            
        return completion.text.strip()
    
    return None
```

We also need to interpret the audio data response. The process_whisper_response function takes the resulting completion from Whisper and checks if the audio was just background noise or had actual speaking that was transcribed. It uses a threshold of 0.7 to interpret the no_speech_prob, and will return None if there was no speech. Otherwise, it will return the text transcript of the conversational response from the human.


---

### Adding Conversational Intelligence with LLM Integration

Our chatbot needs to provide intelligent, friendly responses that flow naturally. Weâ€™ll use a Groq-hosted Llama-3.2 for this:

```python
def generate_chat_completion(client, history):
    messages = []
    messages.append(
        {
            "role": "system",
            "content": "In conversation with the user, ask questions to estimate and provide (1) total calories, (2) protein, carbs, and fat in grams, (3) fiber and sugar content. Only ask *one question at a time*. Be conversational and natural.",
        }
    )

    for message in history:
        messages.append(message)

    try:
        completion = client.chat.completions.create(
            model="llama-3.2-11b-vision-preview",
            messages=messages,
        )
        return completion.choices[0].message.content
    except Exception as e:
        return f"Error in generating chat completion: {str(e)}"
```

Weâ€™re defining a system prompt to guide the chatbotâ€™s behavior, ensuring it asks one question at a time and keeps things conversational. This setup also includes error handling to ensure the app gracefully manages any issues.

---

### Voice Activity Detection for Hands-Free Interaction

To make our chatbot hands-free, weâ€™ll add Voice Activity Detection (VAD) to automatically detect when someone starts or stops speaking. Hereâ€™s how to implement it using ONNX in JavaScript:

```javascript
async function main() {
  const script1 = document.createElement("script");
  script1.src = "https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.js";
  document.head.appendChild(script1)
  const script2 = document.createElement("script");
  script2.onload = async () =>  {
    console.log("vad loaded");
    var record = document.querySelector('.record-button');
    record.textContent = "Just Start Talking!"
    
    const myvad = await vad.MicVAD.new({
      onSpeechStart: () => {
        var record = document.querySelector('.record-button');
        var player = document.querySelector('#streaming-out')
        if (record != null && (player == null || player.paused)) {
          record.click();
        }
      },
      onSpeechEnd: (audio) => {
        var stop = document.querySelector('.stop-button');
        if (stop != null) {
          stop.click();
        }
      }
    })
    myvad.start()
  }
  script2.src = "https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.min.js";
}
```

This script loads our VAD model and sets up functions to start and stop recording automatically. When the user starts speaking, it triggers the recording, and when they stop, it ends the recording.

---

### Building a User Interface with Gradio

Now, letâ€™s create an intuitive and visually appealing user interface with Gradio. This interface will include an audio input for capturing voice, a chat window for displaying responses, and state management to keep things synchronized.

```python
with gr.Blocks() as demo:
    with gr.Row():
        input_audio = gr.Audio(
            label="Input Audio",
            sources=["microphone"],
            type="numpy",
            streaming=False,
            waveform_options=gr.WaveformOptions(waveform_color="#B83A4B"),
        )
    with gr.Row():
        chatbot = gr.Chatbot(label="Conversation")
    state = gr.State(value=AppState())
demo.launch(theme=theme, js=js)
```

In this code block, weâ€™re using Gradioâ€™s `Blocks` API to create an interface with an audio input, a chat display, and an application state manager. The color customization for the waveform adds a nice visual touch.

---

### Handling Recording and Responses

Finally, letâ€™s link the recording and response components to ensure the app reacts smoothly to user inputs and provides responses in real-time.

```python
    stream = input_audio.start_recording(
        process_audio,
        [input_audio, state],
        [input_audio, state],
    )
    respond = input_audio.stop_recording(
        response, [state, input_audio], [state, chatbot]
    )
```

These lines set up event listeners for starting and stopping the recording, processing the audio input, and generating responses. By linking these events, we create a cohesive experience where users can simply talk, and the chatbot handles the rest.

---

## Summary

1. When you open the app, the VAD system automatically initializes and starts listening for speech
2. As soon as you start talking, it triggers the recording automatically
3. When you stop speaking, the recording ends and:
   - The audio is transcribed using Whisper
   - The transcribed text is sent to the LLM
   - The LLM generates a response about calorie tracking
   - The response is displayed in the chat interface
4. This creates a natural back-and-forth conversation where you can simply talk about your meals and get instant feedback on nutritional content

This app demonstrates how to create a natural voice interface that feels responsive and intuitive. By combining Groq's fast inference with automatic speech detection, we've eliminated the need for manual recording controls while maintaining high-quality interactions. The result is a practical calorie tracking assistant that users can simply talk to as naturally as they would to a human nutritionist.

Link to GitHub repository: [Groq Gradio Basics](https://github.com/bklieger-groq/gradio-groq-basics/tree/main/calorie-tracker)

---

<!-- Source: guides/08_custom-components/06_frequently-asked-questions.md -->
# Frequently Asked Questions

## What do I need to install before using Custom Components?

Before using Custom Components, make sure you have Python 3.10+, Node.js v18+, npm 9+, and Gradio 4.0+ (preferably Gradio 5.0+) installed.

## Are custom components compatible between Gradio 4.0 and 5.0?

Custom components built with Gradio 5.0 should be compatible with Gradio 4.0. If you built your custom component in Gradio 4.0 you will have to rebuild your component to be compatible with Gradio 5.0. Simply follow these steps:
1. Update the `@gradio/preview` package. `cd` into the `frontend` directory and run `npm update`.
2.  Modify the `dependencies` key in `pyproject.toml` to pin the maximum allowed Gradio version at version 5, e.g. `dependencies = ["gradio>=4.0,<6.0"]`.
3. Run the build and publish commands

## What templates can I use to create my custom component?
Run `gradio cc show` to see the list of built-in templates.
You can also start off from other's custom components!
Simply `git clone` their repository and make your modifications.

## What is the development server?
When you run `gradio cc dev`, a development server will load and run a Gradio app of your choosing.
This is like when you run `python <app-file>.py`, however the `gradio` command will hot reload so you can instantly see your changes. 

## The development server didn't work for me 

**1. Check your terminal and browser console**

Make sure there are no syntax errors or other obvious problems in your code. Exceptions triggered from python will be displayed in the terminal. Exceptions from javascript will be displayed in the browser console and/or the terminal.

**2. Are you developing on Windows?**

Chrome on Windows will block the local compiled svelte files for security reasons. We recommend developing your custom component in the windows subsystem for linux (WSL) while the team looks at this issue.

**3. Inspect the window.__GRADIO_CC__ variable**

In the browser console, print the `window.__GRADIO__CC` variable (just type it into the console). If it is an empty object, that means
that the CLI could not find your custom component source code. Typically, this happens when the custom component is installed in a different virtual environment than the one used to run the dev command. Please use the `--python-path` and `gradio-path` CLI arguments to specify the path of the python and gradio executables for the environment your component is installed in. For example, if you are using a virtualenv located at `/Users/mary/venv`, pass in `/Users/mary/bin/python` and `/Users/mary/bin/gradio` respectively.

If the `window.__GRADIO__CC` variable is not empty (see below for an example), then the dev server should be working correctly. 

![](https://gradio-builds.s3.amazonaws.com/demo-files/gradio_CC_DEV.png)

**4. Make sure you are using a virtual environment**
It is highly recommended you use a virtual environment to prevent conflicts with other python dependencies installed in your system.


## Do I always need to start my component from scratch?
No! You can start off from an existing gradio component as a template, see the [five minute guide](./custom-components-in-five-minutes).
You can also start from an existing custom component if you'd like to tweak it further. Once you find the source code of a custom component you like, clone the code to your computer and run `gradio cc install`. Then you can run the development server to make changes.If you run into any issues, contact the author of the component by opening an issue in their repository. The [gallery](https://www.gradio.app/custom-components/gallery) is a good place to look for published components. For example, to start from the [PDF component](https://www.gradio.app/custom-components/gallery?id=freddyaboulton%2Fgradio_pdf), clone the space with `git clone https://huggingface.co/spaces/freddyaboulton/gradio_pdf`, `cd` into the `src` directory, and run `gradio cc install`.


## Do I need to host my custom component on HuggingFace Spaces?
You can develop and build your custom component without hosting or connecting to HuggingFace.
If you would like to share your component with the gradio community, it is recommended to publish your package to PyPi and host a demo on HuggingFace so that anyone can install it or try it out.

## What methods are mandatory for implementing a custom component in Gradio?

You must implement the `preprocess`, `postprocess`, `example_payload`, and `example_value` methods. If your component does not use a data model, you must also define the `api_info`, `flag`, and `read_from_flag` methods. Read more in the [backend guide](./backend).

## What is the purpose of a `data_model` in Gradio custom components?

A `data_model` defines the expected data format for your component, simplifying the component development process and self-documenting your code. It streamlines API usage and example caching.

## Why is it important to use `FileData` for components dealing with file uploads?

Utilizing `FileData` is crucial for components that expect file uploads. It ensures secure file handling, automatic caching, and streamlined client library functionality.

## How can I add event triggers to my custom Gradio component?

You can define event triggers in the `EVENTS` class attribute by listing the desired event names, which automatically adds corresponding methods to your component.

## Can I implement a custom Gradio component without defining a `data_model`?

Yes, it is possible to create custom components without a `data_model`, but you are going to have to manually implement `api_info`, `flag`, and `read_from_flag` methods.

## Are there sample custom components I can learn from?

We have prepared this [collection](https://huggingface.co/collections/gradio/custom-components-65497a761c5192d981710b12) of custom components on the HuggingFace Hub that you can use to get started!

## How can I find custom components created by the Gradio community?

We're working on creating a gallery to make it really easy to discover new custom components.
In the meantime, you can search for HuggingFace Spaces that are tagged as a `gradio-custom-component` [here](https://huggingface.co/search/full-text?q=gradio-custom-component&type=space)

---

<!-- Source: guides/03_building-with-blocks/07_custom-CSS-and-JS.md -->
# Customizing your demo with CSS and Javascript

Gradio allows you to customize your demo in several ways. You can customize the layout of your demo, add custom HTML, and add custom theming as well. This tutorial will go beyond that and walk you through how to add custom CSS and JavaScript code to your demo in order to add custom styling, animations, custom UI functionality, analytics, and more.

## Adding custom CSS to your demo

Gradio themes are the easiest way to customize the look and feel of your app. You can choose from a variety of themes, or create your own. To do so, pass the `theme=` kwarg to the `launch()` method of the `Blocks` constructor. For example:

```python
with gr.Blocks() as demo:
    ... # your code here
demo.launch(theme=gr.themes.Glass())
    ...
```

Gradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. You can extend these themes or create your own themes from scratch - see the [Theming guide](/guides/theming-guide) for more details.

For additional styling ability, you can pass any CSS to your app as a string using the `css=` kwarg in the `launch()` method. You can also pass a pathlib.Path to a css file or a list of such paths to the `css_paths=` kwarg in the `launch()` method.

**Warning**: The use of query selectors in custom JS and CSS is _not_ guaranteed to work across Gradio versions that bind to Gradio's own HTML elements as the Gradio HTML DOM may change. We recommend using query selectors sparingly.

The base class for the Gradio app is `gradio-container`, so here's an example that changes the background color of the Gradio app:

```python
with gr.Blocks() as demo:
    ... # your code here
demo.launch(css=".gradio-container {background-color: red}")
    ...
```

If you'd like to reference external files in your css, preface the file path (which can be a relative or absolute path) with `"/gradio_api/file="`, for example:

```python
with gr.Blocks() as demo:
    ... # your code here
demo.launch(css=".gradio-container {background: url('/gradio_api/file=clouds.jpg')}")
    ...
```

Note: By default, most files in the host machine are not accessible to users running the Gradio app. As a result, you should make sure that any referenced files (such as `clouds.jpg` here) are either URLs or [allowed paths, as described here](/main/guides/file-access).


## The `elem_id` and `elem_classes` Arguments

You can `elem_id` to add an HTML element `id` to any component, and `elem_classes` to add a class or list of classes. This will allow you to select elements more easily with CSS. This approach is also more likely to be stable across Gradio versions as built-in class names or ids may change (however, as mentioned in the warning above, we cannot guarantee complete compatibility between Gradio versions if you use custom CSS as the DOM elements may themselves change).

```python
css = """
#warning {background-color: #FFCCCB}
.feedback textarea {font-size: 24px !important}
"""

with gr.Blocks() as demo:
    box1 = gr.Textbox(value="Good Job", elem_classes="feedback")
    box2 = gr.Textbox(value="Failure", elem_id="warning", elem_classes="feedback")
demo.launch(css=css)
```

The CSS `#warning` ruleset will only target the second Textbox, while the `.feedback` ruleset will target both. Note that when targeting classes, you might need to put the `!important` selector to override the default Gradio styles.

## Adding custom JavaScript to your demo

There are 3 ways to add javascript code to your Gradio demo:

1. You can add JavaScript code as a string to the `js` parameter of the `Blocks` or `Interface` initializer. This will run the JavaScript code when the demo is first loaded.

Below is an example of adding custom js to show an animated welcome message when the demo first loads.

$code_blocks_js_load
$demo_blocks_js_load


2. When using `Blocks` and event listeners, events have a `js` argument that can take a JavaScript function as a string and treat it just like a Python event listener function. You can pass both a JavaScript function and a Python function (in which case the JavaScript function is run first) or only Javascript (and set the Python `fn` to `None`). Take a look at the code below:
   
$code_blocks_js_methods
$demo_blocks_js_methods

3. Lastly, you can add JavaScript code to the `head` param of the `Blocks` initializer. This will add the code to the head of the HTML document. For example, you can add Google Analytics to your demo like so:


```python
head = f"""
<script async src="https://www.googletagmanager.com/gtag/js?id={google_analytics_tracking_id}"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){{dataLayer.push(arguments);}}
  gtag('js', new Date());
  gtag('config', '{google_analytics_tracking_id}');
</script>
"""

with gr.Blocks() as demo:
    gr.HTML("<h1>My App</h1>")

demo.launch(head=head)
```

The `head` parameter accepts any HTML tags you would normally insert into the `<head>` of a page. For example, you can also include `<meta>` tags to `head` in order to update the social sharing preview for your Gradio app like this:

```py
import gradio as gr

custom_head = """
<!-- HTML Meta Tags -->
<title>Sample App</title>
<meta name="description" content="An open-source web application showcasing various features and capabilities.">

<!-- Facebook Meta Tags -->
<meta property="og:url" content="https://example.com">
<meta property="og:type" content="website">
<meta property="og:title" content="Sample App">
<meta property="og:description" content="An open-source web application showcasing various features and capabilities.">
<meta property="og:image" content="https://cdn.britannica.com/98/152298-050-8E45510A/Cheetah.jpg">

<!-- Twitter Meta Tags -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:creator" content="@example_user">
<meta name="twitter:title" content="Sample App">
<meta name="twitter:description" content="An open-source web application showcasing various features and capabilities.">
<meta name="twitter:image" content="https://cdn.britannica.com/98/152298-050-8E45510A/Cheetah.jpg">
<meta property="twitter:domain" content="example.com">
<meta property="twitter:url" content="https://example.com">  
"""

with gr.Blocks(title="My App") as demo:
    gr.HTML("<h1>My App</h1>")

demo.launch(head=custom_head)
```



Note that injecting custom JS can affect browser behavior and accessibility (e.g. keyboard shortcuts may be lead to unexpected behavior if your Gradio app is embedded in another webpage). You should test your interface across different browsers and be mindful of how scripts may interact with browser defaults. Here's an example where pressing `Shift + s` triggers the `click` event of a specific `Button` component if the browser focus is _not_ on an input component (e.g. `Textbox` component):

```python
import gradio as gr

shortcut_js = """
<script>
function shortcuts(e) {
    var event = document.all ? window.event : e;
    switch (e.target.tagName.toLowerCase()) {
        case "input":
        case "textarea":
        break;
        default:
        if (e.key.toLowerCase() == "s" && e.shiftKey) {
            document.getElementById("my_btn").click();
        }
    }
}
document.addEventListener('keypress', shortcuts, false);
</script>
"""

with gr.Blocks() as demo:
    action_button = gr.Button(value="Name", elem_id="my_btn")
    textbox = gr.Textbox()
    action_button.click(lambda : "button pressed", None, textbox)
    
demo.launch(head=shortcut_js)
```

---

<!-- Source: guides/04_additional-features/07_sharing-your-app.md -->
# Sharing Your App

In this Guide, we dive more deeply into the various aspects of sharing a Gradio app with others. We will cover:

1. [Sharing demos with the share parameter](#sharing-demos)
2. [Hosting on HF Spaces](#hosting-on-hf-spaces)
3. [Sharing Deep Links](#sharing-deep-links)
4. [Embedding hosted spaces](#embedding-hosted-spaces)
5. [Using the API page](#api-page)
6. [Accessing network requests](#accessing-the-network-request-directly)
7. [Mounting within FastAPI](#mounting-within-another-fast-api-app)
8. [Authentication](#authentication)
9. [MCP Servers](#mcp-servers)
10. [Rate Limits](#rate-limits)
11. [Analytics](#analytics)
12. [Progressive Web Apps (PWAs)](#progressive-web-app-pwa)

## Sharing Demos

Gradio demos can be easily shared publicly by setting `share=True` in the `launch()` method. Like this:

```python
import gradio as gr

def greet(name):
    return "Hello " + name + "!"

demo = gr.Interface(fn=greet, inputs="textbox", outputs="textbox")

demo.launch(share=True)  # Share your demo with just 1 extra parameter ðŸš€
```

This generates a public, shareable link that you can send to anybody! When you send this link, the user on the other side can try out the model in their browser. Because the processing happens on your device (as long as your device stays on), you don't have to worry about any packaging any dependencies.

![sharing](https://github.com/gradio-app/gradio/blob/main/guides/assets/sharing.svg?raw=true)


A share link usually looks something like this: **https://07ff8706ab.gradio.live**. Although the link is served through the Gradio Share Servers, these servers are only a proxy for your local server, and do not store any data sent through your app. Share links expire after 1 week. (it is [also possible to set up your own Share Server](https://github.com/huggingface/frp/) on your own cloud server to overcome this restriction.)

Tip: Keep in mind that share links are publicly accessible, meaning that anyone can use your model for prediction! Therefore, make sure not to expose any sensitive information through the functions you write, or allow any critical changes to occur on your device. Or you can [add authentication to your Gradio app](#authentication) as discussed below.

Note that by default, `share=False`, which means that your server is only running locally. (This is the default, except in Google Colab notebooks, where share links are automatically created). As an alternative to using share links, you can use use [SSH port-forwarding](https://www.ssh.com/ssh/tunneling/example) to share your local server with specific users.


## Hosting on HF Spaces

If you'd like to have a permanent link to your Gradio demo on the internet, use Hugging Face Spaces. [Hugging Face Spaces](http://huggingface.co/spaces/) provides the infrastructure to permanently host your machine learning model for free!

After you have [created a free Hugging Face account](https://huggingface.co/join), you have two methods to deploy your Gradio app to Hugging Face Spaces:

1. From terminal: run `gradio deploy` in your app directory. The CLI will gather some basic metadata, upload all the files in the current directory (respecting any `.gitignore` file that may be present in the root of the directory), and then launch your app on Spaces. To update your Space, you can re-run this command or enable the Github Actions option in the CLI to automatically update the Spaces on `git push`.

2. From your browser: Drag and drop a folder containing your Gradio model and all related files [here](https://huggingface.co/new-space). See [this guide how to host on Hugging Face Spaces](https://huggingface.co/blog/gradio-spaces) for more information, or watch the embedded video:

<video autoplay muted loop>
  <source src="https://github.com/gradio-app/gradio/blob/main/guides/assets/hf_demo.mp4?raw=true" type="video/mp4" />
</video>

## Sharing Deep Links

You can add a button to your Gradio app that creates a unique URL you can use to share your app and all components **as they currently are** with others. This is useful for sharing unique and interesting generations from your application , or for saving a snapshot of your app at a particular point in time.

To add a deep link button to your app, place the `gr.DeepLinkButton` component anywhere in your app.
For the URL to be accessible to others, your app must be available at a public URL. So be sure to host your app like Hugging Face Spaces or use the `share=True` parameter when launching your app.

Let's see an example of how this works. Here's a simple Gradio chat ap that uses the `gr.DeepLinkButton` component. After a couple of messages, click the deep link button and paste it into a new browser tab to see the app as it is at that point in time.

$code_deep_link
$demo_deep_link


## Embedding Hosted Spaces

Once you have hosted your app on Hugging Face Spaces (or on your own server), you may want to embed the demo on a different website, such as your blog or your portfolio. Embedding an interactive demo allows people to try out the machine learning model that you have built, without needing to download or install anything â€” right in their browser! The best part is that you can embed interactive demos even in static websites, such as GitHub pages.

There are two ways to embed your Gradio demos. You can find quick links to both options directly on the Hugging Face Space page, in the "Embed this Space" dropdown option:

![Embed this Space dropdown option](https://github.com/gradio-app/gradio/blob/main/guides/assets/embed_this_space.png?raw=true)

### Embedding with Web Components

Web components typically offer a better experience to users than IFrames. Web components load lazily, meaning that they won't slow down the loading time of your website, and they automatically adjust their height based on the size of the Gradio app.

To embed with Web Components:

1. Import the gradio JS library into into your site by adding the script below in your site (replace {GRADIO_VERSION} in the URL with the library version of Gradio you are using).

```html
<script
	type="module"
	src="https://gradio.s3-us-west-2.amazonaws.com/{GRADIO_VERSION}/gradio.js"
></script>
```

2. Add

```html
<gradio-app src="https://$your_space_host.hf.space"></gradio-app>
```

element where you want to place the app. Set the `src=` attribute to your Space's embed URL, which you can find in the "Embed this Space" button. For example:

```html
<gradio-app
	src="https://abidlabs-pytorch-image-classifier.hf.space"
></gradio-app>
```

<script>
fetch("https://pypi.org/pypi/gradio/json"
).then(r => r.json()
).then(obj => {
    let v = obj.info.version;
    content = document.querySelector('.prose');
    content.innerHTML = content.innerHTML.replaceAll("{GRADIO_VERSION}", v);
});
</script>

You can see examples of how web components look <a href="https://www.gradio.app">on the Gradio landing page</a>.

You can also customize the appearance and behavior of your web component with attributes that you pass into the `<gradio-app>` tag:

- `src`: as we've seen, the `src` attributes links to the URL of the hosted Gradio demo that you would like to embed
- `space`: an optional shorthand if your Gradio demo is hosted on Hugging Face Space. Accepts a `username/space_name` instead of a full URL. Example: `gradio/Echocardiogram-Segmentation`. If this attribute attribute is provided, then `src` does not need to be provided.
- `control_page_title`: a boolean designating whether the html title of the page should be set to the title of the Gradio app (by default `"false"`)
- `initial_height`: the initial height of the web component while it is loading the Gradio app, (by default `"300px"`). Note that the final height is set based on the size of the Gradio app.
- `container`: whether to show the border frame and information about where the Space is hosted (by default `"true"`)
- `info`: whether to show just the information about where the Space is hosted underneath the embedded app (by default `"true"`)
- `autoscroll`: whether to autoscroll to the output when prediction has finished (by default `"false"`)
- `eager`: whether to load the Gradio app as soon as the page loads (by default `"false"`)
- `theme_mode`: whether to use the `dark`, `light`, or default `system` theme mode (by default `"system"`)
- `render`: an event that is triggered once the embedded space has finished rendering.

Here's an example of how to use these attributes to create a Gradio app that does not lazy load and has an initial height of 0px.

```html
<gradio-app
	space="gradio/Echocardiogram-Segmentation"
	eager="true"
	initial_height="0px"
></gradio-app>
```

Here's another example of how to use the `render` event. An event listener is used to capture the `render` event and will call the `handleLoadComplete()` function once rendering is complete.

```html
<script>
	function handleLoadComplete() {
		console.log("Embedded space has finished rendering");
	}

	const gradioApp = document.querySelector("gradio-app");
	gradioApp.addEventListener("render", handleLoadComplete);
</script>
```

_Note: While Gradio's CSS will never impact the embedding page, the embedding page can affect the style of the embedded Gradio app. Make sure that any CSS in the parent page isn't so general that it could also apply to the embedded Gradio app and cause the styling to break. Element selectors such as `header { ... }` and `footer { ... }` will be the most likely to cause issues._

### Embedding with IFrames

To embed with IFrames instead (if you cannot add javascript to your website, for example), add this element:

```html
<iframe src="https://$your_space_host.hf.space"></iframe>
```

Again, you can find the `src=` attribute to your Space's embed URL, which you can find in the "Embed this Space" button.

Note: if you use IFrames, you'll probably want to add a fixed `height` attribute and set `style="border:0;"` to remove the border. In addition, if your app requires permissions such as access to the webcam or the microphone, you'll need to provide that as well using the `allow` attribute.

## API Page

You can use almost any Gradio app as an API! In the footer of a Gradio app [like this one](https://huggingface.co/spaces/gradio/hello_world), you'll see a "Use via API" link.

![Use via API](https://github.com/gradio-app/gradio/blob/main/guides/assets/use_via_api.png?raw=true)

This is a page that lists the endpoints that can be used to query the Gradio app, via our supported clients: either [the Python client](https://gradio.app/guides/getting-started-with-the-python-client/), or [the JavaScript client](https://gradio.app/guides/getting-started-with-the-js-client/). For each endpoint, Gradio automatically generates the parameters and their types, as well as example inputs, like this.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api.png)

The endpoints are automatically created when you launch a Gradio application. If you are using Gradio `Blocks`, you can also name each event listener, such as

```python
btn.click(add, [num1, num2], output, api_name="addition")
```

This will add and document the endpoint `/addition/` to the automatically generated API page. Read more about the [API page here](./view-api-page).

## Accessing the Network Request Directly

When a user makes a prediction to your app, you may need the underlying network request, in order to get the request headers (e.g. for advanced authentication), log the client's IP address, getting the query parameters, or for other reasons. Gradio supports this in a similar manner to FastAPI: simply add a function parameter whose type hint is `gr.Request` and Gradio will pass in the network request as that parameter. Here is an example:

```python
import gradio as gr

def echo(text, request: gr.Request):
    if request:
        print("Request headers dictionary:", request.headers)
        print("IP address:", request.client.host)
        print("Query parameters:", dict(request.query_params))
    return text

io = gr.Interface(echo, "textbox", "textbox").launch()
```

Note: if your function is called directly instead of through the UI (this happens, for
example, when examples are cached, or when the Gradio app is called via API), then `request` will be `None`.
You should handle this case explicitly to ensure that your app does not throw any errors. That is why
we have the explicit check `if request`.

## Mounting Within Another FastAPI App

In some cases, you might have an existing FastAPI app, and you'd like to add a path for a Gradio demo.
You can easily do this with `gradio.mount_gradio_app()`.

Here's a complete example:

$code_custom_path

Note that this approach also allows you run your Gradio apps on custom paths (`http://localhost:8000/gradio` in the example above).


## Authentication

### Password-protected app

You may wish to put an authentication page in front of your app to limit who can open your app. With the `auth=` keyword argument in the `launch()` method, you can provide a tuple with a username and password, or a list of acceptable username/password tuples; Here's an example that provides password-based authentication for a single user named "admin":

```python
demo.launch(auth=("admin", "pass1234"))
```

For more complex authentication handling, you can even pass a function that takes a username and password as arguments, and returns `True` to allow access, `False` otherwise.

Here's an example of a function that accepts any login where the username and password are the same:

```python
def same_auth(username, password):
    return username == password
demo.launch(auth=same_auth)
```

If you have multiple users, you may wish to customize the content that is shown depending on the user that is logged in. You can retrieve the logged in user by [accessing the network request directly](#accessing-the-network-request-directly) as discussed above, and then reading the `.username` attribute of the request. Here's an example:


```python
import gradio as gr

def update_message(request: gr.Request):
    return f"Welcome, {request.username}"

with gr.Blocks() as demo:
    m = gr.Markdown()
    demo.load(update_message, None, m)

demo.launch(auth=[("Abubakar", "Abubakar"), ("Ali", "Ali")])
```

Note: For authentication to work properly, third party cookies must be enabled in your browser. This is not the case by default for Safari or for Chrome Incognito Mode.

If users visit the `/logout` page of your Gradio app, they will automatically be logged out and session cookies deleted. This allows you to add logout functionality to your Gradio app as well. Let's update the previous example to include a log out button:

```python
import gradio as gr

def update_message(request: gr.Request):
    return f"Welcome, {request.username}"

with gr.Blocks() as demo:
    m = gr.Markdown()
    logout_button = gr.Button("Logout", link="/logout")
    demo.load(update_message, None, m)

demo.launch(auth=[("Pete", "Pete"), ("Dawood", "Dawood")])
```
By default, visiting `/logout` logs the user out from **all sessions** (e.g. if they are logged in from multiple browsers or devices, all will be signed out). If you want to log out only from the **current session**, add the query parameter `all_session=false` (i.e. `/logout?all_session=false`).

Note: Gradio's built-in authentication provides a straightforward and basic layer of access control but does not offer robust security features for applications that require stringent access controls (e.g.  multi-factor authentication, rate limiting, or automatic lockout policies).

### OAuth (Login via Hugging Face)

Gradio natively supports OAuth login via Hugging Face. In other words, you can easily add a _"Sign in with Hugging Face"_ button to your demo, which allows you to get a user's HF username as well as other information from their HF profile. Check out [this Space](https://huggingface.co/spaces/Wauplin/gradio-oauth-demo) for a live demo.

To enable OAuth, you must set `hf_oauth: true` as a Space metadata in your README.md file. This will register your Space
as an OAuth application on Hugging Face. Next, you can use `gr.LoginButton` to add a login button to
your Gradio app. Once a user is logged in with their HF account, you can retrieve their profile by adding a parameter of type
`gr.OAuthProfile` to any Gradio function. The user profile will be automatically injected as a parameter value. If you want
to perform actions on behalf of the user (e.g. list user's private repos, create repo, etc.), you can retrieve the user
token by adding a parameter of type `gr.OAuthToken`. You must define which scopes you will use in your Space metadata
(see [documentation](https://huggingface.co/docs/hub/spaces-oauth#scopes) for more details).

Here is a short example:

$code_login_with_huggingface

When the user clicks on the login button, they get redirected in a new page to authorize your Space.

<center>
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/oauth_sign_in.png" style="width:300px; max-width:80%">
</center>

Users can revoke access to their profile at any time in their [settings](https://huggingface.co/settings/connected-applications).

As seen above, OAuth features are available only when your app runs in a Space. However, you often need to test your app
locally before deploying it. To test OAuth features locally, your machine must be logged in to Hugging Face. Please run `huggingface-cli login` or set `HF_TOKEN` as environment variable with one of your access token. You can generate a new token in your settings page (https://huggingface.co/settings/tokens). Then, clicking on the `gr.LoginButton` will log in to your local Hugging Face profile, allowing you to debug your app with your Hugging Face account before deploying it to a Space.

**Security Note**: It is important to note that adding a `gr.LoginButton` does not restrict users from using your app, in the same way that adding [username-password authentication](/guides/sharing-your-app#password-protected-app) does. This means that users of your app who have not logged in with Hugging Face can still access and run events in your Gradio app -- the difference is that the `gr.OAuthProfile` or `gr.OAuthToken` will be `None` in the corresponding functions.


### OAuth (with external providers)

It is also possible to authenticate with external OAuth providers (e.g. Google OAuth) in your Gradio apps. To do this, first mount your Gradio app within a FastAPI app ([as discussed above](#mounting-within-another-fast-api-app)). Then, you must write an *authentication function*, which gets the user's username from the OAuth provider and returns it. This function should be passed to the `auth_dependency` parameter in `gr.mount_gradio_app`.

Similar to [FastAPI dependency functions](https://fastapi.tiangolo.com/tutorial/dependencies/), the function specified by `auth_dependency` will run before any Gradio-related route in your FastAPI app. The function should accept a single parameter: the FastAPI `Request` and return either a string (representing a user's username) or `None`. If a string is returned, the user will be able to access the Gradio-related routes in your FastAPI app.

First, let's show a simplistic example to illustrate the `auth_dependency` parameter:

```python
from fastapi import FastAPI, Request
import gradio as gr

app = FastAPI()

def get_user(request: Request):
    return request.headers.get("user")

demo = gr.Interface(lambda s: f"Hello {s}!", "textbox", "textbox")

app = gr.mount_gradio_app(app, demo, path="/demo", auth_dependency=get_user)

if __name__ == '__main__':
    uvicorn.run(app)
```

In this example, only requests that include a "user" header will be allowed to access the Gradio app. Of course, this does not add much security, since any user can add this header in their request.

Here's a more complete example showing how to add Google OAuth to a Gradio app (assuming you've already created OAuth Credentials on the [Google Developer Console](https://console.cloud.google.com/project)):

```python
import os
from authlib.integrations.starlette_client import OAuth, OAuthError
from fastapi import FastAPI, Depends, Request
from starlette.config import Config
from starlette.responses import RedirectResponse
from starlette.middleware.sessions import SessionMiddleware
import uvicorn
import gradio as gr

app = FastAPI()

# Replace these with your own OAuth settings
GOOGLE_CLIENT_ID = "..."
GOOGLE_CLIENT_SECRET = "..."
SECRET_KEY = "..."

config_data = {'GOOGLE_CLIENT_ID': GOOGLE_CLIENT_ID, 'GOOGLE_CLIENT_SECRET': GOOGLE_CLIENT_SECRET}
starlette_config = Config(environ=config_data)
oauth = OAuth(starlette_config)
oauth.register(
    name='google',
    server_metadata_url='https://accounts.google.com/.well-known/openid-configuration',
    client_kwargs={'scope': 'openid email profile'},
)

SECRET_KEY = os.environ.get('SECRET_KEY') or "a_very_secret_key"
app.add_middleware(SessionMiddleware, secret_key=SECRET_KEY)

# Dependency to get the current user
def get_user(request: Request):
    user = request.session.get('user')
    if user:
        return user['name']
    return None

@app.get('/')
def public(user: dict = Depends(get_user)):
    if user:
        return RedirectResponse(url='/gradio')
    else:
        return RedirectResponse(url='/login-demo')

@app.route('/logout')
async def logout(request: Request):
    request.session.pop('user', None)
    return RedirectResponse(url='/')

@app.route('/login')
async def login(request: Request):
    redirect_uri = request.url_for('auth')
    # If your app is running on https, you should ensure that the
    # `redirect_uri` is https, e.g. uncomment the following lines:
    #
    # from urllib.parse import urlparse, urlunparse
    # redirect_uri = urlunparse(urlparse(str(redirect_uri))._replace(scheme='https'))
    return await oauth.google.authorize_redirect(request, redirect_uri)

@app.route('/auth')
async def auth(request: Request):
    try:
        access_token = await oauth.google.authorize_access_token(request)
    except OAuthError:
        return RedirectResponse(url='/')
    request.session['user'] = dict(access_token)["userinfo"]
    return RedirectResponse(url='/')

with gr.Blocks() as login_demo:
    gr.Button("Login", link="/login")

app = gr.mount_gradio_app(app, login_demo, path="/login-demo")

def greet(request: gr.Request):
    return f"Welcome to Gradio, {request.username}"

with gr.Blocks() as main_demo:
    m = gr.Markdown("Welcome to Gradio!")
    gr.Button("Logout", link="/logout")
    main_demo.load(greet, None, m)

app = gr.mount_gradio_app(app, main_demo, path="/gradio", auth_dependency=get_user)

if __name__ == '__main__':
    uvicorn.run(app)
```

There are actually two separate Gradio apps in this example! One that simply displays a log in button (this demo is accessible to any user), while the other main demo is only accessible to users that are logged in. You can try this example out on [this Space](https://huggingface.co/spaces/gradio/oauth-example).

## MCP Servers

Gradio apps can function as MCP (Model Context Protocol) servers, allowing LLMs to use your app's functions as tools. By simply setting `mcp_server=True` in the `.launch()` method, Gradio automatically converts your app's functions into MCP tools that can be called by MCP clients like Claude Desktop, Cursor, or Cline. The server exposes tools based on your function names, docstrings, and type hints, and can handle file uploads, authentication headers, and progress updates. You can also create MCP-only functions using `gr.api` and expose resources and prompts using decorators. For a comprehensive guide on building MCP servers with Gradio, see [Building an MCP Server with Gradio](https://www.gradio.app/guides/building-mcp-server-with-gradio).

## Rate Limits

When publishing your app publicly, and making it available via API or via MCP server, you might want to set rate limits to prevent users from abusing your app. You can identify users using their IP address (using the `gr.Request` object [as discussed above](#accessing-the-network-request-directly)) or, if they are logged in via Hugging Face OAuth, using their username. To see a complete example of how to set rate limits, please see [this Gradio app](https://github.com/gradio-app/gradio/blob/main/demo/rate_limit/run.py).

## Analytics

By default, Gradio collects certain analytics to help us better understand the usage of the `gradio` library. This includes the following information:

* What environment the Gradio app is running on (e.g. Colab Notebook, Hugging Face Spaces)
* What input/output components are being used in the Gradio app
* Whether the Gradio app is utilizing certain advanced features, such as `auth` or `show_error`
* The IP address which is used solely to measure the number of unique developers using Gradio
* The version of Gradio that is running

No information is collected from _users_ of your Gradio app. If you'd like to disable analytics altogether, you can do so by setting the `analytics_enabled` parameter to `False` in `gr.Blocks`, `gr.Interface`, or `gr.ChatInterface`. Or, you can set the GRADIO_ANALYTICS_ENABLED environment variable to `"False"` to apply this to all Gradio apps created across your system.

*Note*: this reflects the analytics policy as of `gradio>=4.32.0`.

## Progressive Web App (PWA)

[Progressive Web Apps (PWAs)](https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps) are web applications that are regular web pages or websites, but can appear to the user like installable platform-specific applications.

Gradio apps can be easily served as PWAs by setting the `pwa=True` parameter in the `launch()` method. Here's an example:

```python
import gradio as gr

def greet(name):
    return "Hello " + name + "!"

demo = gr.Interface(fn=greet, inputs="textbox", outputs="textbox")

demo.launch(pwa=True)  # Launch your app as a PWA
```

This will generate a PWA that can be installed on your device. Here's how it looks:

![Installing PWA](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/install-pwa.gif)

When you specify `favicon_path` in the `launch()` method, the icon will be used as the app's icon. Here's an example:

```python
demo.launch(pwa=True, favicon_path="./hf-logo.svg")  # Use a custom icon for your PWA
```

![Custom PWA Icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/pwa-favicon.png)

---

<!-- Source: guides/05_chatbots/07_creating-a-slack-bot-from-a-gradio-app.md -->
# ðŸš€ Creating a Slack Bot from a Gradio App ðŸš€

Tags: CHAT, DEPLOY, SLACK

You can make your Gradio app available as a Slack bot to let users in your Slack workspace interact with it directly. 

## How does it work?

The Slack bot will listen to messages mentioning it in channels. When it receives a message (which can include text as well as files), it will send it to your Gradio app via Gradio's built-in API. Your bot will reply with the response it receives from the API. 

Because Gradio's API is very flexible, you can create Slack bots that support text, images, audio, streaming, chat history, and a wide variety of other features very easily. 

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/Screen%20Recording%202024-12-19%20at%203.30.00%E2%80%AFPM.gif)

## Prerequisites

* Install the latest version of `gradio` and the `slack-bolt` library:

```bash
pip install --upgrade gradio slack-bolt~=1.0
```

* Have a running Gradio app. This app can be running locally or on Hugging Face Spaces. In this example, we will be using the [Gradio Playground Space](https://huggingface.co/spaces/abidlabs/gradio-playground-bot), which takes in an image and/or text and generates the code to generate the corresponding Gradio app.

Now, we are ready to get started!

### 1. Create a Slack App

1. Go to [api.slack.com/apps](https://api.slack.com/apps) and click "Create New App"
2. Choose "From scratch" and give your app a name
3. Select the workspace where you want to develop your app
4. Under "OAuth & Permissions", scroll to "Scopes" and add these Bot Token Scopes:
   - `app_mentions:read`
   - `chat:write`
   - `files:read`
   - `files:write`
5. In the same "OAuth & Permissions" page, scroll back up and click the button to install the app to your workspace.
6. Note the "Bot User OAuth Token" (starts with `xoxb-`) that appears as we'll need it later
7. Click on "Socket Mode" in the menu bar. When the page loads, click the toggle to "Enable Socket Mode"
8. Give your token a name, such as `socket-token` and copy the token that is generated (starts with `xapp-`) as we'll need it later.
9. Finally, go to the "Event Subscription" option in the menu bar. Click the toggle to "Enable Events" and subscribe to the `app_mention` bot event.

### 2. Write a Slack bot

Let's start by writing a very simple Slack bot, just to make sure that everything is working. Write the following Python code in a file called `bot.py`, pasting the two tokens from step 6 and step 8 in the previous section.

```py
from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler

SLACK_BOT_TOKEN = # PASTE YOUR SLACK BOT TOKEN HERE
SLACK_APP_TOKEN = # PASTE YOUR SLACK APP TOKEN HERE

app = App(token=SLACK_BOT_TOKEN)

@app.event("app_mention")
def handle_app_mention_events(body, say):
    user_id = body["event"]["user"]
    say(f"Hi <@{user_id}>! You mentioned me and said: {body['event']['text']}")

if __name__ == "__main__":
    handler = SocketModeHandler(app, SLACK_APP_TOKEN)
    handler.start()
```

If that is working, we are ready to add Gradio-specific code. We will be using the [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) to query the Gradio Playground Space mentioned above. Here's the updated `bot.py` file:

```python
from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler

SLACK_BOT_TOKEN = # PASTE YOUR SLACK BOT TOKEN HERE
SLACK_APP_TOKEN = # PASTE YOUR SLACK APP TOKEN HERE

app = App(token=SLACK_BOT_TOKEN)
gradio_client = Client("abidlabs/gradio-playground-bot")

def download_image(url, filename):
    headers = {"Authorization": f"Bearer {SLACK_BOT_TOKEN}"}
    response = httpx.get(url, headers=headers)
    image_path = f"./images/{filename}"
    os.makedirs("./images", exist_ok=True)
    with open(image_path, "wb") as f:
        f.write(response.content)
    return image_path

def slackify_message(message):   
    # Replace markdown links with slack format and remove code language specifier after triple backticks
    pattern = r'\[(.*?)\]\((.*?)\)'
    cleaned = re.sub(pattern, r'<\2|\1>', message)
    cleaned = re.sub(r'```\w+\n', '```', cleaned)
    return cleaned.strip()

@app.event("app_mention")
def handle_app_mention_events(body, say):
    # Extract the message content without the bot mention
    text = body["event"]["text"]
    bot_user_id = body["authorizations"][0]["user_id"]
    clean_message = text.replace(f"<@{bot_user_id}>", "").strip()
    
    # Handle images if present
    files = []
    if "files" in body["event"]:
        for file in body["event"]["files"]:
            if file["filetype"] in ["png", "jpg", "jpeg", "gif", "webp"]:
                image_path = download_image(file["url_private_download"], file["name"])
                files.append(handle_file(image_path))
                break
    
    # Submit to Gradio and send responses back to Slack
    for response in gradio_client.submit(
        message={"text": clean_message, "files": files},
    ):
        cleaned_response = slackify_message(response[-1])
        say(cleaned_response)

if __name__ == "__main__":
    handler = SocketModeHandler(app, SLACK_APP_TOKEN)
    handler.start()
```
### 3. Add the bot to your Slack Workplace

Now, create a new channel or navigate to an existing channel in your Slack workspace where you want to use the bot. Click the "+" button next to "Channels" in your Slack sidebar and follow the prompts to create a new channel.

Finally, invite your bot to the channel:
1. In your new channel, type `/invite @YourBotName`
2. Select your bot from the dropdown
3. Click "Invite to Channel"

### 4. That's it!

Now you can mention your bot in any channel it's in, optionally attach an image, and it will respond with generated Gradio app code!

The bot will:
1. Listen for mentions
2. Process any attached images
3. Send the text and images to your Gradio app
4. Stream the responses back to the Slack channel

This is just a basic example - you can extend it to handle more types of files, add error handling, or integrate with different Gradio apps!

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/Screen%20Recording%202024-12-19%20at%203.30.00%E2%80%AFPM.gif)

If you build a Slack bot from a Gradio app, feel free to share it on X and tag [the Gradio account](https://x.com/Gradio), and we are happy to help you amplify!

---

<!-- Source: guides/08_custom-components/07_pdf-component-example.md -->
# Case Study: A Component to Display PDFs

Let's work through an example of building a custom gradio component for displaying PDF files.
This component will come in handy for showcasing [document question answering](https://huggingface.co/models?pipeline_tag=document-question-answering&sort=trending) models, which typically work on PDF input.
This is a sneak preview of what our finished component will look like:

![demo](https://gradio-builds.s3.amazonaws.com/assets/PDFDisplay.png)

## Step 0: Prerequisites
Make sure you have gradio 5.0 or higher installed as well as node 20+.
As of the time of publication, the latest release is 4.1.1.
Also, please read the [Five Minute Tour](./custom-components-in-five-minutes) of custom components and the [Key Concepts](./key-component-concepts) guide before starting.


## Step 1: Creating the custom component

Navigate to a directory of your choosing and run the following command:

```bash
gradio cc create PDF
```


Tip: You should change the name of the component.
Some of the screenshots assume the component is called `PDF` but the concepts are the same!

This will create a subdirectory called `pdf` in your current working directory.
There are three main subdirectories in `pdf`: `frontend`, `backend`, and `demo`.
If you open `pdf` in your code editor, it will look like this:

![directory structure](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/CodeStructure.png)

Tip: For this demo we are not templating off a current gradio component. But you can see the list of available templates with `gradio cc show` and then pass the template name to the `--template` option, e.g. `gradio cc create <Name> --template <foo>`

## Step 2: Frontend - modify javascript dependencies

We're going to use the [pdfjs](https://mozilla.github.io/pdf.js/) javascript library to display the pdfs in the frontend. 
Let's start off by adding it to our frontend project's dependencies, as well as adding a couple of other projects we'll need.

From within the `frontend` directory, run `npm install @gradio/client @gradio/upload @gradio/icons @gradio/button` and `npm install --save-dev pdfjs-dist@3.11.174`.
Also, let's uninstall the `@zerodevx/svelte-json-view` dependency by running `npm uninstall @zerodevx/svelte-json-view`.

The complete `package.json` should look like this:

```json
{
  "name": "gradio_pdf",
  "version": "0.2.0",
  "description": "Gradio component for displaying PDFs",
  "type": "module",
  "author": "",
  "license": "ISC",
  "private": false,
  "main_changeset": true,
  "exports": {
    ".": "./Index.svelte",
    "./example": "./Example.svelte",
    "./package.json": "./package.json"
  },
  "devDependencies": {
    "pdfjs-dist": "3.11.174"
  },
  "dependencies": {
    "@gradio/atoms": "0.2.0",
    "@gradio/statustracker": "0.3.0",
    "@gradio/utils": "0.2.0",
    "@gradio/client": "0.7.1",
    "@gradio/upload": "0.3.2",
    "@gradio/icons": "0.2.0",
    "@gradio/button": "0.2.3",
    "pdfjs-dist": "3.11.174"
  }
}
```


Tip: Running `npm install` will install the latest version of the package available. You can install a specific version with `npm install package@<version>`.  You can find all of the gradio javascript package documentation [here](https://www.gradio.app/main/docs/js). It is recommended you use the same versions as me as the API can change.

Navigate to `Index.svelte` and delete mentions of `JSONView`

```ts
import { JsonView } from "@zerodevx/svelte-json-view";
```

```svelte
<JsonView json={value} />
```

## Step 3: Frontend - Launching the Dev Server

Run the `dev` command to launch the development server.
This will open the demo in `demo/app.py` in an environment where changes to the `frontend` and `backend` directories will reflect instantaneously in the launched app.

After launching the dev server, you should see a link printed to your console that says `Frontend Server (Go here): ... `.
 
![](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/dev_server_terminal.png)

You should see the following:

![](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/frontend_start.png)


Its not impressive yet but we're ready to start coding!

## Step 4: Frontend - The basic skeleton

We're going to start off by first writing the skeleton of our frontend and then adding the pdf rendering logic.
Add the following imports and expose the following properties to the top of your file in the `<script>` tag.
You may get some warnings from your code editor that some props are not used.
That's ok.

```ts
    import { tick } from "svelte";
    import type { Gradio } from "@gradio/utils";
    import { Block, BlockLabel } from "@gradio/atoms";
    import { File } from "@gradio/icons";
    import { StatusTracker } from "@gradio/statustracker";
    import type { LoadingStatus } from "@gradio/statustracker";
    import type { FileData } from "@gradio/client";
    import { Upload, ModifyUpload } from "@gradio/upload";

	export let elem_id = "";
	export let elem_classes: string[] = [];
	export let visible = true;
	export let value: FileData | null = null;
	export let container = true;
	export let scale: number | null = null;
	export let root: string;
	export let height: number | null = 500;
	export let label: string;
	export let proxy_url: string;
	export let min_width: number | undefined = undefined;
	export let loading_status: LoadingStatus;
	export let gradio: Gradio<{
		change: never;
		upload: never;
	}>;

    let _value = value;
    let old_value = _value;
```


Tip: The `gradio`` object passed in here contains some metadata about the application as well as some utility methods. One of these utilities is a dispatch method. We want to dispatch change and upload events whenever our PDF is changed or updated. This line provides type hints that these are the only events we will be dispatching.

We want our frontend component to let users upload a PDF document if there isn't one already loaded.
If it is loaded, we want to display it underneath a "clear" button that lets our users upload a new document. 
We're going to use the `Upload` and `ModifyUpload` components that come with the `@gradio/upload` package to do this.
Underneath the `</script>` tag, delete all the current code and add the following:

```svelte
<Block {visible} {elem_id} {elem_classes} {container} {scale} {min_width}>
    {#if loading_status}
        <StatusTracker
            autoscroll={gradio.autoscroll}
            i18n={gradio.i18n}
            {...loading_status}
        />
    {/if}
    <BlockLabel
        show_label={label !== null}
        Icon={File}
        float={value === null}
        label={label || "File"}
    />
    {#if _value}
        <ModifyUpload i18n={gradio.i18n} absolute />
    {:else}
        <Upload
            filetype={"application/pdf"}
            file_count="single"
            {root}
        >
            Upload your PDF
        </Upload>
    {/if}
</Block>
```

You should see the following when you navigate to your app after saving your current changes:

![](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/frontend_1.png)

## Step 5: Frontend - Nicer Upload Text

The `Upload your PDF` text looks a bit small and barebones. 
Lets customize it!

Create a new file called `PdfUploadText.svelte` and copy the following code.
Its creating a new div to display our "upload text" with some custom styling.

Tip: Notice that we're leveraging Gradio core's existing css variables here: `var(--size-60)` and `var(--body-text-color-subdued)`. This allows our component to work nicely in light mode and dark mode, as well as with Gradio's built-in themes.


```svelte
<script lang="ts">
	import { Upload as UploadIcon } from "@gradio/icons";
	export let hovered = false;

</script>

<div class="wrap">
	<span class="icon-wrap" class:hovered><UploadIcon /> </span>
    Drop PDF
    <span class="or">- or -</span>
    Click to Upload
</div>

<style>
	.wrap {
		display: flex;
		flex-direction: column;
		justify-content: center;
		align-items: center;
		min-height: var(--size-60);
		color: var(--block-label-text-color);
		line-height: var(--line-md);
		height: 100%;
		padding-top: var(--size-3);
	}

	.or {
		color: var(--body-text-color-subdued);
		display: flex;
	}

	.icon-wrap {
		width: 30px;
		margin-bottom: var(--spacing-lg);
	}

	@media (--screen-md) {
		.wrap {
			font-size: var(--text-lg);
		}
	}

	.hovered {
		color: var(--color-accent);
	}
</style>
```

Now import `PdfUploadText.svelte` in your `<script>` and pass it to the `Upload` component!

```svelte
	import PdfUploadText from "./PdfUploadText.svelte";

...

    <Upload
        filetype={"application/pdf"}
        file_count="single"
        {root}
    >
        <PdfUploadText />
    </Upload>
```

After saving your code, the frontend should now look like this:

![](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/better_upload.png)

## Step 6: PDF Rendering logic

This is the most advanced javascript part.
It took me a while to figure it out!
Do not worry if you have trouble, the important thing is to not be discouraged ðŸ’ª
Ask for help in the gradio [discord](https://discord.gg/hugging-face-879548962464493619) if you need and ask for help.

With that out of the way, let's start off by importing `pdfjs` and loading the code of the pdf worker from the mozilla cdn.

```ts
	import pdfjsLib from "pdfjs-dist";
    ...
    pdfjsLib.GlobalWorkerOptions.workerSrc =  "https://cdn.bootcss.com/pdf.js/3.11.174/pdf.worker.js";
```

Also create the following variables:

```ts
    let pdfDoc;
    let numPages = 1;
    let currentPage = 1;
    let canvasRef;
```

Now, we will use `pdfjs` to render a given page of the PDF onto an `html` document.
Add the following code to `Index.svelte`:

```ts
    async function get_doc(value: FileData) {
        const loadingTask = pdfjsLib.getDocument(value.url);
        pdfDoc = await loadingTask.promise;
        numPages = pdfDoc.numPages;
        render_page();
    }

    function render_page() {
    // Render a specific page of the PDF onto the canvas
        pdfDoc.getPage(currentPage).then(page => {
            const ctx  = canvasRef.getContext('2d')
            ctx.clearRect(0, 0, canvasRef.width, canvasRef.height);
            let viewport = page.getViewport({ scale: 1 });
            let scale = height / viewport.height;
            viewport = page.getViewport({ scale: scale });

            const renderContext = {
                canvasContext: ctx,
                viewport,
            };
            canvasRef.width = viewport.width;
            canvasRef.height = viewport.height;
            page.render(renderContext);
        });
    }

    // If the value changes, render the PDF of the currentPage
    $: if(JSON.stringify(old_value) != JSON.stringify(_value)) {
        if (_value){
            get_doc(_value);
        }
        old_value = _value;
        gradio.dispatch("change");
    }
```


Tip: The `$:` syntax in svelte is how you declare statements to be reactive. Whenever any of the inputs of the statement change, svelte will automatically re-run that statement.

Now place the `canvas` underneath the `ModifyUpload` component:

```svelte
<div class="pdf-canvas" style="height: {height}px">
    <canvas bind:this={canvasRef}></canvas>
</div>
```

And add the following styles to the `<style>` tag:

```svelte
<style>
    .pdf-canvas {
        display: flex;
        justify-content: center;
        align-items: center;
    }
</style>
```

## Step 7: Handling The File Upload And Clear

Now for the fun part - actually rendering the PDF when the file is uploaded!
Add the following functions to the `<script>` tag:

```ts
    async function handle_clear() {
        _value = null;
        await tick();
        gradio.dispatch("change");
    }

    async function handle_upload({detail}: CustomEvent<FileData>): Promise<void> {
        value = detail;
        await tick();
        gradio.dispatch("change");
        gradio.dispatch("upload");
    }
```


Tip: The `gradio.dispatch` method is actually what is triggering the `change` or `upload` events in the backend. For every event defined in the component's backend, we will explain how to do this in Step 9, there must be at least one `gradio.dispatch("<event-name>")` call. These are called `gradio` events and they can be listended from the entire Gradio application. You can dispatch a built-in `svelte` event with the `dispatch` function. These events can only be listened to from the component's direct parent. Learn about svelte events from the [official documentation](https://learn.svelte.dev/tutorial/component-events).

Now we will run these functions whenever the `Upload` component uploads a file and whenever the `ModifyUpload` component clears the current file. The `<Upload>` component dispatches a `load` event with a payload of type `FileData` corresponding to the uploaded file. The `on:load` syntax tells `Svelte` to automatically run this function in response to the event.

```svelte
    <ModifyUpload i18n={gradio.i18n} on:clear={handle_clear} absolute />
    
    ...
    
    <Upload
        on:load={handle_upload}
        filetype={"application/pdf"}
        file_count="single"
        {root}
    >
        <PdfUploadText/>
    </Upload>
```

Congratulations! You have a working pdf uploader!

![upload-gif](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/pdf_component_gif_docs.gif)

## Step 8: Adding buttons to navigate pages

If a user uploads a PDF document with multiple pages, they will only be able to see the first one.
Let's add some buttons to help them navigate the page.
We will use the `BaseButton` from `@gradio/button` so that they look like regular Gradio buttons.

Import the `BaseButton` and add the following functions that will render the next and previous page of the PDF.

```ts
    import { BaseButton } from "@gradio/button";

    ...

    function next_page() {
        if (currentPage >= numPages) {
            return;
        }
        currentPage++;
        render_page();
    }

    function prev_page() {
        if (currentPage == 1) {
            return;
        }
        currentPage--;
        render_page();
    }
```

Now we will add them underneath the canvas in a separate `<div>`

```svelte
    ...

    <ModifyUpload i18n={gradio.i18n} on:clear={handle_clear} absolute />
    <div class="pdf-canvas" style="height: {height}px">
        <canvas bind:this={canvasRef}></canvas>
    </div>
    <div class="button-row">
        <BaseButton on:click={prev_page}>
            â¬…ï¸
        </BaseButton>
        <span class="page-count"> {currentPage} / {numPages} </span>
        <BaseButton on:click={next_page}>
            âž¡ï¸
        </BaseButton>
    </div>
    
    ...

<style>
    .button-row {
        display: flex;
        flex-direction: row;
        width: 100%;
        justify-content: center;
        align-items: center;
    }

    .page-count {
        margin: 0 10px;
        font-family: var(--font-mono);
    }
```

Congratulations! The frontend is almost complete ðŸŽ‰

![multipage-pdf-gif](https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/pdf_multipage.gif)

## Step 8.5: The Example view

We're going to want users of our component to get a preview of the PDF if its used as an `example` in a `gr.Interface` or `gr.Examples`.

To do so, we're going to add some of the pdf rendering logic in `Index.svelte` to `Example.svelte`.


```svelte
<script lang="ts">
	export let value: string;
	export let type: "gallery" | "table";
	export let selected = false;
	import pdfjsLib from "pdfjs-dist";
	pdfjsLib.GlobalWorkerOptions.workerSrc =  "https://cdn.bootcss.com/pdf.js/3.11.174/pdf.worker.js";
	
	let pdfDoc;
	let canvasRef;

	async function get_doc(url: string) {
		const loadingTask = pdfjsLib.getDocument(url);
		pdfDoc = await loadingTask.promise;
		renderPage();
		}

	function renderPage() {
		// Render a specific page of the PDF onto the canvas
			pdfDoc.getPage(1).then(page => {
				const ctx  = canvasRef.getContext('2d')
				ctx.clearRect(0, 0, canvasRef.width, canvasRef.height);
				
				const viewport = page.getViewport({ scale: 0.2 });
				
				const renderContext = {
					canvasContext: ctx,
					viewport
				};
				canvasRef.width = viewport.width;
				canvasRef.height = viewport.height;
				page.render(renderContext);
			});
		}
	
	$: get_doc(value);
</script>

<div
	class:table={type === "table"}
	class:gallery={type === "gallery"}
	class:selected
	style="justify-content: center; align-items: center; display: flex; flex-direction: column;"
>
	<canvas bind:this={canvasRef}></canvas>
</div>

<style>
	.gallery {
		padding: var(--size-1) var(--size-2);
	}
</style>
```


Tip: Exercise for the reader - reduce the code duplication between `Index.svelte` and `Example.svelte` ðŸ˜Š


You will not be able to render examples until we make some changes to the backend code in the next step!

## Step 9: The backend

The backend changes needed are smaller.
We're almost done!

What we're going to do is:
* Add `change` and `upload` events to our component.
* Add a `height` property to let users control the height of the PDF.
* Set the `data_model` of our component to be `FileData`. This is so that Gradio can automatically cache and safely serve any files that are processed by our component.
* Modify the `preprocess` method to return a string corresponding to the path of our uploaded PDF.
* Modify the `postprocess` to turn a path to a PDF created in an event handler to a `FileData`.

When all is said an done, your component's backend code should look like this:

```python
from __future__ import annotations
from typing import Any, Callable, TYPE_CHECKING

from gradio.components.base import Component
from gradio.data_classes import FileData
from gradio import processing_utils
if TYPE_CHECKING:
    from gradio.components import Timer

class PDF(Component):

    EVENTS = ["change", "upload"]

    data_model = FileData

    def __init__(self, value: Any = None, *,
                 height: int | None = None,
                 label: str | I18nData | None = None,
                 info: str | I18nData | None = None,
                 show_label: bool | None = None,
                 container: bool = True,
                 scale: int | None = None,
                 min_width: int | None = None,
                 interactive: bool | None = None,
                 visible: bool = True,
                 elem_id: str | None = None,
                 elem_classes: list[str] | str | None = None,
                 render: bool = True,
                 load_fn: Callable[..., Any] | None = None,
                 every: Timer | float | None = None):
        super().__init__(value, label=label, info=info,
                         show_label=show_label, container=container,
                         scale=scale, min_width=min_width,
                         interactive=interactive, visible=visible,
                         elem_id=elem_id, elem_classes=elem_classes,
                         render=render, load_fn=load_fn, every=every)
        self.height = height

    def preprocess(self, payload: FileData) -> str:
        return payload.path

    def postprocess(self, value: str | None) -> FileData:
        if not value:
            return None
        return FileData(path=value)

    def example_payload(self):
        return "https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/fw9.pdf"

    def example_value(self):
        return "https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/fw9.pdf"
```

## Step 10: Add a demo and publish!

To test our backend code, let's add a more complex demo that performs Document Question and Answering with huggingface transformers.

In our `demo` directory, create a `requirements.txt` file with the following packages

```
torch
transformers
pdf2image
pytesseract
```


Tip: Remember to install these yourself and restart the dev server! You may need to install extra non-python dependencies for `pdf2image`. See [here](https://pypi.org/project/pdf2image/). Feel free to write your own demo if you have trouble.


```python
import gradio as gr
from gradio_pdf import PDF
from pdf2image import convert_from_path
from transformers import pipeline
from pathlib import Path

dir_ = Path(__file__).parent

p = pipeline(
    "document-question-answering",
    model="impira/layoutlm-document-qa",
)

def qa(question: str, doc: str) -> str:
    img = convert_from_path(doc)[0]
    output = p(img, question)
    return sorted(output, key=lambda x: x["score"], reverse=True)[0]['answer']


demo = gr.Interface(
    qa,
    [gr.Textbox(label="Question"), PDF(label="Document")],
    gr.Textbox(),
)

demo.launch()
```

See our demo in action below!

<video autoplay muted loop>
  <source src="https://gradio-builds.s3.amazonaws.com/assets/pdf-guide/PDFDemo.mov" type="video/mp4" />
</video>

Finally lets build our component with `gradio cc build` and publish it with the `gradio cc publish` command!
This will guide you through the process of uploading your component to [PyPi](https://pypi.org/) and [HuggingFace Spaces](https://huggingface.co/spaces).


Tip: You may need to add the following lines to the `Dockerfile` of your HuggingFace Space.

```Dockerfile
RUN mkdir -p /tmp/cache/
RUN chmod a+rwx -R /tmp/cache/
RUN apt-get update && apt-get install -y poppler-utils tesseract-ocr

ENV TRANSFORMERS_CACHE=/tmp/cache/
```

## Conclusion

In order to use our new component in **any** gradio 4.0 app, simply install it with pip, e.g. `pip install gradio-pdf`. Then you can use it like the built-in `gr.File()` component (except that it will only accept and display PDF files).

Here is a simple demo with the Blocks api:

```python
import gradio as gr
from gradio_pdf import PDF

with gr.Blocks() as demo:
    pdf = PDF(label="Upload a PDF", interactive=True)
    name = gr.Textbox()
    pdf.upload(lambda f: f, pdf, name)

demo.launch()
```


I hope you enjoyed this tutorial!
The complete source code for our component is [here](https://huggingface.co/spaces/freddyaboulton/gradio_pdf/tree/main/src).
Please don't hesitate to reach out to the gradio community on the [HuggingFace Discord](https://discord.gg/hugging-face-879548962464493619) if you get stuck.

---

<!-- Source: guides/09_gradio-clients-and-lite/07_fastapi-app-with-the-gradio-client.md -->
# Building a Web App with the Gradio Python Client

Tags: CLIENT, API, WEB APP

In this blog post, we will demonstrate how to use the `gradio_client` [Python library](getting-started-with-the-python-client/), which enables developers to make requests to a Gradio app programmatically, by creating an end-to-end example web app using FastAPI. The web app we will be building is called "Acapellify," and it will allow users to upload video files as input and return a version of that video without instrumental music. It will also display a gallery of generated videos.

**Prerequisites**

Before we begin, make sure you are running Python 3.9 or later, and have the following libraries installed:

- `gradio_client`
- `fastapi`
- `uvicorn`

You can install these libraries from `pip`:

```bash
$ pip install gradio_client fastapi uvicorn
```

You will also need to have ffmpeg installed. You can check to see if you already have ffmpeg by running in your terminal:

```bash
$ ffmpeg version
```

Otherwise, install ffmpeg [by following these instructions](https://www.hostinger.com/tutorials/how-to-install-ffmpeg).

## Step 1: Write the Video Processing Function

Let's start with what seems like the most complex bit -- using machine learning to remove the music from a video.

Luckily for us, there's an existing Space we can use to make this process easier: [https://huggingface.co/spaces/abidlabs/music-separation](https://huggingface.co/spaces/abidlabs/music-separation). This Space takes an audio file and produces two separate audio files: one with the instrumental music and one with all other sounds in the original clip. Perfect to use with our client!

Open a new Python file, say `main.py`, and start by importing the `Client` class from `gradio_client` and connecting it to this Space:

```py
from gradio_client import Client, handle_file

client = Client("abidlabs/music-separation")

def acapellify(audio_path):
    result = client.predict(handle_file(audio_path), api_name="/predict")
    return result[0]
```

That's all the code that's needed -- notice that the API endpoints returns two audio files (one without the music, and one with just the music) in a list, and so we just return the first element of the list.

---

**Note**: since this is a public Space, there might be other users using this Space as well, which might result in a slow experience. You can duplicate this Space with your own [Hugging Face token](https://huggingface.co/settings/tokens) and create a private Space that only you have will have access to and bypass the queue. To do that, simply replace the first two lines above with:

```py
from gradio_client import Client

client = Client.duplicate("abidlabs/music-separation", token=YOUR_HF_TOKEN)
```

Everything else remains the same!

---

Now, of course, we are working with video files, so we first need to extract the audio from the video files. For this, we will be using the `ffmpeg` library, which does a lot of heavy lifting when it comes to working with audio and video files. The most common way to use `ffmpeg` is through the command line, which we'll call via Python's `subprocess` module:

Our video processing workflow will consist of three steps:

1. First, we start by taking in a video filepath and extracting the audio using `ffmpeg`.
2. Then, we pass in the audio file through the `acapellify()` function above.
3. Finally, we combine the new audio with the original video to produce a final acapellified video.

Here's the complete code in Python, which you can add to your `main.py` file:

```python
import subprocess

def process_video(video_path):
    old_audio = os.path.basename(video_path).split(".")[0] + ".m4a"
    subprocess.run(['ffmpeg', '-y', '-i', video_path, '-vn', '-acodec', 'copy', old_audio])

    new_audio = acapellify(old_audio)

    new_video = f"acap_{video_path}"
    subprocess.call(['ffmpeg', '-y', '-i', video_path, '-i', new_audio, '-map', '0:v', '-map', '1:a', '-c:v', 'copy', '-c:a', 'aac', '-strict', 'experimental', f"static/{new_video}"])
    return new_video
```

You can read up on [ffmpeg documentation](https://ffmpeg.org/ffmpeg.html) if you'd like to understand all of the command line parameters, as they are beyond the scope of this tutorial.

## Step 2: Create a FastAPI app (Backend Routes)

Next up, we'll create a simple FastAPI app. If you haven't used FastAPI before, check out [the great FastAPI docs](https://fastapi.tiangolo.com/). Otherwise, this basic template, which we add to `main.py`, will look pretty familiar:

```python
import os
from fastapi import FastAPI, File, UploadFile, Request
from fastapi.responses import HTMLResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates

app = FastAPI()
os.makedirs("static", exist_ok=True)
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

videos = []

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    return templates.TemplateResponse(
        "home.html", {"request": request, "videos": videos})

@app.post("/uploadvideo/")
async def upload_video(video: UploadFile = File(...)):
    video_path = video.filename
    with open(video_path, "wb+") as fp:
        fp.write(video.file.read())

    new_video = process_video(video.filename)
    videos.append(new_video)
    return RedirectResponse(url='/', status_code=303)
```

In this example, the FastAPI app has two routes: `/` and `/uploadvideo/`.

The `/` route returns an HTML template that displays a gallery of all uploaded videos.

The `/uploadvideo/` route accepts a `POST` request with an `UploadFile` object, which represents the uploaded video file. The video file is "acapellified" via the `process_video()` method, and the output video is stored in a list which stores all of the uploaded videos in memory.

Note that this is a very basic example and if this were a production app, you will need to add more logic to handle file storage, user authentication, and security considerations.

## Step 3: Create a FastAPI app (Frontend Template)

Finally, we create the frontend of our web application. First, we create a folder called `templates` in the same directory as `main.py`. We then create a template, `home.html` inside the `templates` folder. Here is the resulting file structure:

```csv
â”œâ”€â”€ main.py
â”œâ”€â”€ templates
â”‚   â””â”€â”€ home.html
```

Write the following as the contents of `home.html`:

```html
&lt;!DOCTYPE html> &lt;html> &lt;head> &lt;title>Video Gallery&lt;/title>
&lt;style> body { font-family: sans-serif; margin: 0; padding: 0;
background-color: #f5f5f5; } h1 { text-align: center; margin-top: 30px;
margin-bottom: 20px; } .gallery { display: flex; flex-wrap: wrap;
justify-content: center; gap: 20px; padding: 20px; } .video { border: 2px solid
#ccc; box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2); border-radius: 5px; overflow:
hidden; width: 300px; margin-bottom: 20px; } .video video { width: 100%; height:
200px; } .video p { text-align: center; margin: 10px 0; } form { margin-top:
20px; text-align: center; } input[type="file"] { display: none; } .upload-btn {
display: inline-block; background-color: #3498db; color: #fff; padding: 10px
20px; font-size: 16px; border: none; border-radius: 5px; cursor: pointer; }
.upload-btn:hover { background-color: #2980b9; } .file-name { margin-left: 10px;
} &lt;/style> &lt;/head> &lt;body> &lt;h1>Video Gallery&lt;/h1> {% if videos %}
&lt;div class="gallery"> {% for video in videos %} &lt;div class="video">
&lt;video controls> &lt;source src="{{ url_for('static', path=video) }}"
type="video/mp4"> Your browser does not support the video tag. &lt;/video>
&lt;p>{{ video }}&lt;/p> &lt;/div> {% endfor %} &lt;/div> {% else %} &lt;p>No
videos uploaded yet.&lt;/p> {% endif %} &lt;form action="/uploadvideo/"
method="post" enctype="multipart/form-data"> &lt;label for="video-upload"
class="upload-btn">Choose video file&lt;/label> &lt;input type="file"
name="video" id="video-upload"> &lt;span class="file-name">&lt;/span> &lt;button
type="submit" class="upload-btn">Upload&lt;/button> &lt;/form> &lt;script> //
Display selected file name in the form const fileUpload =
document.getElementById("video-upload"); const fileName =
document.querySelector(".file-name"); fileUpload.addEventListener("change", (e)
=> { fileName.textContent = e.target.files[0].name; }); &lt;/script> &lt;/body>
&lt;/html>
```

## Step 4: Run your FastAPI app

Finally, we are ready to run our FastAPI app, powered by the Gradio Python Client!

Open up a terminal and navigate to the directory containing `main.py`. Then run the following command in the terminal:

```bash
$ uvicorn main:app
```

You should see an output that looks like this:

```csv
Loaded as API: https://abidlabs-music-separation.hf.space âœ”
INFO:     Started server process [1360]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

And that's it! Start uploading videos and you'll get some "acapellified" videos in response (might take seconds to minutes to process depending on the length of your videos). Here's how the UI looks after uploading two videos:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/acapellify.png)

If you'd like to learn more about how to use the Gradio Python Client in your projects, [read the dedicated Guide](/guides/getting-started-with-the-python-client/).

---

<!-- Source: guides/03_building-with-blocks/08_using-blocks-like-functions.md -->
# Using Gradio Blocks Like Functions

Tags: TRANSLATION, HUB, SPACES

**Prerequisite**: This Guide builds on the Blocks Introduction. Make sure to [read that guide first](https://gradio.app/blocks-and-event-listeners).

## Introduction

Did you know that apart from being a full-stack machine learning demo, a Gradio Blocks app is also a regular-old python function!?

This means that if you have a gradio Blocks (or Interface) app called `demo`, you can use `demo` like you would any python function.

So doing something like `output = demo("Hello", "friend")` will run the first event defined in `demo` on the inputs "Hello" and "friend" and store it
in the variable `output`.

If I put you to sleep ðŸ¥±, please bear with me! By using apps like functions, you can seamlessly compose Gradio apps.
The following section will show how.

## Treating Blocks like functions

Let's say we have the following demo that translates english text to german text.

$code_english_translator

I already went ahead and hosted it in Hugging Face spaces at [gradio/english_translator](https://huggingface.co/spaces/gradio/english_translator).

You can see the demo below as well:

$demo_english_translator

Now, let's say you have an app that generates english text, but you wanted to additionally generate german text.

You could either:

1. Copy the source code of my english-to-german translation and paste it in your app.

2. Load my english-to-german translation in your app and treat it like a normal python function.

Option 1 technically always works, but it often introduces unwanted complexity.

Option 2 lets you borrow the functionality you want without tightly coupling our apps.

All you have to do is call the `Blocks.load` class method in your source file.
After that, you can use my translation app like a regular python function!

The following code snippet and demo shows how to use `Blocks.load`.

Note that the variable `english_translator` is my english to german app, but its used in `generate_text` like a regular function.

$code_generate_english_german

$demo_generate_english_german

## How to control which function in the app to use

If the app you are loading defines more than one function, you can specify which function to use
with the `fn_index` and `api_name` parameters.

In the code for our english to german demo, you'll see the following line:

```python
translate_btn.click(translate, inputs=english, outputs=german, api_name="translate-to-german")
```

The `api_name` gives this function a unique name in our app. You can use this name to tell gradio which
function in the upstream space you want to use:

```python
english_generator(text, api_name="translate-to-german")[0]["generated_text"]
```

You can also use the `fn_index` parameter.
Imagine my app also defined an english to spanish translation function.
In order to use it in our text generation app, we would use the following code:

```python
english_generator(text, fn_index=1)[0]["generated_text"]
```

Functions in gradio spaces are zero-indexed, so since the spanish translator would be the second function in my space,
you would use index 1.

## Parting Remarks

We showed how treating a Blocks app like a regular python helps you compose functionality across different apps.
Any Blocks app can be treated like a function, but a powerful pattern is to `load` an app hosted on
[Hugging Face Spaces](https://huggingface.co/spaces) prior to treating it like a function in your own app.
You can also load models hosted on the [Hugging Face Model Hub](https://huggingface.co/models) - see the [Using Hugging Face Integrations](/using_hugging_face_integrations) guide for an example.

Happy building! âš’ï¸

---

<!-- Source: guides/04_additional-features/08_file-access.md -->
# Security and File Access

Sharing your Gradio app with others (by hosting it on Spaces, on your own server, or through temporary share links) **exposes** certain files on your machine to the internet. Files that are exposed can be accessed at a special URL:

```bash
http://<your-gradio-app-url>/gradio_api/file=<local-file-path>
```

This guide explains which files are exposed as well as some best practices for making sure the files on your machine are secure.

## Files Gradio allows users to access 

- **1. Static files**. You can designate static files or directories using the `gr.set_static_paths` function. Static files  are not be copied to the Gradio cache (see below) and will be served directly from your computer. This can help save disk space and reduce the time your app takes to launch but be mindful of possible security implications as any static files are accessible to all useres of your Gradio app.

- **2. Files in the `allowed_paths` parameter in `launch()`**. This parameter allows you to pass in a list of additional directories or exact filepaths you'd like to allow users to have access to. (By default, this parameter is an empty list).

- **3. Files in Gradio's cache**. After you launch your Gradio app, Gradio copies certain files into a temporary cache and makes these files accessible to users. Let's unpack this in more detail below.


## The Gradio cache

First, it's important to understand why Gradio has a cache at all. Gradio copies files to a cache directory before returning them to the frontend. This prevents files from being overwritten by one user while they are still needed by another user of your application. For example, if your prediction function returns a video file, then Gradio will move that video to the cache after your prediction function runs and returns a URL the frontend can use to show the video. Any file in the cache is available via URL to all users of your running application.

Tip: You can customize the location of the cache by setting the `GRADIO_TEMP_DIR` environment variable to an absolute path, such as `/home/usr/scripts/project/temp/`. 

### Files Gradio moves to the cache

Gradio moves three kinds of files into the cache

1. Files specified by the developer before runtime, e.g. cached examples, default values of components, or files passed into parameters such as the `avatar_images` of `gr.Chatbot`

2. File paths returned by a prediction function in your Gradio application, if they ALSO meet one of the conditions below:

* It is in the `allowed_paths` parameter of the `Blocks.launch` method.
* It is in the current working directory of the python interpreter.
* It is in the temp directory obtained by `tempfile.gettempdir()`.

**Note:** files in the current working directory whose name starts with a period (`.`) will not be moved to the cache, even if they are returned from a prediction function, since they often contain sensitive information. 

If none of these criteria are met, the prediction function that is returning that file will raise an exception instead of moving the file to cache. Gradio performs this check so that arbitrary files on your machine cannot be accessed.

3. Files uploaded by a user to your Gradio app (e.g. through the `File` or `Image` input components).

Tip: If at any time Gradio blocks a file that you would like it to process, add its path to the `allowed_paths` parameter.

## The files Gradio will not allow others to access

While running, Gradio apps will NOT ALLOW users to access:

- **Files that you explicitly block via the `blocked_paths` parameter in `launch()`**. You can pass in a list of additional directories or exact filepaths to the `blocked_paths` parameter in `launch()`. This parameter takes precedence over the files that Gradio exposes by default, or by the `allowed_paths` parameter or the `gr.set_static_paths` function.

- **Any other paths on the host machine**. Users should NOT be able to access other arbitrary paths on the host.

## Uploading Files

Sharing your Gradio application will also allow users to upload files to your computer or server. You can set a maximum file size for uploads to prevent abuse and to preserve disk space. You can do this with the `max_file_size` parameter of `.launch`. For example, the following two code snippets limit file uploads to 5 megabytes per file.

```python
import gradio as gr

demo = gr.Interface(lambda x: x, "image", "image")

demo.launch(max_file_size="5mb")
# or
demo.launch(max_file_size=5 * gr.FileSize.MB)
```

## Best Practices

* Set a `max_file_size` for your application.
* Do not return arbitrary user input from a function that is connected to a file-based output component (`gr.Image`, `gr.File`, etc.). For example, the following interface would allow anyone to move an arbitrary file in your local directory to the cache: `gr.Interface(lambda s: s, "text", "file")`. This is because the user input is treated as an arbitrary file path. 
* Make `allowed_paths` as small as possible. If a path in `allowed_paths` is a directory, any file within that directory can be accessed. Make sure the entires of `allowed_paths` only contains files related to your application.
* Run your gradio application from the same directory the application file is located in. This will narrow the scope of files Gradio will be allowed to move into the cache. For example, prefer `python app.py` to `python Users/sources/project/app.py`.


## Example: Accessing local files
Both `gr.set_static_paths` and the `allowed_paths` parameter in launch expect absolute paths. Below is a minimal example to display a local `.png` image file in an HTML block.

```txt
â”œâ”€â”€ assets
â”‚   â””â”€â”€ logo.png
â””â”€â”€ app.py
```
For the example directory structure, `logo.png` and any other files in the `assets` folder can be accessed from your Gradio app in `app.py` as follows:

```python
from pathlib import Path

import gradio as gr

gr.set_static_paths(paths=[Path.cwd().absolute()/"assets"])

with gr.Blocks() as demo:
    gr.HTML("<img src='/gradio_api/file=assets/logo.png'>")

demo.launch()
```

---

<!-- Source: guides/05_chatbots/08_creating-a-website-widget-from-a-gradio-chatbot.md -->
# ðŸš€ Creating a Website Chat Widget with Gradio ðŸš€

Tags: CHAT, DEPLOY, WEB

You can make your Gradio Chatbot available as an embedded chat widget on your website, similar to popular customer service widgets like Intercom. This is particularly useful for:

- Adding AI assistance to your documentation pages
- Providing interactive help on your portfolio or product website
- Creating a custom chatbot interface for your Gradio app

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/Screen%20Recording%202024-12-19%20at%203.32.46%E2%80%AFPM.gif)

## How does it work?

The chat widget appears as a small button in the corner of your website. When clicked, it opens a chat interface that communicates with your Gradio app via the JavaScript Client API. Users can ask questions and receive responses directly within the widget.


## Prerequisites

* A running Gradio app (local or on Hugging Face Spaces). In this example, we'll use the [Gradio Playground Space](https://huggingface.co/spaces/abidlabs/gradio-playground-bot), which helps generate code for Gradio apps based on natural language descriptions.

### 1. Create and Style the Chat Widget

First, add this HTML and CSS to your website:

```html
<div id="chat-widget" class="chat-widget">
    <button id="chat-toggle" class="chat-toggle">ðŸ’¬</button>
    <div id="chat-container" class="chat-container hidden">
        <div id="chat-header">
            <h3>Gradio Assistant</h3>
            <button id="close-chat">Ã—</button>
        </div>
        <div id="chat-messages"></div>
        <div id="chat-input-area">
            <input type="text" id="chat-input" placeholder="Ask a question...">
            <button id="send-message">Send</button>
        </div>
    </div>
</div>

<style>
.chat-widget {
    position: fixed;
    bottom: 20px;
    right: 20px;
    z-index: 1000;
}

.chat-toggle {
    width: 50px;
    height: 50px;
    border-radius: 50%;
    background: #007bff;
    border: none;
    color: white;
    font-size: 24px;
    cursor: pointer;
}

.chat-container {
    position: fixed;
    bottom: 80px;
    right: 20px;
    width: 300px;
    height: 400px;
    background: white;
    border-radius: 10px;
    box-shadow: 0 0 10px rgba(0,0,0,0.1);
    display: flex;
    flex-direction: column;
}

.chat-container.hidden {
    display: none;
}

#chat-header {
    padding: 10px;
    background: #007bff;
    color: white;
    border-radius: 10px 10px 0 0;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

#chat-messages {
    flex-grow: 1;
    overflow-y: auto;
    padding: 10px;
}

#chat-input-area {
    padding: 10px;
    border-top: 1px solid #eee;
    display: flex;
}

#chat-input {
    flex-grow: 1;
    padding: 8px;
    border: 1px solid #ddd;
    border-radius: 4px;
    margin-right: 8px;
}

.message {
    margin: 8px 0;
    padding: 8px;
    border-radius: 4px;
}

.user-message {
    background: #e9ecef;
    margin-left: 20px;
}

.bot-message {
    background: #f8f9fa;
    margin-right: 20px;
}
</style>
```

### 2. Add the JavaScript

Then, add the following JavaScript code (which uses the Gradio JavaScript Client to connect to the Space) to your website by including this in the `<head>` section of your website:

```html
<script type="module">
    import { Client } from "https://cdn.jsdelivr.net/npm/@gradio/client/dist/index.min.js";
    
    async function initChatWidget() {
        const client = await Client.connect("https://abidlabs-gradio-playground-bot.hf.space");
        
        const chatToggle = document.getElementById('chat-toggle');
        const chatContainer = document.getElementById('chat-container');
        const closeChat = document.getElementById('close-chat');
        const chatInput = document.getElementById('chat-input');
        const sendButton = document.getElementById('send-message');
        const messagesContainer = document.getElementById('chat-messages');
    
        chatToggle.addEventListener('click', () => {
            chatContainer.classList.remove('hidden');
        });
    
        closeChat.addEventListener('click', () => {
            chatContainer.classList.add('hidden');
        });
    
        async function sendMessage() {
            const userMessage = chatInput.value.trim();
            if (!userMessage) return;

            appendMessage(userMessage, 'user');
            chatInput.value = '';

            try {
                const result = await client.predict("/chat", {
                    message: {"text": userMessage, "files": []}
                });
                const message = result.data[0];
                console.log(result.data[0]);
                const botMessage = result.data[0].join('\n');
                appendMessage(botMessage, 'bot');
            } catch (error) {
                console.error('Error:', error);
                appendMessage('Sorry, there was an error processing your request.', 'bot');
            }
        }
    
        function appendMessage(text, sender) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${sender}-message`;
            
            if (sender === 'bot') {
                messageDiv.innerHTML = marked.parse(text);
            } else {
                messageDiv.textContent = text;
            }
            
            messagesContainer.appendChild(messageDiv);
            messagesContainer.scrollTop = messagesContainer.scrollHeight;
        }
    
        sendButton.addEventListener('click', sendMessage);
        chatInput.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') sendMessage();
        });
    }
    
    initChatWidget();
</script>
```

### 3. That's it!

Your website now has a chat widget that connects to your Gradio app! Users can click the chat button to open the widget and start interacting with your app.

### Customization

You can customize the appearance of the widget by modifying the CSS. Some ideas:
- Change the colors to match your website's theme
- Adjust the size and position of the widget
- Add animations for opening/closing
- Modify the message styling

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/Screen%20Recording%202024-12-19%20at%203.32.46%E2%80%AFPM.gif)

If you build a website widget from a Gradio app, feel free to share it on X and tag [the Gradio account](https://x.com/Gradio), and we are happy to help you amplify!

---

<!-- Source: guides/08_custom-components/08_multimodal-chatbot-part1.md -->
# Build a Custom Multimodal Chatbot - Part 1

This is the first in a two part series where we build a custom Multimodal Chatbot component.
In part 1, we will modify the Gradio Chatbot component to display text and media files (video, audio, image) in the same message.
In part 2, we will build a custom Textbox component that will be able to send multimodal messages (text and media files) to the chatbot.

You can follow along with the author of this post as he implements the chatbot component in the following YouTube video!

<iframe width="560" height="315" src="https://www.youtube.com/embed/IVJkOHTBPn0?si=bs-sBv43X-RVA8ly" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

Here's a preview of what our multimodal chatbot component will look like:

![MultiModal Chatbot](https://gradio-builds.s3.amazonaws.com/assets/MultimodalChatbot.png)


## Part 1 - Creating our project

For this demo we will be tweaking the existing Gradio `Chatbot` component to display text and media files in the same message.
Let's create a new custom component directory by templating off of the `Chatbot` component source code.

```bash
gradio cc create MultimodalChatbot --template Chatbot
```

And we're ready to go!

Tip: Make sure to modify the `Author` key in the `pyproject.toml` file.

## Part 2a - The backend data_model

Open up the `multimodalchatbot.py` file in your favorite code editor and let's get started modifying the backend of our component.

The first thing we will do is create the `data_model` of our component.
The `data_model` is the data format that your python component will receive and send to the javascript client running the UI.
You can read more about the `data_model` in the [backend guide](./backend).

For our component, each chatbot message will consist of two keys: a `text` key that displays the text message and an optional list of media files that can be displayed underneath the text.

Import the `FileData` and `GradioModel` classes from `gradio.data_classes` and modify the existing `ChatbotData` class to look like the following:

```python
class FileMessage(GradioModel):
    file: FileData
    alt_text: Optional[str] = None


class MultimodalMessage(GradioModel):
    text: Optional[str] = None
    files: Optional[List[FileMessage]] = None


class ChatbotData(GradioRootModel):
    root: List[Tuple[Optional[MultimodalMessage], Optional[MultimodalMessage]]]


class MultimodalChatbot(Component):
    ...
    data_model = ChatbotData
```


Tip: The `data_model`s are implemented using `Pydantic V2`. Read the documentation [here](https://docs.pydantic.dev/latest/).

We've done the hardest part already!

## Part 2b - The pre and postprocess methods

For the `preprocess` method, we will keep it simple and pass a list of `MultimodalMessage`s to the python functions that use this component as input. 
This will let users of our component access the chatbot data with `.text` and `.files` attributes.
This is a design choice that you can modify in your implementation!
We can return the list of messages with the `root` property of the `ChatbotData` like so:

```python
def preprocess(
    self,
    payload: ChatbotData | None,
) -> List[MultimodalMessage] | None:
    if payload is None:
        return payload
    return payload.root
```


Tip: Learn about the reasoning behind the `preprocess` and `postprocess` methods in the [key concepts guide](./key-component-concepts)

In the `postprocess` method we will coerce each message returned by the python function to be a `MultimodalMessage` class. 
We will also clean up any indentation in the `text` field so that it can be properly displayed as markdown in the frontend.

We can leave the `postprocess` method as is and modify the `_postprocess_chat_messages`

```python
def _postprocess_chat_messages(
    self, chat_message: MultimodalMessage | dict | None
) -> MultimodalMessage | None:
    if chat_message is None:
        return None
    if isinstance(chat_message, dict):
        chat_message = MultimodalMessage(**chat_message)
    chat_message.text = inspect.cleandoc(chat_message.text or "")
    for file_ in chat_message.files:
        file_.file.mime_type = client_utils.get_mimetype(file_.file.path)
    return chat_message
```

Before we wrap up with the backend code, let's modify the `example_value` and `example_payload` method to return a valid dictionary representation of the `ChatbotData`:

```python
def example_value(self) -> Any:
    return [[{"text": "Hello!", "files": []}, None]]

def example_payload(self) -> Any:
    return [[{"text": "Hello!", "files": []}, None]]
```

Congrats - the backend is complete!

## Part 3a - The Index.svelte file

The frontend for the `Chatbot` component is divided into two parts - the `Index.svelte` file and the `shared/Chatbot.svelte` file.
The `Index.svelte` file applies some processing to the data received from the server and then delegates the rendering of the conversation to the `shared/Chatbot.svelte` file.
First we will modify the `Index.svelte` file to apply processing to the new data type the backend will return.

Let's begin by porting our custom types  from our python `data_model` to typescript.
Open `frontend/shared/utils.ts` and add the following type definitions at the top of the file:

```ts
export type FileMessage = {
	file: FileData;
	alt_text?: string;
};


export type MultimodalMessage = {
	text: string;
	files?: FileMessage[];
}
```

Now let's import them in `Index.svelte` and modify the type annotations for `value` and `_value`.

```ts
import type { FileMessage, MultimodalMessage } from "./shared/utils";

export let value: [
    MultimodalMessage | null,
    MultimodalMessage | null
][] = [];

let _value: [
    MultimodalMessage | null,
    MultimodalMessage | null
][];
```

We need to normalize each message to make sure each file has a proper URL to fetch its contents from.
We also need to format any embedded file links in the `text` key.
Let's add a `process_message` utility function and apply it whenever the `value` changes.

```ts
function process_message(msg: MultimodalMessage | null): MultimodalMessage | null {
    if (msg === null) {
        return msg;
    }
    msg.text = redirect_src_url(msg.text);
    msg.files = msg.files.map(normalize_messages);
    return msg;
}

$: _value = value
    ? value.map(([user_msg, bot_msg]) => [
            process_message(user_msg),
            process_message(bot_msg)
        ])
    : [];
```

## Part 3b - the Chatbot.svelte file

Let's begin similarly to the `Index.svelte` file and let's first modify the type annotations.
Import `Mulimodal` message at the top of the `<script>` section and use it to type the `value` and `old_value` variables.

```ts
import type { MultimodalMessage } from "./utils";

export let value:
    | [
            MultimodalMessage | null,
            MultimodalMessage | null
        ][]
    | null;
let old_value:
    | [
            MultimodalMessage | null,
            MultimodalMessage | null
        ][]
    | null = null;
```

We also need to modify the `handle_select` and `handle_like` functions:

```ts
function handle_select(
    i: number,
    j: number,
    message: MultimodalMessage | null
): void {
    dispatch("select", {
        index: [i, j],
        value: message
    });
}

function handle_like(
    i: number,
    j: number,
    message: MultimodalMessage | null,
    liked: boolean
): void {
    dispatch("like", {
        index: [i, j],
        value: message,
        liked: liked
    });
}
```

Now for the fun part, actually rendering the text and files in the same message!

You should see some code like the following that determines whether a file or a markdown message should be displayed depending on the type of the message:

```svelte
{#if typeof message === "string"}
    <Markdown
        {message}
        {latex_delimiters}
        {sanitize_html}
        {render_markdown}
        {line_breaks}
        on:load={scroll}
    />
{:else if message !== null && message.file?.mime_type?.includes("audio")}
    <audio
        data-testid="chatbot-audio"
        controls
        preload="metadata"
        ...
```

We will modify this code to always display the text message and then loop through the files and display all of them that are present:

```svelte
<Markdown
    message={message.text}
    {latex_delimiters}
    {sanitize_html}
    {render_markdown}
    {line_breaks}
    on:load={scroll}
/>
{#each message.files as file, k}
    {#if file !== null && file.file.mime_type?.includes("audio")}
        <audio
            data-testid="chatbot-audio"
            controls
            preload="metadata"
            src={file.file?.url}
            title={file.alt_text}
            on:play
            on:pause
            on:ended
        />
    {:else if message !== null && file.file?.mime_type?.includes("video")}
        <video
            data-testid="chatbot-video"
            controls
            src={file.file?.url}
            title={file.alt_text}
            preload="auto"
            on:play
            on:pause
            on:ended
        >
            <track kind="captions" />
        </video>
    {:else if message !== null && file.file?.mime_type?.includes("image")}
        <img
            data-testid="chatbot-image"
            src={file.file?.url}
            alt={file.alt_text}
        />
    {:else if message !== null && file.file?.url !== null}
        <a
            data-testid="chatbot-file"
            href={file.file?.url}
            target="_blank"
            download={window.__is_colab__
                ? null
                : file.file?.orig_name || file.file?.path}
        >
            {file.file?.orig_name || file.file?.path}
        </a>
    {:else if pending_message && j === 1}
        <Pending {layout} />
    {/if}
{/each}
```

We did it! ðŸŽ‰

## Part 4 - The demo

For this tutorial, let's keep the demo simple and just display a static conversation between a hypothetical user and a bot.
This demo will show how both the user and the bot can send files. 
In part 2 of this tutorial series we will build a fully functional chatbot demo!

The demo code will look like the following:

```python
import gradio as gr
from gradio_multimodalchatbot import MultimodalChatbot
from gradio.data_classes import FileData

user_msg1 = {"text": "Hello, what is in this image?",
             "files": [{"file": FileData(path="https://gradio-builds.s3.amazonaws.com/diffusion_image/cute_dog.jpg")}]
             }
bot_msg1 = {"text": "It is a very cute dog",
            "files": []}

user_msg2 = {"text": "Describe this audio clip please.",
             "files": [{"file": FileData(path="cantina.wav")}]}
bot_msg2 = {"text": "It is the cantina song from Star Wars",
            "files": []}

user_msg3 = {"text": "Give me a video clip please.",
             "files": []}
bot_msg3 = {"text": "Here is a video clip of the world",
            "files": [{"file": FileData(path="world.mp4")},
                      {"file": FileData(path="cantina.wav")}]}

conversation = [[user_msg1, bot_msg1], [user_msg2, bot_msg2], [user_msg3, bot_msg3]]

with gr.Blocks() as demo:
    MultimodalChatbot(value=conversation, height=800)


demo.launch()
```


Tip: Change the filepaths so that they correspond to files on your machine. Also, if you are running in development mode, make sure the files are located in the top level of your custom component directory.

## Part 5 - Deploying and Conclusion

Let's build and deploy our demo with `gradio cc build` and `gradio cc deploy`!

You can check out our component deployed to [HuggingFace Spaces](https://huggingface.co/spaces/freddyaboulton/gradio_multimodalchatbot) and all of the source code is available [here](https://huggingface.co/spaces/freddyaboulton/gradio_multimodalchatbot/tree/main/src).

See you in the next installment of this series!

---

<!-- Source: guides/04_additional-features/09_multipage-apps.md -->
# Multipage Apps

Your Gradio app can support multiple pages with the `Blocks.route()` method. Here's what a multipage Gradio app generally looks like:

```python
with gr.Blocks() as demo:  # Main page
    name = gr.Textbox(label="Name")
    ...
with demo.route("Second page", "/second"):
    num = gr.Number()
    ...

demo.launch()
```

This allows you to define links to separate pages, each with a separate URL, which are  linked to the top of the Gradio app in an automatically-generated navbar. 

Here's a complete example:

$code_multipage

All of these pages will share the same backend, including the same queue.

Note: multipage apps do not support interactions between pages, e.g. an event listener on one page cannot output to a component on another page. Use `gr.Tabs()` for this type of functionality instead of pages.

**Separate Files**

For maintainability, you may want to write the code for different pages in different files. Because any Gradio Blocks can be imported and rendered inside another Blocks using the `.render()` method, you can do this as follows.

Create one main file, say `app.py` and create separate Python files for each page:

```
- app.py
- main_page.py
- second_page.py
```

The Python file corresponding to each page should consist of a regular Gradio Blocks, Interface, or ChatInterface application, e.g.

`main_page.py`

```py
import gradio as gr

with gr.Blocks() as demo:
    gr.Image()

if __name__ == "__main__":
    demo.launch()
```

`second_page.py`

```py
import gradio as gr

with gr.Blocks() as demo:
    t = gr.Textbox()
    demo.load(lambda : "Loaded", None, t)

if __name__ == "__main__":
    demo.launch()
```

In your main `app.py` file, simply import the Gradio demos from the page files and `.render()` them:

`app.py`

```py
import gradio as gr

import main_page, second_page

with gr.Blocks() as demo:
    main_page.demo.render()
with demo.route("Second Page"):
    second_page.demo.render()

if __name__ == "__main__":
    demo.launch()
```

This allows you to run each page as an independent Gradio app for testing, while also creating a single file `app.py` that serves as the entrypoint for the complete multipage app.

## Customizing the Navbar

By default, Gradio automatically generates a navigation bar for multipage apps that displays all your pages with "Home" as the title for the main page. You can customize the navbar behavior using the `gr.Navbar` component.

### Per-Page Navbar Configuration

You can have different navbar configurations for each page of your app:

```python
import gradio as gr

with gr.Blocks() as demo:
    # Navbar for the main page
    navbar = gr.Navbar(
        visible=True,
        main_page_name="Dashboard",
        value=[("About", "https://example.com/about")]
    )
    
    gr.Textbox(label="Main page content")

with demo.route("Settings"):
    # Different navbar for the Settings page
    navbar = gr.Navbar(
        visible=True,
        main_page_name="Home",
        value=[("Documentation", "https://docs.example.com")]
    )
    gr.Textbox(label="Settings page")

demo.launch()
```


**Important Notes:**
- You can have one `gr.Navbar` component per page. Each page's navbar configuration is independent.
- The `main_page_name` parameter customizes the title of the home page link in the navbar.
- The `value` parameter allows you to add additional links to the navbar, which can be internal pages or external URLs.
- If no `gr.Navbar` component is present on a page, the default navbar behavior is used (visible with "Home" as the home page title).
- You can update the navbar properties using standard Gradio event handling, just like with any other component.

Here's an example that demonstrates the last point:

$code_navbar_customization

---

<!-- Source: guides/08_custom-components/09_documenting-custom-components.md -->
# Documenting Custom Components

In 4.15, we added a  new `gradio cc docs` command to the Gradio CLI to generate rich documentation for your custom component. This command will generate documentation for users automatically, but to get the most out of it, you need to do a few things.

## How do I use it?

The documentation will be generated when running `gradio cc build`. You can pass the `--no-generate-docs` argument to turn off this behaviour.

There is also a standalone `docs` command that allows for greater customisation. If you are running this command manually it should be run _after_ the `version` in your `pyproject.toml` has been bumped but before building the component.

All arguments are optional.

```bash
gradio cc docs
  path # The directory of the custom component.
  --demo-dir # Path to the demo directory.
  --demo-name # Name of the demo file
  --space-url # URL of the Hugging Face Space to link to
  --generate-space # create a documentation space.
  --no-generate-space # do not create a documentation space
  --readme-path # Path to the README.md file.
  --generate-readme # create a REAMDE.md file
  --no-generate-readme # do not create a README.md file
  --suppress-demo-check # suppress validation checks and warnings
```

## What gets generated?

The `gradio cc docs` command will generate an interactive Gradio app and a static README file with various features. You can see an example here:

- [Gradio app deployed on Hugging Face Spaces]()
- [README.md rendered by GitHub]()

The README.md and space both have the following features:

- A description.
- Installation instructions.
- A fully functioning code snippet.
- Optional links to PyPi, GitHub, and Hugging Face Spaces.
- API documentation including:
  - An argument table for component initialisation showing types, defaults, and descriptions.
  - A description of how the component affects the user's predict function.
  - A table of events and their descriptions.
  - Any additional interfaces or classes that may be used during initialisation or in the pre- or post- processors.

Additionally, the Gradio includes:

- A live demo.
- A richer, interactive version of the parameter tables.
- Nicer styling!

## What do I need to do?

The documentation generator uses existing standards to extract the necessary information, namely Type Hints and Docstrings. There are no Gradio-specific APIs for documentation, so following best practices will generally yield the best results.

If you already use type hints and docstrings in your component source code, you don't need to do much to benefit from this feature, but there are some details that you should be aware of.

### Python version

To get the best documentation experience, you need to use Python `3.10` or greater when generating documentation. This is because some introspection features used to generate the documentation were only added in `3.10`.

### Type hints

Python type hints are used extensively to provide helpful information for users. 

<details> 
<summary> What are type hints?</summary>


If you need to become more familiar with type hints in Python, they are a simple way to express what Python types are expected for arguments and return values of functions and methods. They provide a helpful in-editor experience, aid in maintenance, and integrate with various other tools. These types can be simple primitives, like `list` `str` `bool`; they could be more compound types like `list[str]`, `str | None` or `tuple[str, float | int]`; or they can be more complex types using utility classed like [`TypedDict`](https://peps.python.org/pep-0589/#abstract).

[Read more about type hints in Python.](https://realpython.com/lessons/type-hinting/)


</details>

#### What do I need to add hints to?

You do not need to add type hints to every part of your code. For the documentation to work correctly, you will need to add type hints to the following component methods:

- `__init__` parameters should be typed.
- `postprocess` parameters and return value should be typed.
- `preprocess` parameters and return value should be typed.

If you are using `gradio cc create`, these types should already exist, but you may need to tweak them based on any changes you make.

##### `__init__`

Here, you only need to type the parameters. If you have cloned a template with `gradio` cc create`, these should already be in place. You will only need to add new hints for anything you have added or changed:

```py
def __init__(
  self,
  value: str | None = None,
  *,
  sources: Literal["upload", "microphone"] = "upload,
  every: Timer | float | None = None,
  ...
):
  ...
```

##### `preprocess` and `postprocess`

The `preprocess` and `postprocess` methods determine the value passed to the user function and the value that needs to be returned.

Even if the design of your component is primarily as an input or an output, it is worth adding type hints to both the input parameters and the return values because Gradio has no way of limiting how components can be used.

In this case, we specifically care about:

- The return type of `preprocess`.
- The input type of `postprocess`.

```py
def preprocess(
  self, payload: FileData | None # input is optional
) -> tuple[int, str] | str | None:

# user function input  is the preprocess return â–²
# user function output is the postprocess input â–¼

def postprocess(
  self, value: tuple[int, str] | None
) -> FileData | bytes | None: # return is optional
  ...
```

### Docstrings

Docstrings are also used extensively to extract more meaningful, human-readable descriptions of certain parts of the API.

<details> 
<summary> What are docstrings?</summary>


If you need to become more familiar with docstrings in Python, they are a way to annotate parts of your code with human-readable decisions and explanations. They offer a rich in-editor experience like type hints, but unlike type hints, they don't have any specific syntax requirements. They are simple strings and can take almost any form. The only requirement is where they appear. Docstrings should be "a string literal that occurs as the first statement in a module, function, class, or method definition".

[Read more about Python docstrings.](https://peps.python.org/pep-0257/#what-is-a-docstring)

</details>

While docstrings don't have any syntax requirements, we need a particular structure for documentation purposes.

As with type hint, the specific information we care about is as follows:

- `__init__` parameter docstrings.
- `preprocess` return docstrings.
- `postprocess` input parameter docstrings.

Everything else is optional.

Docstrings should always take this format to be picked up by the documentation generator:

#### Classes

```py
"""
A description of the class.

This can span multiple lines and can _contain_ *markdown*.
"""
```

#### Methods and functions 

Markdown in these descriptions will not be converted into formatted text.

```py
"""
Parameters:
    param_one: A description for this parameter.
    param_two: A description for this parameter.
Returns:
    A description for this return value.
"""
```

### Events

In custom components, events are expressed as a list stored on the `events` field of the component class. While we do not need types for events, we _do_ need a human-readable description so users can understand the behaviour of the event.

To facilitate this, we must create the event in a specific way.

There are two ways to add events to a custom component.

#### Built-in events

Gradio comes with a variety of built-in events that may be enough for your component. If you are using built-in events, you do not need to do anything as they already have descriptions we can extract:

```py
from gradio.events import Events

class ParamViewer(Component):
  ...

  EVENTS = [
    Events.change,
    Events.upload,
  ]
```

#### Custom events

You can define a custom event if the built-in events are unsuitable for your use case. This is a straightforward process, but you must create the event in this way for docstrings to work correctly:

```py
from gradio.events import Events, EventListener

class ParamViewer(Component):
  ...

  EVENTS = [
    Events.change,
    EventListener(
        "bingbong",
        doc="This listener is triggered when the user does a bingbong."
      )
  ]
```

### Demo

The `demo/app.py`, often used for developing the component, generates the live demo and code snippet. The only strict rule here is that the `demo.launch()` command must be contained with a `__name__ == "__main__"` conditional as below:

```py
if __name__ == "__main__":
  demo.launch()
```

The documentation generator will scan for such a clause and error if absent. If you are _not_ launching the demo inside the `demo/app.py`, then you can pass `--suppress-demo-check` to turn off this check.

#### Demo recommendations

Although there are no additional rules, there are some best practices you should bear in mind to get the best experience from the documentation generator.

These are only guidelines, and every situation is unique, but they are sound principles to remember.

##### Keep the demo compact

Compact demos look better and make it easier for users to understand what the demo does. Try to remove as many extraneous UI elements as possible to focus the users' attention on the core use case. 

Sometimes, it might make sense to have a `demo/app.py` just for the docs and an additional, more complex app for your testing purposes. You can also create other spaces, showcasing more complex examples and linking to them from the main class docstring or the `pyproject.toml` description.

#### Keep the code concise

The 'getting started' snippet utilises the demo code, which should be as short as possible to keep users engaged and avoid confusion.

It isn't the job of the sample snippet to demonstrate the whole API; this snippet should be the shortest path to success for a new user. It should be easy to type or copy-paste and easy to understand. Explanatory comments should be brief and to the point.

#### Avoid external dependencies

As mentioned above, users should be able to copy-paste a snippet and have a fully working app. Try to avoid third-party library dependencies to facilitate this.

You should carefully consider any examples; avoiding examples that require additional files or that make assumptions about the environment is generally a good idea.

#### Ensure the `demo` directory is self-contained

Only the `demo` directory will be uploaded to Hugging Face spaces in certain instances, as the component will be installed via PyPi if possible. It is essential that this directory is self-contained and any files needed for the correct running of the demo are present.

### Additional URLs

The documentation generator will generate a few buttons, providing helpful information and links to users. They are obtained automatically in some cases, but some need to be explicitly included in the `pyproject.yaml`. 

- PyPi Version and link - This is generated automatically.
- GitHub Repository - This is populated via the `pyproject.toml`'s `project.urls.repository`.
- Hugging Face Space - This is populated via the `pyproject.toml`'s `project.urls.space`.

An example `pyproject.toml` urls section might look like this:

```toml
[project.urls]
repository = "https://github.com/user/repo-name"
space = "https://huggingface.co/spaces/user/space-name"
```

---

<!-- Source: guides/04_additional-features/10_environment-variables.md -->
# Environment Variables

Environment variables in Gradio provide a way to customize your applications and launch settings without changing the codebase. In this guide, we'll explore the key environment variables supported in Gradio and how to set them.

## Key Environment Variables

### 1. `GRADIO_SERVER_PORT`

- **Description**: Specifies the port on which the Gradio app will run.
- **Default**: `7860`
- **Example**:
  ```bash
  export GRADIO_SERVER_PORT=8000
  ```

### 2. `GRADIO_SERVER_NAME`

- **Description**: Defines the host name for the Gradio server. To make Gradio accessible from any IP address, set this to `"0.0.0.0"`
- **Default**: `"127.0.0.1"` 
- **Example**:
  ```bash
  export GRADIO_SERVER_NAME="0.0.0.0"
  ```

### 3. `GRADIO_NUM_PORTS`

- **Description**: Defines the number of ports to try when starting the Gradio server.
- **Default**: `100`
- **Example**:
  ```bash
  export GRADIO_NUM_PORTS=200
  ```

### 4. `GRADIO_ANALYTICS_ENABLED`

- **Description**: Whether Gradio should provide 
- **Default**: `"True"`
- **Options**: `"True"`, `"False"`
- **Example**:
  ```sh
  export GRADIO_ANALYTICS_ENABLED="True"
  ```

### 5. `GRADIO_DEBUG`

- **Description**: Enables or disables debug mode in Gradio. If debug mode is enabled, the main thread does not terminate allowing error messages to be printed in environments such as Google Colab.
- **Default**: `0`
- **Example**:
  ```sh
  export GRADIO_DEBUG=1
  ```

### 6. `GRADIO_FLAGGING_MODE`

- **Description**: Controls whether users can flag inputs/outputs in the Gradio interface. See [the Guide on flagging](/guides/using-flagging) for more details.
- **Default**: `"manual"`
- **Options**: `"never"`, `"manual"`, `"auto"`
- **Example**:
  ```sh
  export GRADIO_FLAGGING_MODE="never"
  ```

### 7. `GRADIO_TEMP_DIR`

- **Description**: Specifies the directory where temporary files created by Gradio are stored.
- **Default**: System default temporary directory
- **Example**:
  ```sh
  export GRADIO_TEMP_DIR="/path/to/temp"
  ```

### 8. `GRADIO_ROOT_PATH`

- **Description**: Sets the root path for the Gradio application. Useful if running Gradio [behind a reverse proxy](/guides/running-gradio-on-your-web-server-with-nginx).
- **Default**: `""`
- **Example**:
  ```sh
  export GRADIO_ROOT_PATH="/myapp"
  ```

### 9. `GRADIO_SHARE`

- **Description**: Enables or disables sharing the Gradio app.
- **Default**: `"False"`
- **Options**: `"True"`, `"False"`
- **Example**:
  ```sh
  export GRADIO_SHARE="True"
  ```

### 10. `GRADIO_ALLOWED_PATHS`

- **Description**: Sets a list of complete filepaths or parent directories that gradio is allowed to serve. Must be absolute paths. Warning: if you provide directories, any files in these directories or their subdirectories are accessible to all users of your app. Multiple items can be specified by separating items with commas.
- **Default**: `""`
- **Example**:
  ```sh
  export GRADIO_ALLOWED_PATHS="/mnt/sda1,/mnt/sda2"
  ```

### 11. `GRADIO_BLOCKED_PATHS`

- **Description**: Sets a list of complete filepaths or parent directories that gradio is not allowed to serve (i.e. users of your app are not allowed to access). Must be absolute paths. Warning: takes precedence over `allowed_paths` and all other directories exposed by Gradio by default. Multiple items can be specified by separating items with commas.
- **Default**: `""`
- **Example**:
  ```sh
  export GRADIO_BLOCKED_PATHS="/users/x/gradio_app/admin,/users/x/gradio_app/keys"
  ```

### 12. `FORWARDED_ALLOW_IPS`

- **Description**: This is not a Gradio-specific environment variable, but rather one used in server configurations, specifically `uvicorn` which is used by Gradio internally. This environment variable is useful when deploying applications behind a reverse proxy. It defines a list of IP addresses that are trusted to forward traffic to your application. When set, the application will trust the `X-Forwarded-For` header from these IP addresses to determine the original IP address of the user making the request. This means that if you use the `gr.Request` [object's](https://www.gradio.app/docs/gradio/request) `client.host` property, it will correctly get the user's IP address instead of the IP address of the reverse proxy server. Note that only trusted IP addresses (i.e. the IP addresses of your reverse proxy servers) should be added, as any server with these IP addresses can modify the `X-Forwarded-For` header and spoof the client's IP address.
- **Default**: `"127.0.0.1"`
- **Example**:
  ```sh
  export FORWARDED_ALLOW_IPS="127.0.0.1,192.168.1.100"
  ```

### 13. `GRADIO_CACHE_EXAMPLES`

- **Description**: Whether or not to cache examples by default in `gr.Interface()`, `gr.ChatInterface()` or in `gr.Examples()` when no explicit argument is passed for the `cache_examples` parameter. You can set this environment variable to either the string "true" or "false".
- **Default**: `"false"`
- **Example**:
  ```sh
  export GRADIO_CACHE_EXAMPLES="true"
  ```


### 14. `GRADIO_CACHE_MODE`

- **Description**: How to cache examples. Only applies if `cache_examples` is set to `True` either via enviornment variable or by an explicit parameter, AND no no explicit argument is passed for the `cache_mode` parameter in `gr.Interface()`, `gr.ChatInterface()` or in `gr.Examples()`. Can be set to either the strings "lazy" or "eager." If "lazy", examples are cached after their first use for all users of the app. If "eager", all examples are cached at app launch.

- **Default**: `"eager"`
- **Example**:
  ```sh
  export GRADIO_CACHE_MODE="lazy"
  ```


### 15. `GRADIO_EXAMPLES_CACHE`

- **Description**:  If you set `cache_examples=True` in `gr.Interface()`, `gr.ChatInterface()` or in `gr.Examples()`, Gradio will run your prediction function and save the results to disk. By default, this is in the `.gradio/cached_examples//` subdirectory within your app's working directory. You can customize the location of cached example files created by Gradio by setting the environment variable `GRADIO_EXAMPLES_CACHE` to an absolute path or a path relative to your working directory.
- **Default**: `".gradio/cached_examples/"`
- **Example**:
  ```sh
  export GRADIO_EXAMPLES_CACHE="custom_cached_examples/"
  ```


### 16. `GRADIO_SSR_MODE`

- **Description**: Controls whether server-side rendering (SSR) is enabled. When enabled, the initial HTML is rendered on the server rather than the client, which can improve initial page load performance and SEO.

- **Default**: `"False"` (except on Hugging Face Spaces, where this environment variable sets it to `True`)
- **Options**: `"True"`, `"False"`
- **Example**:
  ```sh
  export GRADIO_SSR_MODE="True"
  ```

### 17. `GRADIO_NODE_SERVER_NAME`

- **Description**: Defines the host name for the Gradio node server. (Only applies if `ssr_mode` is set to `True`.)
- **Default**: `GRADIO_SERVER_NAME` if it is set, otherwise `"127.0.0.1"`
- **Example**:
  ```sh
  export GRADIO_NODE_SERVER_NAME="0.0.0.0"
  ```

### 18. `GRADIO_NODE_NUM_PORTS`

- **Description**: Defines the number of ports to try when starting the Gradio node server. (Only applies if `ssr_mode` is set to `True`.)
- **Default**: `100`
- **Example**:
  ```sh
  export GRADIO_NODE_NUM_PORTS=200
  ```

### 19. `GRADIO_RESET_EXAMPLES_CACHE`

- **Description**: If set to "True", Gradio will delete and recreate the examples cache directory when the app starts instead of reusing the cached example if they already exist. 
- **Default**: `"False"`
- **Options**: `"True"`, `"False"`
- **Example**:
  ```sh
  export GRADIO_RESET_EXAMPLES_CACHE="True"
  ```

### 20. `GRADIO_CHAT_FLAGGING_MODE`

- **Description**: Controls whether users can flag messages in `gr.ChatInterface` applications. Similar to `GRADIO_FLAGGING_MODE` but specifically for chat interfaces.
- **Default**: `"never"`
- **Options**: `"never"`, `"manual"`
- **Example**:
  ```sh
  export GRADIO_CHAT_FLAGGING_MODE="manual"
  ```

### 21. `GRADIO_WATCH_DIRS`

- **Description**: Specifies directories to watch for file changes when running Gradio in development mode. When files in these directories change, the Gradio app will automatically reload. Multiple directories can be specified by separating them with commas. This is primarily used by the `gradio` CLI command for development workflows.
- **Default**: `""`
- **Example**:
  ```sh
  export GRADIO_WATCH_DIRS="/path/to/src,/path/to/templates"
  ```

### 22. `GRADIO_VIBE_MODE`

- **Description**: Enables the Vibe editor mode, which provides an in-browser chat that can be used to write or edit your Gradio app using natural language. When enabled, anyone who can access the Gradio endpoint can modify files and run arbitrary code on the host machine. Use with extreme caution in production environments.
- **Default**: `""`
- **Options**: Any non-empty string enables the mode
- **Example**:
  ```sh
  export GRADIO_VIBE_MODE="1"
  ```

### 23. `GRADIO_MCP_SERVER`

- **Description**: Enables the MCP (Model Context Protocol) server functionality in Gradio. When enabled, the Gradio app will be set up as an MCP server and documented functions will be added as MCP tools that can be used by LLMs. This allows LLMs to interact with your Gradio app's functionality through the MCP protocol.
- **Default**: `"False"`
- **Options**: `"True"`, `"False"`
- **Example**:
  ```sh
  export GRADIO_MCP_SERVER="True"
  ```





## How to Set Environment Variables

To set environment variables in your terminal, use the `export` command followed by the variable name and its value. For example:

```sh
export GRADIO_SERVER_PORT=8000
```

If you're using a `.env` file to manage your environment variables, you can add them like this:

```sh
GRADIO_SERVER_PORT=8000
GRADIO_SERVER_NAME="localhost"
```

Then, use a tool like `dotenv` to load these variables when running your application.

---

<!-- Source: guides/04_additional-features/11_resource-cleanup.md -->
# Resource Cleanup

Your Gradio application may create resources during its lifetime.
Examples of resources are `gr.State` variables, any variables you create and explicitly hold in memory, or files you save to disk. 
Over time, these resources can use up all of your server's RAM or disk space and crash your application.

Gradio provides some tools for you to clean up the resources created by your app:

1. Automatic deletion of `gr.State` variables.
2. Automatic cache cleanup with the `delete_cache` parameter.
2. The `Blocks.unload` event.

Let's take a look at each of them individually.

## Automatic deletion of `gr.State`

When a user closes their browser tab, Gradio will automatically delete any `gr.State` variables associated with that user session after 60 minutes. If the user connects again within those 60 minutes, no state will be deleted.

You can control the deletion behavior further with the following two parameters of `gr.State`:

1. `delete_callback` - An arbitrary function that will be called when the variable is deleted. This function must take the state value as input. This function is useful for deleting variables from GPU memory.
2. `time_to_live` - The number of seconds the state should be stored for after it is created or updated. This will delete variables before the session is closed, so it's useful for clearing state for potentially long running sessions.

## Automatic cache cleanup via `delete_cache`

Your Gradio application will save uploaded and generated files to a special directory called the cache directory. Gradio uses a hashing scheme to ensure that duplicate files are not saved to the cache but over time the size of the cache will grow (especially if your app goes viral ðŸ˜‰).

Gradio can periodically clean up the cache for you if you specify the `delete_cache` parameter of `gr.Blocks()`, `gr.Interface()`, or `gr.ChatInterface()`. 
This parameter is a tuple of the form `[frequency, age]` both expressed in number of seconds.
Every `frequency` seconds, the temporary files created by this Blocks instance will be deleted if more than `age` seconds have passed since the file was created. 
For example, setting this to (86400, 86400) will delete temporary files every day if they are older than a day old.
Additionally, the cache will be deleted entirely when the server restarts.

## The `unload` event

Additionally, Gradio now includes a `Blocks.unload()` event, allowing you to run arbitrary cleanup functions when users disconnect (this does not have a 60 minute delay).
Unlike other gradio events, this event does not accept inputs or outptus.
You can think of the `unload` event as the opposite of the `load` event.

## Putting it all together

The following demo uses all of these features. When a user visits the page, a special unique directory is created for that user.
As the user interacts with the app, images are saved to disk in that special directory.
When the user closes the page, the images created in that session are deleted via the `unload` event.
The state and files in the cache are cleaned up automatically as well.

$code_state_cleanup
$demo_state_cleanup

---

<!-- Source: guides/04_additional-features/12_themes.md -->
# Gradio Themes

Gradio themes are the easiest way to customize the look and feel of your app. You can choose from a variety of themes, or create your own. To do so, pass the `theme=` kwarg to the `launch()` method of `Interface`, `ChatInterface`, or `Blocks`. For example:

```python
demo = gr.Interface()
demo.launch(theme=gr.themes.Monochrome())
```

or

```python
with gr.Blocks() as demo:
    ... # your code here
demo.launch(theme=gr.themes.Soft())
    ...
```

Gradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. You can extend these themes or create your own themes from scratch - see the [theming guide](https://gradio.app/guides/theming-guide) for more details.

For additional styling ability, you can pass any CSS (as well as custom JavaScript) to your Gradio application. This is discussed in more detail in our [custom JS and CSS guide](/guides/custom-CSS-and-JS).

---

<!-- Source: guides/04_additional-features/13_client-side-functions.md -->
# Client Side Functions

Gradio allows you to run certain "simple" functions directly in the browser by setting `js=True` in your event listeners. This will **automatically convert your Python code into JavaScript**, which significantly improves the responsiveness of your app by avoiding a round trip to the server for simple UI updates.

The difference in responsiveness is most noticeable on hosted applications (like Hugging Face Spaces), when the server is under heavy load, with high-latency connections, or when many users are accessing the app simultaneously.

## When to Use Client Side Functions

Client side functions are ideal for updating component properties (like visibility, placeholders, interactive state, or styling). 

Here's a basic example:

```py
import gradio as gr

with gr.Blocks() as demo:
    with gr.Row() as row:
        btn = gr.Button("Hide this row")
    
    # This function runs in the browser without a server roundtrip
    btn.click(
        lambda: gr.Row(visible=False), 
        None, 
        row, 
        js=True
    )

demo.launch()
```


## Limitations

Client side functions have some important restrictions:
* They can only update component properties (not values)
* They cannot take any inputs

Here are some functions that will work with `js=True`:

```py
# Simple property updates
lambda: gr.Textbox(lines=4)

# Multiple component updates
lambda: [gr.Textbox(lines=4), gr.Button(interactive=False)]

# Using gr.update() for property changes
lambda: gr.update(visible=True, interactive=False)
```

We are working to increase the space of functions that can be transpiled to JavaScript so that they can be run in the browser. [Follow the Groovy library for more info](https://github.com/abidlabs/groovy-transpiler).


## Complete Example

Here's a more complete example showing how client side functions can improve the user experience:

$code_todo_list_js


## Behind the Scenes

When you set `js=True`, Gradio:

1. Transpiles your Python function to JavaScript

2. Runs the function directly in the browser

3. Still sends the request to the server (for consistency and to handle any side effects)

This provides immediate visual feedback while ensuring your application state remains consistent.

---

<!-- Source: guides/04_additional-features/14_view-api-page.md -->
# API Page

You can use almost any Gradio app programmatically via the built-in API! In the footer of any Gradio app, you'll see a "Use via API" link. Clicking on the link opens up a detailed documentation page for the API that Gradio generates based on the function signatures in your Gradio app.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api-animated.gif)

## Configuring the API Page

**API endpoint names**

When you create a Gradio application, the API endpoint names are automatically generated based on the function names. You can change this by using the `api_name` parameter in `gr.Interface` or `gr.ChatInterface`. If you are using Gradio `Blocks`, you can name each event listener, like this:

```python
btn.click(add, [num1, num2], output, api_name="addition")
```

**Hiding API endpoints**

When building a complex Gradio app, you might want to hide certain API endpoints from appearing on the view API page, e.g. if they correspond to functions that simply update the UI. You can set the  `show_api` parameter to `False` in any `Blocks` event listener to achieve this, e.g. 

```python
btn.click(add, [num1, num2], output, show_api=False)
```

**Disabling API endpoints**

Hiding the API endpoint doesn't disable it. A user can still programmatically call the API endpoint if they know the name. If you want to disable an API endpoint altogether, set `api_name=False`, e.g. 

```python
btn.click(add, [num1, num2], output, api_name=False)
```

Note: setting an `api_name=False` also means that downstream apps will not be able to load your Gradio app using `gr.load()` as this function uses the Gradio API under the hood.

**Adding API endpoints**

You can also add new API routes to your Gradio application that do not correspond to events in your UI.

For example, in this Gradio application, we add a new route that adds numbers and slices a list:

```py
import gradio as gr
with gr.Blocks() as demo:
    with gr.Row():
        input = gr.Textbox()
        button = gr.Button("Submit")
    output = gr.Textbox()
    def fn(a: int, b: int, c: list[str]) -> tuple[int, str]:
        return a + b, c[a:b]
    gr.api(fn, api_name="add_and_slice")

_, url, _ = demo.launch()
```

This will create a new route `/add_and_slice` which will show up in the "view API" page. It can be programmatically called by the Python or JS Clients (discussed below) like this:

```py
from gradio_client import Client

client = Client(url)
result = client.predict(
        a=3,
        b=5,
        c=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
        api_name="/add_and_slice"
)
print(result)
```

## The Clients

This API page not only lists all of the endpoints that can be used to query the Gradio app, but also shows the usage of both [the Gradio Python client](https://gradio.app/guides/getting-started-with-the-python-client/), and [the Gradio JavaScript client](https://gradio.app/guides/getting-started-with-the-js-client/). 

For each endpoint, Gradio automatically generates a complete code snippet with the parameters and their types, as well as example inputs, allowing you to immediately test an endpoint. Here's an example showing an image file input and `str` output:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api-snippet.png)


## The API Recorder ðŸª„

Instead of reading through the view API page, you can also use Gradio's built-in API recorder to generate the relevant code snippet. Simply click on the "API Recorder" button, use your Gradio app via the UI as you would normally, and then the API Recorder will generate the code using the Clients to recreate your all of your interactions programmatically.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/api-recorder.gif)

## MCP Server

The API page also includes instructions on how to use the Gradio app as an Model Context Protocol (MCP) server, which is a standardized way to expose functions as tools so that they can be used by LLMs. 

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api-mcp.png)

For the MCP sever, each tool, its description, and its parameters are listed, along with instructions on how to integrate with popular MCP Clients. Read more about Gradio's [MCP integration here](https://www.gradio.app/guides/building-mcp-server-with-gradio).

## OpenAPI Specification

You can access the complete OpenAPI (formerly Swagger) specification of your Gradio app's API at the endpoint `<your-gradio-app-url>/gradio_api/openapi.json`. The OpenAPI specification is a standardized, language-agnostic interface description for REST APIs that enables both humans and computers to discover and understand the capabilities of your service.

---

<!-- Source: guides/04_additional-features/15_internationalization.md -->
Tags: internationalization, i18n, language
Related spaces:

# Internationalization (i18n)

Gradio comes with ready-to-use internationalization (i18n) support:

- Built-in translations: Gradio automatically translates standard UI elements (like "Submit", "Clear", "Cancel") in more than 40 languages based on the user's browser locale.
- Custom translations: For app-specific text, Gradio provides the I18n class that lets you extend the built-in system with your own translations.

## Setting Up Translations

You can initialize the `I18n` class with multiple language dictionaries to add custom translations:

```python
import gradio as gr

# Create an I18n instance with translations for multiple languages
i18n = gr.I18n(
    en={"greeting": "Hello, welcome to my app!", "submit": "Submit"},
    es={"greeting": "Â¡Hola, bienvenido a mi aplicaciÃ³n!", "submit": "Enviar"},
    fr={"greeting": "Bonjour, bienvenue dans mon application!", "submit": "Soumettre"}
)

with gr.Blocks() as demo:
    # Use the i18n method to translate the greeting
    gr.Markdown(i18n("greeting"))
    with gr.Row():
        input_text = gr.Textbox(label="Input")
        output_text = gr.Textbox(label="Output")
    
    submit_btn = gr.Button(i18n("submit"))

# Pass the i18n instance to the launch method
demo.launch(i18n=i18n)
```

## How It Works

When you use the `i18n` instance with a translation key, Gradio will show the corresponding translation to users based on their browser's language settings or the language they've selected in your app.

If a translation isn't available for the user's locale, the system will fall back to English (if available) or display the key itself.

## Valid Locale Codes

Locale codes should follow the BCP 47 format (e.g., 'en', 'en-US', 'zh-CN'). The `I18n` class will warn you if you use an invalid locale code.

## Supported Component Properties

The following component properties typically support internationalization:

- `description`
- `info`
- `title`
- `placeholder`
- `value`
- `label`

Note that support may vary depending on the component, and some properties might have exceptions where internationalization is not applicable. You can check this by referring to the typehint for the parameter and if it contains `I18nData`, then it supports internationalization.

---

<!-- Source: guides/04_additional-features/16_custom-buttons.md -->
# Custom Buttons

Many Gradio components support custom buttons in their toolbar, allowing you to add interactive buttons that can trigger Python functions, JavaScript functions, or both. Custom buttons appear alongside built-in buttons (like "copy" or "download") in the component's toolbar.

## Basic Usage

To add custom buttons to a component, pass a list of `gr.Button()` instances to the `buttons` parameter:

```python
import gradio as gr

refresh_btn = gr.Button("Refresh", variant="secondary", size="sm")
clear_btn = gr.Button("Clear", variant="secondary", size="sm")

textbox = gr.Textbox(
    value="Sample text",
    label="Text Input",
    buttons=[refresh_btn, clear_btn]
)
```

You can also mix built-in buttons (as strings) with custom buttons:

```python
code = gr.Code(
    value="print('Hello')",
    language="python",
    buttons=["copy", "download", refresh_btn, export_btn]
)
```

## Connecting Button Events

Custom buttons work just like regular `gr.Button` components. You can connect them to Python functions or JavaScript functions using the `.click()` method:

### Python Functions

```python
def refresh_data():
    import random
    return f"Refreshed: {random.randint(1000, 9999)}"

refresh_btn.click(refresh_data, outputs=textbox)
```

### JavaScript Functions

```python
clear_btn.click(
    None,
    inputs=[],
    outputs=textbox,
    js="() => ''"
)
```

### Combined Python and JavaScript

You can use the same button for both Python and JavaScript logic:

```python
alert_btn.click(
    None,
    inputs=textbox,
    outputs=[],
    js="(text) => { alert('Text: ' + text); return []; }"
)
```

## Complete Example

Here's a complete example showing custom buttons with both Python and JavaScript functions:

$code_textbox_custom_buttons


## Notes

- Custom buttons appear in the component's toolbar, typically in the top-right corner
- Only the `value` of the Button is used, other attributes like `icon` are not used.
- Buttons are rendered in the order they appear in the `buttons` list
- Built-in buttons (like "copy", "download") can be hidden by omitting them from the list
- Custom buttons work with component events in the same way as as regular buttons

---

<!-- Source: guides/11_other-tutorials/create-immersive-demo.md -->
# Create a Real-Time Immersive Audio + Video Demo with FastRTC

Tags: REAL-TIME, IMMERSIVE, FASTRTC, VIDEO, AUDIO, STREAMING, GEMINI, WEBRTC

FastRTC is a library that lets you build low-latency real-time apps over WebRTC. In this guide, youâ€™ll implement a fun demo where Gemini is an art critic and will critique your uploaded artwork:
- Streams your webcam and microphone to a Gemini real-time session
- Sends periodic video frames (and an optional uploaded image) to the model
- Streams back the modelâ€™s audio responses in real time
- Creates a polished full-screen Gradio `WebRTC` UI

### What youâ€™ll build
<video autoplay loop>
  <source src="https://github.com/gradio-app/gradio/blob/main/guides/assets/art-critic.mp4?raw=true" type="video/mp4" />
</video>

### Prerequisites
- Python 3.10+
- A Gemini API key: `GEMINI_API_KEY`

Install the dependencies:

```bash
pip install "fastrtc[vad, tts]" gradio google-genai python-dotenv websockets pillow
```

## 1) Encoders for audio and images
Encoder functions to send audio as base64-encoded data and images as base64-encoded JPEG.

```python
import base64
import numpy as np
from io import BytesIO
from PIL import Image

def encode_audio(data: np.ndarray) -> dict:
    """Encode audio data (int16 mono) for Gemini."""
    return {
        "mime_type": "audio/pcm",
        "data": base64.b64encode(data.tobytes()).decode("UTF-8"),
    }

def encode_image(data: np.ndarray) -> dict:
    with BytesIO() as output_bytes:
        pil_image = Image.fromarray(data)
        pil_image.save(output_bytes, "JPEG")
        bytes_data = output_bytes.getvalue()
    base64_str = str(base64.b64encode(bytes_data), "utf-8")
    return {"mime_type": "image/jpeg", "data": base64_str}
```


## 2) Implement the Gemini audio-video handler
This handler:
- Opens a Gemini Live session on startup
- Receives streaming audio from Gemini and yields it back to the client
- Sends microphone audio as it arrives
- Sends a video frame at most once per second (to avoid flooding the API)
- Optionally sends an uploaded image (`gr.Image`) alongside the webcam frame

```python
import asyncio
import os
import time
import numpy as np
import websockets
from dotenv import load_dotenv
from google import genai
from fastrtc import AsyncAudioVideoStreamHandler, wait_for_item, WebRTCError

load_dotenv()

class GeminiHandler(AsyncAudioVideoStreamHandler):
    def __init__(self) -> None:
        super().__init__(
            "mono",
            output_sample_rate=24000,
            input_sample_rate=16000,
        )
        self.audio_queue = asyncio.Queue()
        self.video_queue = asyncio.Queue()
        self.session = None
        self.last_frame_time = 0.0
        self.quit = asyncio.Event()

    def copy(self) -> "GeminiHandler":
        return GeminiHandler()

    async def start_up(self):
        await self.wait_for_args()
        api_key = self.latest_args[3]
        hf_token = self.latest_args[4]
        if hf_token is None or hf_token == "":
            raise WebRTCError("HF Token is required")
        os.environ["HF_TOKEN"] = hf_token
        client = genai.Client(
            api_key=api_key, http_options={"api_version": "v1alpha"}
        )
        config = {"response_modalities": ["AUDIO"], "system_instruction": "You are an art critic that will critique the artwork passed in as an image to the user. Critique the artwork in a funny and lighthearted way. Be concise and to the point. Be friendly and engaging. Be helpful and informative. Be funny and lighthearted."}
        async with client.aio.live.connect(
            model="gemini-2.0-flash-exp",
            config=config,
        ) as session:
            self.session = session
            while not self.quit.is_set():
                turn = self.session.receive()
                try:
                    async for response in turn:
                        if data := response.data:
                            audio = np.frombuffer(data, dtype=np.int16).reshape(1, -1)
                        self.audio_queue.put_nowait(audio)
                except websockets.exceptions.ConnectionClosedOK:
                    print("connection closed")
                    break

    # Video: receive and (optionally) send frames to Gemini
    async def video_receive(self, frame: np.ndarray):
        self.video_queue.put_nowait(frame)
        if self.session and (time.time() - self.last_frame_time > 1.0):
            self.last_frame_time = time.time()
            await self.session.send(input=encode_image(frame))
            # If there is an uploaded image passed alongside the WebRTC component,
            # it will be available in latest_args[2]
            if self.latest_args[2] is not None:
                await self.session.send(input=encode_image(self.latest_args[2]))

    async def video_emit(self) -> np.ndarray:
        frame = await wait_for_item(self.video_queue, 0.01)
        if frame is not None:
            return frame
        # Fallback while waiting for first frame
        return np.zeros((100, 100, 3), dtype=np.uint8)

    # Audio: forward microphone audio to Gemini
    async def receive(self, frame: tuple[int, np.ndarray]) -> None:
        _, array = frame
        array = array.squeeze()  # (num_samples,)
        audio_message = encode_audio(array)
        if self.session:
            await self.session.send(input=audio_message)

    # Audio: emit Geminiâ€™s audio back to the client
    async def emit(self):
        array = await wait_for_item(self.audio_queue, 0.01)
        if array is not None:
            return (self.output_sample_rate, array)
        return array

    async def shutdown(self) -> None:
        if self.session:
            self.quit.set()
            await self.session.close()
            self.quit.clear()
```


## 3) Setup Stream and Gradio UI
Weâ€™ll add an optional `gr.Image` input alongside the `WebRTC` component. The handler will access this in `self.latest_args[1]` when sending frames to Gemini.

```python
import gradio as gr
from fastrtc import Stream, WebRTC, get_hf_turn_credentials


stream = Stream(
    handler=GeminiHandler(),
    modality="audio-video",
    mode="send-receive",
    server_rtc_configuration=get_hf_turn_credentials(ttl=600*10000),
    rtc_configuration=get_hf_turn_credentials(),
    additional_inputs=[
        gr.Markdown(
            "## ðŸŽ¨ Art Critic\n\n"
            "Provide an image of your artwork or hold it up to the webcam, and Gemini will critique it for you."
            "To get a Gemini API key, please visit the [Gemini API Key](https://aistudio.google.com/apikey) page."
            "To get an HF Token, please visit the [HF Token](https://huggingface.co/settings/tokens) page."
        ),
        gr.Image(label="Artwork", value="mona_lisa.jpg", type="numpy", sources=["upload", "clipboard"]),
        gr.Textbox(label="Gemini API Key", type="password"),
        gr.Textbox(label="HF Token", type="password"),
    ],
    ui_args={
        "icon": "https://www.gstatic.com/lamda/images/gemini_favicon_f069958c85030456e93de685481c559f160ea06b.png",
        "pulse_color": "rgb(255, 255, 255)",
        "icon_button_color": "rgb(255, 255, 255)",
        "title": "Gemini Audio Video Chat",
    },
    time_limit=90,
    concurrency_limit=5,
)

if __name__ == "__main__":
    stream.ui.launch()

```

### References
- Gemini Audio Video Chat reference code: [Hugging Face Space](https://huggingface.co/spaces/gradio/gemini-audio-video/blob/main/app.py)
- FastRTC docs: `https://fastrtc.org`
- Audio + video user guide: `https://fastrtc.org/userguide/audio-video/`
- Gradio component integration: `https://fastrtc.org/userguide/gradio/`
- Cookbook (live demos + code): `https://fastrtc.org/cookbook/`

---

<!-- Source: guides/11_other-tutorials/create-your-own-friends-with-a-gan.md -->
# Create Your Own Friends with a GAN

Related spaces: https://huggingface.co/spaces/NimaBoscarino/cryptopunks, https://huggingface.co/spaces/nateraw/cryptopunks-generator
Tags: GAN, IMAGE, HUB

Contributed by <a href="https://huggingface.co/NimaBoscarino">Nima Boscarino</a> and <a href="https://huggingface.co/nateraw">Nate Raw</a>

## Introduction

It seems that cryptocurrencies, [NFTs](https://www.nytimes.com/interactive/2022/03/18/technology/nft-guide.html), and the web3 movement are all the rage these days! Digital assets are being listed on marketplaces for astounding amounts of money, and just about every celebrity is debuting their own NFT collection. While your crypto assets [may be taxable, such as in Canada](https://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/compliance/digital-currency/cryptocurrency-guide.html), today we'll explore some fun and tax-free ways to generate your own assortment of procedurally generated [CryptoPunks](https://www.larvalabs.com/cryptopunks).

Generative Adversarial Networks, often known just as _GANs_, are a specific class of deep-learning models that are designed to learn from an input dataset to create (_generate!_) new material that is convincingly similar to elements of the original training set. Famously, the website [thispersondoesnotexist.com](https://thispersondoesnotexist.com/) went viral with lifelike, yet synthetic, images of people generated with a model called StyleGAN2. GANs have gained traction in the machine learning world, and are now being used to generate all sorts of images, text, and even [music](https://salu133445.github.io/musegan/)!

Today we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo around a pre-trained GAN to see what all the fuss is about. Here's a [peek](https://nimaboscarino-cryptopunks.hf.space) at what we're going to be putting together.

### Prerequisites

Make sure you have the `gradio` Python package already [installed](/getting_started). To use the pretrained model, also install `torch` and `torchvision`.

## GANs: a very brief introduction

Originally proposed in [Goodfellow et al. 2014](https://arxiv.org/abs/1406.2661), GANs are made up of neural networks which compete with the intention of outsmarting each other. One network, known as the _generator_, is responsible for generating images. The other network, the _discriminator_, receives an image at a time from the generator along with a **real** image from the training data set. The discriminator then has to guess: which image is the fake?

The generator is constantly training to create images which are trickier for the discriminator to identify, while the discriminator raises the bar for the generator every time it correctly detects a fake. As the networks engage in this competitive (_adversarial!_) relationship, the images that get generated improve to the point where they become indistinguishable to human eyes!

For a more in-depth look at GANs, you can take a look at [this excellent post on Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/06/a-detailed-explanation-of-gan-with-implementation-using-tensorflow-and-keras/) or this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html). For now, though, we'll dive into a demo!

## Step 1 â€” Create the Generator model

To generate new images with a GAN, you only need the generator model. There are many different architectures that the generator could use, but for this demo we'll use a pretrained GAN generator model with the following architecture:

```python
from torch import nn

class Generator(nn.Module):
    # Refer to the link below for explanations about nc, nz, and ngf
    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs
    def __init__(self, nc=4, nz=100, ngf=64):
        super(Generator, self).__init__()
        self.network = nn.Sequential(
            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),
            nn.Tanh(),
        )

    def forward(self, input):
        output = self.network(input)
        return output
```

We're taking the generator from [this repo by @teddykoker](https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py#L90), where you can also see the original discriminator model structure.

After instantiating the model, we'll load in the weights from the Hugging Face Hub, stored at [nateraw/cryptopunks-gan](https://huggingface.co/nateraw/cryptopunks-gan):

```python
from huggingface_hub import hf_hub_download
import torch

model = Generator()
weights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')
model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available
```

## Step 2 â€” Defining a `predict` function

The `predict` function is the key to making Gradio work! Whatever inputs we choose through the Gradio interface will get passed through our `predict` function, which should operate on the inputs and generate outputs that we can display with Gradio output components. For GANs it's common to pass random noise into our model as the input, so we'll generate a tensor of random numbers and pass that through the model. We can then use `torchvision`'s `save_image` function to save the output of the model as a `png` file, and return the file name:

```python
from torchvision.utils import save_image

def predict(seed):
    num_punks = 4
    torch.manual_seed(seed)
    z = torch.randn(num_punks, 100, 1, 1)
    punks = model(z)
    save_image(punks, "punks.png", normalize=True)
    return 'punks.png'
```

We're giving our `predict` function a `seed` parameter, so that we can fix the random tensor generation with a seed. We'll then be able to reproduce punks if we want to see them again by passing in the same seed.

_Note!_ Our model needs an input tensor of dimensions 100x1x1 to do a single inference, or (BatchSize)x100x1x1 for generating a batch of images. In this demo we'll start by generating 4 punks at a time.

## Step 3 â€” Creating a Gradio interface

At this point you can even run the code you have with `predict(<SOME_NUMBER>)`, and you'll find your freshly generated punks in your file system at `./punks.png`. To make a truly interactive demo, though, we'll build out a simple interface with Gradio. Our goals here are to:

- Set a slider input so users can choose the "seed" value
- Use an image component for our output to showcase the generated punks
- Use our `predict()` to take the seed and generate the images

With `gr.Interface()`, we can define all of that with a single function call:

```python
import gradio as gr

gr.Interface(
    predict,
    inputs=[
        gr.Slider(0, 1000, label='Seed', default=42),
    ],
    outputs="image",
).launch()
```


## Step 4 â€” Even more punks!

Generating 4 punks at a time is a good start, but maybe we'd like to control how many we want to make each time. Adding more inputs to our Gradio interface is as simple as adding another item to the `inputs` list that we pass to `gr.Interface`:

```python
gr.Interface(
    predict,
    inputs=[
        gr.Slider(0, 1000, label='Seed', default=42),
        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # Adding another slider!
    ],
    outputs="image",
).launch()
```

The new input will be passed to our `predict()` function, so we have to make some changes to that function to accept a new parameter:

```python
def predict(seed, num_punks):
    torch.manual_seed(seed)
    z = torch.randn(num_punks, 100, 1, 1)
    punks = model(z)
    save_image(punks, "punks.png", normalize=True)
    return 'punks.png'
```

When you relaunch your interface, you should see a second slider that'll let you control the number of punks!

## Step 5 - Polishing it up

Your Gradio app is pretty much good to go, but you can add a few extra things to really make it ready for the spotlight âœ¨

We can add some examples that users can easily try out by adding this to the `gr.Interface`:

```python
gr.Interface(
    # ...
    # keep everything as it is, and then add
    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],
).launch(cache_examples=True) # cache_examples is optional
```

The `examples` parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the `inputs`. So in our case, `[seed, num_punks]`. Give it a try!

You can also try adding a `title`, `description`, and `article` to the `gr.Interface`. Each of those parameters accepts a string, so try it out and see what happens ðŸ‘€ `article` will also accept HTML, as [explored in a previous guide](/guides/key-features/#descriptive-content)!

When you're all done, you may end up with something like [this](https://nimaboscarino-cryptopunks.hf.space).

For reference, here is our full code:

```python
import torch
from torch import nn
from huggingface_hub import hf_hub_download
from torchvision.utils import save_image
import gradio as gr

class Generator(nn.Module):
    # Refer to the link below for explanations about nc, nz, and ngf
    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs
    def __init__(self, nc=4, nz=100, ngf=64):
        super(Generator, self).__init__()
        self.network = nn.Sequential(
            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),
            nn.Tanh(),
        )

    def forward(self, input):
        output = self.network(input)
        return output

model = Generator()
weights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')
model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available

def predict(seed, num_punks):
    torch.manual_seed(seed)
    z = torch.randn(num_punks, 100, 1, 1)
    punks = model(z)
    save_image(punks, "punks.png", normalize=True)
    return 'punks.png'

gr.Interface(
    predict,
    inputs=[
        gr.Slider(0, 1000, label='Seed', default=42),
        gr.Slider(4, 64, label='Number of Punks', step=1, default=10),
    ],
    outputs="image",
    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],
).launch(cache_examples=True)
```

---

Congratulations! You've built out your very own GAN-powered CryptoPunks generator, with a fancy Gradio interface that makes it easy for anyone to use. Now you can [scour the Hub for more GANs](https://huggingface.co/models?other=gan) (or train your own) and continue making even more awesome demos ðŸ¤—

---

<!-- Source: guides/11_other-tutorials/creating-a-dashboard-from-bigquery-data.md -->
# Creating a Real-Time Dashboard from BigQuery Data

Tags: TABULAR, DASHBOARD, PLOTS

[Google BigQuery](https://cloud.google.com/bigquery) is a cloud-based service for processing very large data sets. It is a serverless and highly scalable data warehousing solution that enables users to analyze data [using SQL-like queries](https://www.oreilly.com/library/view/google-bigquery-the/9781492044451/ch01.html).

In this tutorial, we will show you how to query a BigQuery dataset in Python and display the data in a dashboard that updates in real time using `gradio`. The dashboard will look like this:

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/bigquery-dashboard.gif">

We'll cover the following steps in this Guide:

1. Setting up your BigQuery credentials
2. Using the BigQuery client
3. Building the real-time dashboard (in just _7 lines of Python_)

We'll be working with the [New York Times' COVID dataset](https://www.nytimes.com/interactive/2021/us/covid-cases.html) that is available as a public dataset on BigQuery. The dataset, named `covid19_nyt.us_counties` contains the latest information about the number of confirmed cases and deaths from COVID across US counties.

**Prerequisites**: This Guide uses [Gradio Blocks](/guides/quickstart/#blocks-more-flexibility-and-control), so make your are familiar with the Blocks class.

## Setting up your BigQuery Credentials

To use Gradio with BigQuery, you will need to obtain your BigQuery credentials and use them with the [BigQuery Python client](https://pypi.org/project/google-cloud-bigquery/). If you already have BigQuery credentials (as a `.json` file), you can skip this section. If not, you can do this for free in just a couple of minutes.

1. First, log in to your Google Cloud account and go to the Google Cloud Console (https://console.cloud.google.com/)

2. In the Cloud Console, click on the hamburger menu in the top-left corner and select "APIs & Services" from the menu. If you do not have an existing project, you will need to create one.

3. Then, click the "+ Enabled APIs & services" button, which allows you to enable specific services for your project. Search for "BigQuery API", click on it, and click the "Enable" button. If you see the "Manage" button, then the BigQuery is already enabled, and you're all set.

4. In the APIs & Services menu, click on the "Credentials" tab and then click on the "Create credentials" button.

5. In the "Create credentials" dialog, select "Service account key" as the type of credentials to create, and give it a name. Also grant the service account permissions by giving it a role such as "BigQuery User", which will allow you to run queries.

6. After selecting the service account, select the "JSON" key type and then click on the "Create" button. This will download the JSON key file containing your credentials to your computer. It will look something like this:

```json
{
	"type": "service_account",
	"project_id": "your project",
	"private_key_id": "your private key id",
	"private_key": "private key",
	"client_email": "email",
	"client_id": "client id",
	"auth_uri": "https://accounts.google.com/o/oauth2/auth",
	"token_uri": "https://accounts.google.com/o/oauth2/token",
	"auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
	"client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/email_id"
}
```

## Using the BigQuery Client

Once you have the credentials, you will need to use the BigQuery Python client to authenticate using your credentials. To do this, you will need to install the BigQuery Python client by running the following command in the terminal:

```bash
pip install google-cloud-bigquery[pandas]
```

You'll notice that we've installed the pandas add-on, which will be helpful for processing the BigQuery dataset as a pandas dataframe. Once the client is installed, you can authenticate using your credentials by running the following code:

```py
from google.cloud import bigquery

client = bigquery.Client.from_service_account_json("path/to/key.json")
```

With your credentials authenticated, you can now use the BigQuery Python client to interact with your BigQuery datasets.

Here is an example of a function which queries the `covid19_nyt.us_counties` dataset in BigQuery to show the top 20 counties with the most confirmed cases as of the current day:

```py
import numpy as np

QUERY = (
    'SELECT * FROM `bigquery-public-data.covid19_nyt.us_counties` '
    'ORDER BY date DESC,confirmed_cases DESC '
    'LIMIT 20')

def run_query():
    query_job = client.query(QUERY)
    query_result = query_job.result()
    df = query_result.to_dataframe()
    # Select a subset of columns
    df = df[["confirmed_cases", "deaths", "county", "state_name"]]
    # Convert numeric columns to standard numpy types
    df = df.astype({"deaths": np.int64, "confirmed_cases": np.int64})
    return df
```

## Building the Real-Time Dashboard

Once you have a function to query the data, you can use the `gr.DataFrame` component from the Gradio library to display the results in a tabular format. This is a useful way to inspect the data and make sure that it has been queried correctly.

Here is an example of how to use the `gr.DataFrame` component to display the results. By passing in the `run_query` function to `gr.DataFrame`, we instruct Gradio to run the function as soon as the page loads and show the results. In addition, you also pass in the keyword `every` to tell the dashboard to refresh every hour (60\*60 seconds).

```py
import gradio as gr

with gr.Blocks() as demo:
    gr.DataFrame(run_query, every=gr.Timer(60*60))

demo.launch()
```

Perhaps you'd like to add a visualization to our dashboard. You can use the `gr.ScatterPlot()` component to visualize the data in a scatter plot. This allows you to see the relationship between different variables such as case count and case deaths in the dataset and can be useful for exploring the data and gaining insights. Again, we can do this in real-time
by passing in the `every` parameter.

Here is a complete example showing how to use the `gr.ScatterPlot` to visualize in addition to displaying data with the `gr.DataFrame`

```py
import gradio as gr

with gr.Blocks() as demo:
    gr.Markdown("# ðŸ’‰ Covid Dashboard (Updated Hourly)")
    with gr.Row():
        gr.DataFrame(run_query, every=gr.Timer(60*60))
        gr.ScatterPlot(run_query, every=gr.Timer(60*60), x="confirmed_cases",
                        y="deaths", tooltip="county", width=500, height=500)

demo.queue().launch()  # Run the demo with queuing enabled
```

---

<!-- Source: guides/11_other-tutorials/creating-a-dashboard-from-supabase-data.md -->
# Create a Dashboard from Supabase Data

Tags: TABULAR, DASHBOARD, PLOTS

[Supabase](https://supabase.com/) is a cloud-based open-source backend that provides a PostgreSQL database, authentication, and other useful features for building web and mobile applications. In this tutorial, you will learn how to read data from Supabase and plot it in **real-time** on a Gradio Dashboard.

**Prerequisites:** To start, you will need a free Supabase account, which you can sign up for here: [https://app.supabase.com/](https://app.supabase.com/)

In this end-to-end guide, you will learn how to:

- Create tables in Supabase
- Write data to Supabase using the Supabase Python Client
- Visualize the data in a real-time dashboard using Gradio

If you already have data on Supabase that you'd like to visualize in a dashboard, you can skip the first two sections and go directly to [visualizing the data](#visualize-the-data-in-a-real-time-gradio-dashboard)!

## Create a table in Supabase

First of all, we need some data to visualize. Following this [excellent guide](https://supabase.com/blog/loading-data-supabase-python), we'll create fake commerce data and put it in Supabase.

1\. Start by creating a new project in Supabase. Once you're logged in, click the "New Project" button

2\. Give your project a name and database password. You can also choose a pricing plan (for our purposes, the Free Tier is sufficient!)

3\. You'll be presented with your API keys while the database spins up (can take up to 2 minutes).

4\. Click on "Table Editor" (the table icon) in the left pane to create a new table. We'll create a single table called `Product`, with the following schema:

<center>
<table>
<tr><td>product_id</td><td>int8</td></tr>
<tr><td>inventory_count</td><td>int8</td></tr>
<tr><td>price</td><td>float8</td></tr>
<tr><td>product_name</td><td>varchar</td></tr>
</table>
</center>

5\. Click Save to save the table schema.

Our table is now ready!

## Write data to Supabase

The next step is to write data to a Supabase dataset. We will use the Supabase Python library to do this.

6\. Install `supabase` by running the following command in your terminal:

```bash
pip install supabase
```

7\. Get your project URL and API key. Click the Settings (gear icon) on the left pane and click 'API'. The URL is listed in the Project URL box, while the API key is listed in Project API keys (with the tags `service_role`, `secret`)

8\. Now, run the following Python script to write some fake data to the table (note you have to put the values of `SUPABASE_URL` and `SUPABASE_SECRET_KEY` from step 7):

```python
import supabase

# Initialize the Supabase client
client = supabase.create_client('SUPABASE_URL', 'SUPABASE_SECRET_KEY')

# Define the data to write
import random

main_list = []
for i in range(10):
    value = {'product_id': i,
             'product_name': f"Item {i}",
             'inventory_count': random.randint(1, 100),
             'price': random.random()*100
            }
    main_list.append(value)

# Write the data to the table
data = client.table('Product').insert(main_list).execute()
```

Return to your Supabase dashboard and refresh the page, you should now see 10 rows populated in the `Product` table!

## Visualize the Data in a Real-Time Gradio Dashboard

Finally, we will read the data from the Supabase dataset using the same `supabase` Python library and create a realtime dashboard using `gradio`.

Note: We repeat certain steps in this section (like creating the Supabase client) in case you did not go through the previous sections. As described in Step 7, you will need the project URL and API Key for your database.

9\. Write a function that loads the data from the `Product` table and returns it as a pandas Dataframe:

```python
import supabase
import pandas as pd

client = supabase.create_client('SUPABASE_URL', 'SUPABASE_SECRET_KEY')

def read_data():
    response = client.table('Product').select("*").execute()
    df = pd.DataFrame(response.data)
    return df
```

10\. Create a small Gradio Dashboard with 2 Barplots that plots the prices and inventories of all of the items every minute and updates in real-time:

```python
import gradio as gr

with gr.Blocks() as dashboard:
    with gr.Row():
        gr.BarPlot(read_data, x="product_id", y="price", title="Prices", every=gr.Timer(60))
        gr.BarPlot(read_data, x="product_id", y="inventory_count", title="Inventory", every=gr.Timer(60))

dashboard.queue().launch()
```

Notice that by passing in a function to `gr.BarPlot()`, we have the BarPlot query the database as soon as the web app loads (and then again every 60 seconds because of the `every` parameter). Your final dashboard should look something like this:

<gradio-app space="gradio/supabase"></gradio-app>

## Conclusion

That's it! In this tutorial, you learned how to write data to a Supabase dataset, and then read that data and plot the results as bar plots. If you update the data in the Supabase database, you'll notice that the Gradio dashboard will update within a minute.

Try adding more plots and visualizations to this example (or with a different dataset) to build a more complex dashboard!

---

<!-- Source: guides/11_other-tutorials/creating-a-realtime-dashboard-from-google-sheets.md -->
# Creating a Real-Time Dashboard from Google Sheets

Tags: TABULAR, DASHBOARD, PLOTS

[Google Sheets](https://www.google.com/sheets/about/) are an easy way to store tabular data in the form of spreadsheets. With Gradio and pandas, it's easy to read data from public or private Google Sheets and then display the data or plot it. In this blog post, we'll build a small _real-time_ dashboard, one that updates when the data in the Google Sheets updates.

Building the dashboard itself will just be 9 lines of Python code using Gradio, and our final dashboard will look like this:

<gradio-app space="gradio/line-plot"></gradio-app>

**Prerequisites**: This Guide uses [Gradio Blocks](/guides/quickstart/#blocks-more-flexibility-and-control), so make you are familiar with the Blocks class.

The process is a little different depending on if you are working with a publicly accessible or a private Google Sheet. We'll cover both, so let's get started!

## Public Google Sheets

Building a dashboard from a public Google Sheet is very easy, thanks to the [`pandas` library](https://pandas.pydata.org/):

1\. Get the URL of the Google Sheets that you want to use. To do this, simply go to the Google Sheets, click on the "Share" button in the top-right corner, and then click on the "Get shareable link" button. This will give you a URL that looks something like this:

```html
https://docs.google.com/spreadsheets/d/1UoKzzRzOCt-FXLLqDKLbryEKEgllGAQUEJ5qtmmQwpU/edit#gid=0
```

2\. Now, let's modify this URL and then use it to read the data from the Google Sheets into a Pandas DataFrame. (In the code below, replace the `URL` variable with the URL of your public Google Sheet):

```python
import pandas as pd

URL = "https://docs.google.com/spreadsheets/d/1UoKzzRzOCt-FXLLqDKLbryEKEgllGAQUEJ5qtmmQwpU/edit#gid=0"
csv_url = URL.replace('/edit#gid=', '/export?format=csv&gid=')

def get_data():
    return pd.read_csv(csv_url)
```

3\. The data query is a function, which means that it's easy to display it real-time using the `gr.DataFrame` component, or plot it real-time using the `gr.LinePlot` component (of course, depending on the data, a different plot may be appropriate). To do this, just pass the function into the respective components, and set the `every` parameter based on how frequently (in seconds) you would like the component to refresh. Here's the Gradio code:

```python
import gradio as gr

with gr.Blocks() as demo:
    gr.Markdown("# ðŸ“ˆ Real-Time Line Plot")
    with gr.Row():
        with gr.Column():
            gr.DataFrame(get_data, every=gr.Timer(5))
        with gr.Column():
            gr.LinePlot(get_data, every=gr.Timer(5), x="Date", y="Sales", y_title="Sales ($ millions)", overlay_point=True, width=500, height=500)

demo.queue().launch()  # Run the demo with queuing enabled
```

And that's it! You have a dashboard that refreshes every 5 seconds, pulling the data from your Google Sheet.

## Private Google Sheets

For private Google Sheets, the process requires a little more work, but not that much! The key difference is that now, you must authenticate yourself to authorize access to the private Google Sheets.

### Authentication

To authenticate yourself, obtain credentials from Google Cloud. Here's [how to set up google cloud credentials](https://developers.google.com/workspace/guides/create-credentials):

1\. First, log in to your Google Cloud account and go to the Google Cloud Console (https://console.cloud.google.com/)

2\. In the Cloud Console, click on the hamburger menu in the top-left corner and select "APIs & Services" from the menu. If you do not have an existing project, you will need to create one.

3\. Then, click the "+ Enabled APIs & services" button, which allows you to enable specific services for your project. Search for "Google Sheets API", click on it, and click the "Enable" button. If you see the "Manage" button, then Google Sheets is already enabled, and you're all set.

4\. In the APIs & Services menu, click on the "Credentials" tab and then click on the "Create credentials" button.

5\. In the "Create credentials" dialog, select "Service account key" as the type of credentials to create, and give it a name. **Note down the email of the service account**

6\. After selecting the service account, select the "JSON" key type and then click on the "Create" button. This will download the JSON key file containing your credentials to your computer. It will look something like this:

```json
{
	"type": "service_account",
	"project_id": "your project",
	"private_key_id": "your private key id",
	"private_key": "private key",
	"client_email": "email",
	"client_id": "client id",
	"auth_uri": "https://accounts.google.com/o/oauth2/auth",
	"token_uri": "https://accounts.google.com/o/oauth2/token",
	"auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
	"client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/email_id"
}
```

### Querying

Once you have the credentials `.json` file, you can use the following steps to query your Google Sheet:

1\. Click on the "Share" button in the top-right corner of the Google Sheet. Share the Google Sheets with the email address of the service from Step 5 of authentication subsection (this step is important!). Then click on the "Get shareable link" button. This will give you a URL that looks something like this:

```html
https://docs.google.com/spreadsheets/d/1UoKzzRzOCt-FXLLqDKLbryEKEgllGAQUEJ5qtmmQwpU/edit#gid=0
```

2\. Install the [`gspread` library](https://docs.gspread.org/en/v5.7.0/), which makes it easy to work with the [Google Sheets API](https://developers.google.com/sheets/api/guides/concepts) in Python by running in the terminal: `pip install gspread`

3\. Write a function to load the data from the Google Sheet, like this (replace the `URL` variable with the URL of your private Google Sheet):

```python
import gspread
import pandas as pd

# Authenticate with Google and get the sheet
URL = 'https://docs.google.com/spreadsheets/d/1_91Vps76SKOdDQ8cFxZQdgjTJiz23375sAT7vPvaj4k/edit#gid=0'

gc = gspread.service_account("path/to/key.json")
sh = gc.open_by_url(URL)
worksheet = sh.sheet1

def get_data():
    values = worksheet.get_all_values()
    df = pd.DataFrame(values[1:], columns=values[0])
    return df

```

4\. The data query is a function, which means that it's easy to display it real-time using the `gr.DataFrame` component, or plot it real-time using the `gr.LinePlot` component (of course, depending on the data, a different plot may be appropriate). To do this, we just pass the function into the respective components, and set the `every` parameter based on how frequently (in seconds) we would like the component to refresh. Here's the Gradio code:

```python
import gradio as gr

with gr.Blocks() as demo:
    gr.Markdown("# ðŸ“ˆ Real-Time Line Plot")
    with gr.Row():
        with gr.Column():
            gr.DataFrame(get_data, every=gr.Timer(5))
        with gr.Column():
            gr.LinePlot(get_data, every=gr.Timer(5), x="Date", y="Sales", y_title="Sales ($ millions)", overlay_point=True, width=500, height=500)

demo.queue().launch()  # Run the demo with queuing enabled
```

You now have a Dashboard that refreshes every 5 seconds, pulling the data from your Google Sheet.

## Conclusion

And that's all there is to it! With just a few lines of code, you can use `gradio` and other libraries to read data from a public or private Google Sheet and then display and plot the data in a real-time dashboard.

---

<!-- Source: guides/11_other-tutorials/deploying-gradio-with-disco.md -->
# Self-Hosting a Gradio app with Disco

Tags: DEPLOYMENT


### Introduction

Gradio is a fantastic open-source Python library that allows you to build and share machine learning apps and demos with just a few lines of code. While Gradio offers free hosting on [Hugging Face Spaces](https://huggingface.co/spaces), you might want to deploy your app on your own server for more control, or to integrate it with other services.

This tutorial will guide you through deploying a Gradio application on your own server using [Disco](https://disco.cloud/), an open-source platform that simplifies the deployment process. With Disco, you can enjoy the benefits of self-hosting without the usual complexities of server setup and maintenance. By the end, you'll have a working Gradio app deployed on your own server with automatic HTTPS and continuous deployment from GitHub.

### Prerequisites

Before you begin, make sure you have the following:

- A server with a fresh install of Ubuntu (4GB of RAM or more is recommended). You can get one from providers like [DigitalOcean](https://www.digitalocean.com/), [Hetzner](https://www.hetzner.com/cloud) or [AWS EC2](https://aws.amazon.com/ec2/).
- A domain name that you can configure.
- A GitHub account.
- Basic knowledge of the command line.

### Step 1: Create a Server

First, you'll need a server to host your Gradio app. Choose a provider and create a new server with Ubuntu 24.04 as the operating system.

Once your server is up and running, take note of its IP address. You'll need it for the next step.

### Step 2: Configure DNS Settings

Before going further, you need to set up two domain names. Go to your domain registrar's DNS management panel and add these records:

1.  A domain for your Disco server (e.g., `disco.example.com`).
2.  A domain for your Gradio application (e.g., `gradio.example.com`).

For the server domain, create an **A record** pointing to your server's IP address:

- **Type**: A
- **Name**: disco
- **Value**: `<your_server_ip_address>`

For the application domain, create a **CNAME record** pointing to your server domain:

- **Type**: CNAME
- **Name**: gradio
- **Value**: `disco.example.com`

DNS changes can take a few minutes to propagate. You can verify that your server domain is resolving to the correct IP address by running `ping disco.example.com`

### Step 3: Test Your Server Connection

Now that your DNS is set up, let's test the SSH connection to your server from your local machine. This ensures you can access it before we hand things over to Disco.

```bash
# Replace with your server domain
ssh root@disco.example.com
```

If the connection is successful, great! **This is the last time you'll need to SSH into this server manually.** Now, exit the SSH session to return to your local machine. This is a crucial step!

```bash
exit
```

### Step 4: Install the Disco CLI on Your Local Machine

**Important:** From this point forward, all commands should be run from your **local machine's terminal**. You will not need to SSH into your server again.

Let's install the Disco command-line interface (CLI) on your local machine. This is the tool you'll use to manage your deployments.

```bash
curl https://cli-assets.letsdisco.dev/install.sh | sh
```

After the installation is complete, verify it's working by running:

```bash
disco --version
```

### Step 5: Initialize Your Server with Disco

Now, from your local machine, let's set up Disco on your server using the domain you configured.

```bash
# Replace with your server domain
disco init root@disco.example.com
```

This command will:

- Connect to your server using SSH.
- Install Docker.
- Set up the Disco server.
- Configure the initial SSL certificate.

Tip: Disco will automatically try to use your default SSH keys. If you use a non-standard key, you can specify the path with the `-i` flag, like so: `disco init -i /path/to/your/ssh/key root@disco.example.com`

### Step 6: Fork the Example Gradio App

For this tutorial, we'll use an example Gradio application. Go to this [example Gradio app repository](https://github.com/letsdiscodev/example-gradio-site) on GitHub and click the "Fork" button to create a copy of it in your own GitHub account.

### Step 7: Connect Disco to GitHub

To allow Disco to deploy your application from GitHub, you need to connect your GitHub account. Run the following command on your local machine:

```bash
disco github:apps:add
```

This command will open a browser window where you can authorize Disco with GitHub. You'll need to:

1. Give the GitHub application a name (any name will do).
2. Select the repository you just forked (`example-gradio-site`).
3. Click "Install".

### Step 8: Deploy Your Gradio App

Now you're ready to deploy your Gradio app. We'll use the `projects:add` command on your local machine. Below, replace `<your_github_username>` with your GitHub username and `gradio.example.com` with the application domain you configured earlier.

```bash
disco projects:add \
  --name gradio-app \
  --github <your_github_username>/example-gradio-site \
  --domain gradio.example.com
```

Disco will automatically pull your code from GitHub, build the Docker container, deploy it to your server, and set up HTTPS with Let's Encrypt for your domain.

### Step 9: Test Your Deployed App

Once the deployment is complete, open your web browser and navigate to your application's domain: `https://gradio.example.com`. You should see your Gradio app running live!

### Making Changes and Automatic Deployment

One of the best features of Disco is automatic deployment. Whenever you push changes to your GitHub repository, Disco will detect them, rebuild your application, and deploy it automatically.

To test this, modify the `app.py` file in your forked repository, then commit and push the changes to GitHub. Within seconds, your deployed app will be updated.

### Conclusion

Congratulations! You have successfully deployed a Gradio application on your own server using Disco. You now have a fully managed deployment pipeline with automatic HTTPS, fast deployments triggered by Git pushes, and complete control over your server and application.

This setup provides the best of both worlds: the flexibility and cost-effectiveness of self-hosting combined with the convenience of a platform-as-a-service. For more advanced configurations and features, be sure to check out the [Disco documentation](https://docs.letsdisco.dev/) and the [Gradio documentation](https://www.gradio.app/docs).

---

<!-- Source: guides/11_other-tutorials/deploying-gradio-with-docker.md -->
# Deploying a Gradio app with Docker

Tags: DEPLOYMENT, DOCKER


### Introduction

Gradio is a powerful and intuitive Python library designed for creating web apps that showcase machine learning models. These web apps can be run locally, or [deployed on Hugging Face Spaces ](https://huggingface.co/spaces)for free. Or, you can deploy them on your servers in Docker containers. Dockerizing Gradio apps offers several benefits:

- **Consistency**: Docker ensures that your Gradio app runs the same way, irrespective of where it is deployed, by packaging the application and its environment together.
- **Portability**: Containers can be easily moved across different systems or cloud environments.
- **Scalability**: Docker works well with orchestration systems like Kubernetes, allowing your app to scale up or down based on demand.

## How to Dockerize a Gradio App

Let's go through a simple example to understand how to containerize a Gradio app using Docker.

#### Step 1: Create Your Gradio App

First, we need a simple Gradio app. Let's create a Python file named `app.py` with the following content:

```python
import gradio as gr

def greet(name):
    return f"Hello {name}!"

iface = gr.Interface(fn=greet, inputs="text", outputs="text").launch()
```

This app creates a simple interface that greets the user by name.

#### Step 2: Create a Dockerfile

Next, we'll create a Dockerfile to specify how our app should be built and run in a Docker container. Create a file named `Dockerfile` in the same directory as your app with the following content:

```dockerfile
FROM python:3.10-slim

WORKDIR /usr/src/app
COPY . .
RUN pip install --no-cache-dir gradio
EXPOSE 7860
ENV GRADIO_SERVER_NAME="0.0.0.0"

CMD ["python", "app.py"]
```

This Dockerfile performs the following steps:
- Starts from a Python 3.10 slim image.
- Sets the working directory and copies the app into the container.
- Installs Gradio (you should install all other requirements as well).
- Exposes port 7860 (Gradio's default port).
- Sets the `GRADIO_SERVER_NAME` environment variable to ensure Gradio listens on all network interfaces.
- Specifies the command to run the app.

#### Step 3: Build and Run Your Docker Container

With the Dockerfile in place, you can build and run your container:

```bash
docker build -t gradio-app .
docker run -p 7860:7860 gradio-app
```

Your Gradio app should now be accessible at `http://localhost:7860`.

## Important Considerations

When running Gradio applications in Docker, there are a few important things to keep in mind:

#### Running the Gradio app on `"0.0.0.0"` and exposing port 7860

In the Docker environment, setting `GRADIO_SERVER_NAME="0.0.0.0"` as an environment variable (or directly in your Gradio app's `launch()` function) is crucial for allowing connections from outside the container. And the `EXPOSE 7860` directive in the Dockerfile tells Docker to expose Gradio's default port on the container to enable external access to the Gradio app. 

#### Enable Stickiness for Multiple Replicas

When deploying Gradio apps with multiple replicas, such as on AWS ECS, it's important to enable stickiness with `sessionAffinity: ClientIP`. This ensures that all requests from the same user are routed to the same instance. This is important because Gradio's communication protocol requires multiple separate connections from the frontend to the backend in order for events to be processed correctly. (If you use Terraform, you'll want to add a [stickiness block](https://registry.terraform.io/providers/hashicorp/aws/3.14.1/docs/resources/lb_target_group#stickiness) into your target group definition.)

#### Deploying Behind a Proxy

If you're deploying your Gradio app behind a proxy, like Nginx, it's essential to configure the proxy correctly. Gradio provides a [Guide that walks through the necessary steps](https://www.gradio.app/guides/running-gradio-on-your-web-server-with-nginx). This setup ensures your app is accessible and performs well in production environments.

---

<!-- Source: guides/11_other-tutorials/deploying-gradio-with-modal.md -->
# Deploying a Gradio app with Modal

Tags: DEPLOYMENT, MODAL


### Introduction

Gradio is a great way to test and demo your machine learning apps using a simple and intuitive Python API. When combined with Modal's developer-first cloud infrastructure, you can leverage powerful GPUs to run larger models faster. And you don't need an account with a cloud provider or any config files.

In this tutorial, we will walk you through setting up a Modal account, deploying a simple Gradio app on Modal, and discuss some of the nuance around Gradio's sticky session requirement and handling concurrency.

## Deploying a simple Gradio app on Modal
Let's deploy a Gradio-style "Hello, world" app that lets a user input their name and then responds with a short greeting. We're not going to use this code as-is in our app, but it's useful to see what the initial Gradio version looks like.

```python
import gradio as gr

# A simple Gradio interface for a greeting function
def greet(name):
    return f"Hello {name}!"

demo = gr.Interface(fn=greet, inputs="text", outputs="text")
demo.launch()
```

To deploy this app on Modal you'll need to
- define your container image,
- wrap the Gradio app in a Modal Function,
- and deploy it using Modal's CLI!

### Prerequisite: Install and set up Modal

Before you get started, you'll need to create a Modal account if you don't already have one. Then you can set up your environment by authenticating with those account credentials.

- Sign up at [modal.com](https://www.modal.com?utm_source=partner&utm_medium=github&utm_campaign=livekit). 
- Install the Modal client in your local development environment.
```bash
pip install modal
```
- Authenticate your account.
```
modal setup
```

Great, now we can start building our app!

### Step 1: Define our  `modal.Image`
To start, let's make a new file named `gradio_app.py`, import `modal`, and define our image. Modal `Images` are defined by sequentially calling methods on our `Image` instance. 

For this simple app, we'll 
- start with the `debian_slim` image,
- choose a Python version (3.12),
- and install the dependencies - only `fastapi` and `gradio`.

```python
import modal

app = modal.App("gradio-app")
web_image = modal.Image.debian_slim(python_version="3.12").uv_pip_install(
    "fastapi[standard]",
    "gradio",
)
```

Note, that you don't need to install `gradio` or `fastapi` in your local environement - only `modal` is required locally.

### Step 2: Wrap the Gradio app in a Modal-deployed FastAPI app
Like many Gradio apps, the example above is run by calling `launch()` on our demo at the end of the script. However, Modal doesn't run scripts, it runs functions - serverless functions to be exact.

To get Modal to serve our `demo`, we can leverage Gradio and Modal's support for `fastapi` apps. We do this with the `@modal.asgi_app()` function decorator which deploys the web app returned by the function. And we use the `mount_gradio_app` function to add our Gradio `demo` as a route in the web app.

```python
with web_image.imports():
	import gradio as gr
    from gradio.routes import mount_gradio_app
    from fastapi import FastAPI
     
@app.function(
    image=web_image,
    max_containers = 1, # we'll come to this later 
)
@modal.concurrent(max_inputs=100) # allow multiple users at one time
@modal.asgi_app()
def ui():
    """A simple Gradio interface for a greeting function."""
    def greet(name):
	    return f"Hello {name}!"
	
	demo = gr.Interface(fn=greet, inputs="text", outputs="text")

    return mount_gradio_app(app=FastAPI(), blocks=demo, path="/")
```

Let's quickly review what's going on here:
- We use the `Image.imports` context manager to define our imports. These will be available when your function runs in the cloud.
- We move our code inside a Python function, `ui`, and decorate it with `@app.function` which wraps it as a Modal serverless Function. We provide the image and other parameters (we'll cover this later) as inputs to the decorator.
- We add the `@modal.concurrent` decorator which allows multiple requests per container to be processed at the same time.
- We add the `@modal.asgi_app` decorator which tells Modal that this particular function is serving an ASGI app (here a `fastapi` app). To use this decorator, your ASGI app needs to be the return value from the function.

### Step 3: Deploying on Modal
To deploy the app, just run the following command:
```bash
modal deploy <path-to-file>
```

The first time you run your app, Modal will build and cache the image which, takes about 30 seconds. As long as you don't change the image, subsequent deployments will only take a few seconds.

After the image builds Modal will print the URL to your webapp and to your Modal dashboard. The webapp URL should look something like `https://{workspace}-{environment}--gradio-app-ui.modal.run`. Paste it into your web browser a try out your app!

## Important Considerations

### Sticky Sessions
Modal Functions are serverless which means that each client request is considered independent. While this facilitates autoscaling, it can also mean that extra care should be taken if your application requires any sort of server-side statefulness.

Gradio relies on a REST API, which is itself stateless. But it does require sticky sessions, meaning that every request from a particular client must be routed to the same container. However, Modal does not make any guarantees in this regard.

A simple way to satisfy this constraint is to set `max_containers = 1` in the `@app.function` decorator and setting the `max_inputs` argument of `@modal.concurrent` to a fairly large number - as we did above. This means that Modal won't spin up more than one container to serve requests to your app which effectively satisfies the sticky session requirement.

### Concurrency and Queues

Both Gradio and Modal have concepts of concurrency and queues, and getting the most of out of your compute resources requires understanding how these interact.

Modal queues client requests to each deployed Function and simultaneously executes requests up to the concurrency limit for that Function. If requests come in and the concurrency limit is already satisfied, Modal will spin up a new container - up to the maximum set for the Function. In our case, our Gradio app is represented by one Modal Function, so all requests share one queue and concurrency limit. Therefore Modal constrains the _total_ number of requests running at one time, regardless of what they are doing.

Gradio on the other hand, allows developers to utilize multiple queues each with its own concurrency limit. One or more event listeners can then be assigned to a queue which is useful to manage GPU resources for computationally expensive requests.

Thinking carefully about how these queues and limits interact can help you optimize your app's performance and resource optimization while avoiding unwanted results like shared or lost state.

### Creating a GPU Function

Another option to manage GPU utilization is to deploy your GPU computations in their own Modal Function and calling this remote Function from inside your Gradio app. This allows you to take full advantage of Modal's serverless autoscaling while routing all of the client HTTP requests to a single Gradio CPU container.

---

<!-- Source: guides/11_other-tutorials/developing-faster-with-reload-mode.md -->
# Developing Faster with Reload Mode and Vibe Mode

**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/blocks-and-event-listeners).

This guide covers hot reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.

## Why Hot Reloading?

When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.

To make it faster and more convenient to write your code, we've made it easier to "reload" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous "magic command" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).

This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster.

## Python IDE Reload ðŸ”¥

If you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:

```python
import gradio as gr

with gr.Blocks() as demo:
    gr.Markdown("# Greetings from Gradio!")
    inp = gr.Textbox(placeholder="What is your name?")
    out = gr.Textbox()

    inp.change(fn=lambda x: f"Welcome, {x}!",
               inputs=inp,
               outputs=out)

if __name__ == "__main__":
    demo.launch()
```

The problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.

Instead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:

In the terminal, run `gradio run.py`. That's it!

Now, you'll see that after you'll see something like this:

```bash
Watching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'

Running on local URL:  http://127.0.0.1:7860
```

The important part here is the line that says `Watching...` What's happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically ðŸ¥³

Tip: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show_error` in `launch()` will not be reflected in the app.

There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:

```python
import gradio as gr

with gr.Blocks() as my_demo:
    gr.Markdown("# Greetings from Gradio!")
    inp = gr.Textbox(placeholder="What is your name?")
    out = gr.Textbox()

    inp.change(fn=lambda x: f"Welcome, {x}!",
               inputs=inp,
               outputs=out)

if __name__ == "__main__":
    my_demo.launch()
```

Then you would launch it in reload mode like this: `gradio run.py --demo-name=my_demo`.

By default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:

1. Configure encoding declaration of python script, for example: `# -*- coding: cp1252 -*-`
2. Confirm that your code editor has identified that encoding format. 
3. Run like this: `gradio run.py --encoding cp1252`

ðŸ”¥ If your application accepts command line arguments, you can pass them in as well. Here's an example:

```python
import gradio as gr
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--name", type=str, default="User")
args, unknown = parser.parse_known_args()

with gr.Blocks() as demo:
    gr.Markdown(f"# Greetings {args.name}!")
    inp = gr.Textbox()
    out = gr.Textbox()

    inp.change(fn=lambda x: x, inputs=inp, outputs=out)

if __name__ == "__main__":
    demo.launch()
```

Which you could run like this: `gradio run.py --name Gretel`

As a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code. Meaning that this can be useful if you decide to [contribute to Gradio itself](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md) âœ…


## Controlling the Reload ðŸŽ›ï¸

By default, reload mode will re-run your entire script for every change you make.
But there are some cases where this is not desirable.
For example, loading a machine learning model should probably only happen once to save time. There are also some Python libraries that use C or Rust extensions that throw errors when they are reloaded, like `numpy` and `tiktoken`.

In these situations, you can place code that you do not want to be re-run inside an `if gr.NO_RELOAD:`  codeblock. Here's an example of how you can use it to only load a transformers model once during the development process.

Tip: The value of `gr.NO_RELOAD` is `True`. So you don't have to change your script when you are done developing and want to run it in production. Simply run the file with `python` instead of `gradio`.

```python
import gradio as gr

if gr.NO_RELOAD:
	from transformers import pipeline
	pipe = pipeline("text-classification", model="cardiffnlp/twitter-roberta-base-sentiment-latest")

demo = gr.Interface(lambda s: {d["label"]: d["score"] for d in pipe(s)}, gr.Textbox(), gr.Label())

if __name__ == "__main__":
    demo.launch()
```

## Vibe Mode

You can also enable Gradio's **Vibe Mode**, which, which provides an in-browser chat that can be used to write or edit your Gradio app using natural language. To enable this, simply run use the `--vibe` flag with Gradio, e.g. `gradio --vibe app.py`.

Vibe Mode lets you describe commands using natural language and have an LLM write or edit the code in your Gradio app. The LLM is powered by Hugging Face's [Inference Providers](https://huggingface.co/docs/inference-providers/en/index), so you must be logged into Hugging Face locally to use this. 

Note: When Vibe Mode is enabled, anyone who can access the Gradio endpoint can modify files and run arbitrary code on the host machine. Use only for local development.

## Jupyter Notebook Magic ðŸ”®

What about if you use Jupyter Notebooks (or Colab Notebooks, etc.) to develop code? We got something for you too!

We've developed a **magic command** that will create and run a Blocks demo for you. To use this, load the gradio extension at the top of your notebook:

`%load_ext gradio`

Then, in the cell that you are developing your Gradio demo, simply write the magic command **`%%blocks`** at the top, and then write the layout and components like you would normally:

```py
%%blocks

import gradio as gr

with gr.Blocks() as demo:
    gr.Markdown(f"# Greetings {args.name}!")
    inp = gr.Textbox()
    out = gr.Textbox()

    inp.change(fn=lambda x: x, inputs=inp, outputs=out)
```

Notice that:

- You do not need to launch your demo â€” Gradio does that for you automatically!

- Every time you rerun the cell, Gradio will re-render your app on the same port and using the same underlying web server. This means you'll see your changes _much, much faster_ than if you were rerunning the cell normally.

Here's what it looks like in a jupyter notebook:

![](https://gradio-builds.s3.amazonaws.com/demo-files/jupyter_reload.gif)

ðŸª„ This works in colab notebooks too! [Here's a colab notebook](https://colab.research.google.com/drive/1zAuWoiTIb3O2oitbtVb2_ekv1K6ggtC1?usp=sharing) where you can see the Blocks magic in action. Try making some changes and re-running the cell with the Gradio code!

Tip: You may have to use `%%blocks --share` in Colab to get the demo to appear in the cell.

The Notebook Magic is now the author's preferred way of building Gradio demos. Regardless of how you write Python code, we hope either of these methods will give you a much better development experience using Gradio.

---

## Next Steps

Now that you know how to develop quickly using Gradio, start building your own!

If you are looking for inspiration, try exploring demos other people have built with Gradio, [browse public Hugging Face Spaces](http://hf.space/) ðŸ¤—

---

<!-- Source: guides/11_other-tutorials/from-openapi-spec.md -->
# Creating a Gradio app from an OpenAPI Spec

Tags: OPENAPI, SPEC

## Introduction

**[OpenAPI](https://www.openapis.org/)** is a widely adopted standard for describing RESTful APIs in a machine-readable format, typically as a JSON  file. 

You can create a Gradio UI from an OpenAPI Spec **in 1 line of Python**, instantly generating an interactive web interface for any API, making it accessible for demos, testing, or sharing with non-developers, without writing custom frontend code.

## How it works

Gradio now provides a convenient function, `gr.load_openapi`, that can automatically generate a Gradio app from an OpenAPI v3 specification. This function parses the spec, creates UI components for each endpoint and parameter, and lets you interact with the API directly from your browser.

Here's a minimal example:

```python
import gradio as gr

demo = gr.load_openapi(
    openapi_spec="https://petstore3.swagger.io/api/v3/openapi.json",
    base_url="https://petstore3.swagger.io/api/v3",
    paths=["/pet.*"],
    methods=["get", "post"],
)

demo.launch()
```

**Parameters:**
- **openapi_spec**: URL, file path, or Python dictionary containing the OpenAPI v3 spec (JSON format only).
- **base_url**: The base URL for the API endpoints (e.g., `https://api.example.com/v1`).
- **paths** (optional): List of endpoint path patterns (supports regex) to include. If not set, all paths are included.
- **methods** (optional): List of HTTP methods (e.g., `["get", "post"]`) to include. If not set, all methods are included.

The generated app will display a sidebar with available endpoints and create interactive forms for each operation, letting you make API calls and view responses in real time.

## Next steps

Once your Gradio app is running, you can share the URL with others so they can try out the API through a friendly web interfaceâ€”no code required. For even more power, you can launch the app as an MCP (Model Control Protocol) server using [Gradio's MCP integration](https://www.gradio.app/guides/building-mcp-server-with-gradio), enabling programmatic access and orchestration of your API via the MCP ecosystem. This makes it easy to build, share, and automate API workflows with minimal effort.

---

<!-- Source: guides/11_other-tutorials/gradio-6-migration-guide.md -->
# Gradio 6 Migration Guide

We are excited to release Gradio 6, the latest major version of the Gradio library. Gradio 6 is significantly more performant, lighter, and easier to customize than previous versions of Gradio. The Gradio team is only planning on maintaining future versions of Gradio 6 so we encourage all developers to migrate to Gradio 6.x.

Gradio 6 includes several breaking changes that were made in order to standardize the Python API. This migration guide lists the breaking changes and the specific code changes needed in order to migrate. The easiest way to know whether you need to make changes is to upgrade your Gradio app to 5.50 (`pip install --upgrade gradio==5.50`). Gradio 5.50 emits deprecation warnings for any parameters removed in Gradio 6, allowing you to know whether your Gradio app will be compatible with Gradio 6.

Here, we walk through the breaking changes that were introduced in Gradio 6. Code snippets are provided, allowing you to migrate your code easily to Gradio 6. You can also copy-paste this document as Markdown if you are using an LLM to help migrate your code. 

## App-level Changes

### App-level parameters have been moved from `Blocks` to `launch()`

The `gr.Blocks` class constructor previously contained several parameters that applied to your entire Gradio app, specifically:

* `theme`: The theme for your Gradio app
* `css`: Custom CSS code as a string
* `css_paths`: Paths to custom CSS files
* `js`: Custom JavaScript code
* `head`: Custom HTML code to insert in the head of the page
* `head_paths`: Paths to custom HTML files to insert in the head

Since `gr.Blocks` can be nested and are not necessarily unique to a Gradio app, these parameters have now been moved to `Blocks.launch()`, which can only be called once for your entire Gradio app.

**Before (Gradio 5.x):**

```python
import gradio as gr

with gr.Blocks(
    theme=gr.themes.Soft(),
    css=".my-class { color: red; }",
) as demo:
    gr.Textbox(label="Input")

demo.launch()
```

**After (Gradio 6.x):**

```python
import gradio as gr

with gr.Blocks() as demo:
    gr.Textbox(label="Input")

demo.launch(
    theme=gr.themes.Soft(),
    css=".my-class { color: red; }",
)
```

This change makes it clearer that these parameters apply to the entire app and not to individual `Blocks` instances.

### `show_api` parameter replaced with `footer_links`

The `show_api` parameter in `launch()` has been replaced with a more flexible `footer_links` parameter that allows you to control which links appear in the footer of your Gradio app.

**In Gradio 5.x:**
- `show_api=True` (default) showed the API documentation link in the footer
- `show_api=False` hid the API documentation link

**In Gradio 6.x:**
- `footer_links` accepts a list of strings: `["api", "gradio", "settings"]`
- You can now control precisely which footer links are shown:
  - `"api"`: Shows the API documentation link
  - `"gradio"`: Shows the "Built with Gradio" link
  - `"settings"`: Shows the settings link

**Before (Gradio 5.x):**

```python
import gradio as gr

with gr.Blocks() as demo:
    gr.Textbox(label="Input")

demo.launch(show_api=False)
```

**After (Gradio 6.x):**

```python
import gradio as gr

with gr.Blocks() as demo:
    gr.Textbox(label="Input")

demo.launch(footer_links=["gradio", "settings"])
```

To replicate the old behavior:
- `show_api=True` â†’ `footer_links=["api", "gradio", "settings"]` (or just omit the parameter, as this is the default)
- `show_api=False` â†’ `footer_links=["gradio", "settings"]`

### Event listener parameters: `show_api` removed and `api_name=False` no longer supported

In event listeners (such as `.click()`, `.change()`, etc.), the `show_api` parameter has been removed, and `api_name` no longer accepts `False` as a valid value. These have been replaced with a new `api_visibility` parameter that provides more fine-grained control.

**In Gradio 5.x:**
- `show_api=True` (default) showed the endpoint in the API documentation
- `show_api=False` hid the endpoint from API docs but still allowed downstream apps to use it
- `api_name=False` completely disabled the API endpoint (no downstream apps could use it)

**In Gradio 6.x:**
- `api_visibility` accepts one of three string values:
  - `"public"`: The endpoint is shown in API docs and accessible to all (equivalent to old `show_api=True`)
  - `"undocumented"`: The endpoint is hidden from API docs but still accessible to downstream apps (equivalent to old `show_api=False`)
  - `"private"`: The endpoint is completely disabled and inaccessible (equivalent to old `api_name=False`)

**Before (Gradio 5.x):**

```python
import gradio as gr

with gr.Blocks() as demo:
    btn = gr.Button("Click me")
    output = gr.Textbox()
    
    btn.click(fn=lambda: "Hello", outputs=output, show_api=False)
    
demo.launch()
```

Or to completely disable the API:

```python
btn.click(fn=lambda: "Hello", outputs=output, api_name=False)
```

**After (Gradio 6.x):**

```python
import gradio as gr

with gr.Blocks() as demo:
    btn = gr.Button("Click me")
    output = gr.Textbox()
    
    btn.click(fn=lambda: "Hello", outputs=output, api_visibility="undocumented")
    
demo.launch()
```

Or to completely disable the API:

```python
btn.click(fn=lambda: "Hello", outputs=output, api_visibility="private")
```

To replicate the old behavior:
- `show_api=True` â†’ `api_visibility="public"` (or just omit the parameter, as this is the default)
- `show_api=False` â†’ `api_visibility="undocumented"`
- `api_name=False` â†’ `api_visibility="private"`

### `like_user_message` moved from `.like()` event to constructor 

The `like_user_message` parameter has been moved from the `.like()` event listener to the Chatbot constructor.

**Before (Gradio 5.x):**
```python
chatbot = gr.Chatbot()
chatbot.like(print_like_dislike, None, None, like_user_message=True)
```

**After (Gradio 6.x):**
```python
chatbot = gr.Chatbot(like_user_message=True)
chatbot.like(print_like_dislike, None, None)
```


### Default API names for `Interface` and `ChatInterface` now use function names

The default API endpoint names for `gr.Interface` and `gr.ChatInterface` have changed to be consistent with how `gr.Blocks` events work and to better support MCP (Model Context Protocol) tools.

**In Gradio 5.x:**
- `gr.Interface` had a default API name of `/predict`
- `gr.ChatInterface` had a default API name of `/chat`

**In Gradio 6.x:**
- Both `gr.Interface` and `gr.ChatInterface` now use the name of the function you pass in as the default API endpoint name
- This makes the API more descriptive and consistent with `gr.Blocks` behavior

E.g. if your Gradio app is:

```python
import gradio as gr

def generate_text(prompt):
    return f"Generated: {prompt}"

demo = gr.Interface(fn=generate_text, inputs="text", outputs="text")
demo.launch()
```

Previously, the API endpoint that Gradio generated would be: `/predict`. Now, the API endpoint will be: `/generate_text`

**To maintain the old endpoint names:**

If you need to keep the old endpoint names for backward compatibility (e.g., if you have external services calling these endpoints), you can explicitly set the `api_name` parameter:

```python
demo = gr.Interface(fn=generate_text, inputs="text", outputs="text", api_name="predict")
```

Similarly for `ChatInterface`:

```python
demo = gr.ChatInterface(fn=chat_function, api_name="chat")
```

### `gr.Chatbot` and `gr.ChatInterface` tuple format removed

The tuple format for chatbot messages has been removed in Gradio 6.0. You must now use the messages format with dictionaries containing "role" and "content" keys.

**In Gradio 5.x:**
- You could use `type="tuples"` or the default tuple format: `[["user message", "assistant message"], ...]`
- The tuple format was a list of lists where each inner list had two elements: `[user_message, assistant_message]`

**In Gradio 6.x:**
- Only the messages format is supported: `type="messages"`
- Messages must be dictionaries with "role" and "content" keys: `[{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hi there!"}]`

**Before (Gradio 5.x):**

```python
import gradio as gr

# Using tuple format
chatbot = gr.Chatbot(value=[["Hello", "Hi there!"]])
```

Or with `type="tuples"`:

```python
chatbot = gr.Chatbot(value=[["Hello", "Hi there!"]], type="tuples")
```

**After (Gradio 6.x):**

```python
import gradio as gr

# Must use messages format
chatbot = gr.Chatbot(
    value=[
        {"role": "user", "content": "Hello"},
        {"role": "assistant", "content": "Hi there!"}
    ],
    type="messages"
)
```

Similarly for `gr.ChatInterface`, if you were manually setting the chat history:

```python
# Before (Gradio 5.x)
demo = gr.ChatInterface(
    fn=chat_function,
    examples=[["Hello", "Hi there!"]]
)

# After (Gradio 6.x)
demo = gr.ChatInterface(
    fn=chat_function,
    examples=[{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hi there!"}]
)
```

**Note:** If you're using `gr.ChatInterface` with a function that returns messages, the function should return messages in the new format. The tuple format is no longer supported.

### `gr.ChatInterface` `history` format now uses structured content

The `history` format in `gr.ChatInterface` has been updated to consistently use OpenAI-style structured content format. Content is now always a list of content blocks, even for simple text messages.

**In Gradio 5.x:**
- Content could be a simple string: `{"role": "user", "content": "Hello"}`
- Simple text messages used a string directly

**In Gradio 6.x:**
- Content is always a list of content blocks: `{"role": "user", "content": [{"type": "text", "text": "Hello"}]}`
- This format is consistent with OpenAI's message format and supports multimodal content (text, images, etc.)

**Before (Gradio 5.x):**

```python
history = [
    {"role": "user", "content": "What is the capital of France?"},
    {"role": "assistant", "content": "Paris"}
]
```

**After (Gradio 6.x):**

```python
history = [
    {"role": "user", "content": [{"type": "text", "text": "What is the capital of France?"}]},
    {"role": "assistant", "content": [{"type": "text", "text": "Paris"}]}
]
```

**With files:**

When files are uploaded in the chat, they are represented as content blocks with `"type": "file"`. All content blocks (files and text) are grouped together in the same message's content array:

```python
history = [
    {
        "role": "user",
        "content": [
            {"type": "file", "file": {"path": "cat1.png"}},
            {"type": "file", "file": {"path": "cat2.png"}},
            {"type": "text", "text": "What's the difference between these two images?"}
        ]
    }
]
```

This structured format allows for multimodal content (text, images, files, etc.) in chat messages, making it consistent with OpenAI's API format. All files uploaded in a single message are grouped together in the `content` array along with any text content.

### `cache_examples` parameter updated and `cache_mode` introduced

The `cache_examples` parameter (used in `Interface`, `ChatInterface`, and `Examples`) no longer accepts the string value `"lazy"`. It now strictly accepts boolean values (`True` or `False`). To control the caching strategy, a new `cache_mode` parameter has been introduced.

**In Gradio 5.x:**
- `cache_examples` accepted `True`, `False`, or `"lazy"`.

**In Gradio 6.x:**
- `cache_examples` only accepts `True` or `False`.
- `cache_mode` accepts `"eager"` (default) or `"lazy"`.

**Before (Gradio 5.x):**

```python
import gradio as gr

demo = gr.Interface(
    fn=predict, 
    inputs="text", 
    outputs="text", 
    examples=["Hello", "World"],
    cache_examples="lazy"
)
```

**After (Gradio 6.x):**

You must now set `cache_examples=True` and specify the mode separately:

```python
import gradio as gr

demo = gr.Interface(
    fn=predict, 
    inputs="text", 
    outputs="text", 
    examples=["Hello", "World"],
    cache_examples=True,
    cache_mode="lazy"
)
```

If you previously used `cache_examples=True` (which implied eager caching), no changes are required, as `cache_mode` defaults to `"eager"`.

## Component-level Changes

### `gr.Video` no longer accepts tuple values for video and subtitles

The tuple format for returning video with subtitles has been deprecated. Instead of returning a tuple `(video_path, subtitle_path)`, you should now use the `gr.Video` component directly with the `subtitles` parameter.

**In Gradio 5.x:**
- You could return a tuple of `(video_path, subtitle_path)` from a function
- The tuple format was `(str | Path, str | Path | None)`

**In Gradio 6.x:**
- Return a `gr.Video` component instance with the `subtitles` parameter
- This provides more flexibility and consistency with other components

**Before (Gradio 5.x):**

```python
import gradio as gr

def generate_video_with_subtitles(input):
    video_path = "output.mp4"
    subtitle_path = "subtitles.srt"
    return (video_path, subtitle_path)

demo = gr.Interface(
    fn=generate_video_with_subtitles,
    inputs="text",
    outputs=gr.Video()
)
demo.launch()
```

**After (Gradio 6.x):**

```python
import gradio as gr

def generate_video_with_subtitles(input):
    video_path = "output.mp4"
    subtitle_path = "subtitles.srt"
    return gr.Video(value=video_path, subtitles=subtitle_path)

demo = gr.Interface(
    fn=generate_video_with_subtitles,
    inputs="text",
    outputs=gr.Video()
)
demo.launch()
```

### `gr.HTML` `padding` parameter default changed to `False`

The default value of the `padding` parameter in `gr.HTML` has been changed from `True` to `False` for consistency with `gr.Markdown`.

**In Gradio 5.x:**
- `padding=True` was the default for `gr.HTML`
- HTML components had padding by default

**In Gradio 6.x:**
- `padding=False` is the default for `gr.HTML`
- This matches the default behavior of `gr.Markdown` for consistency

**To maintain the old behavior:**

If you want to keep the padding that was present in Gradio 5.x, explicitly set `padding=True`:

```python
html = gr.HTML("<div>Content</div>", padding=True)
```


### `gr.Dataframe` `row_count` and `col_count` parameters restructured

The `row_count` and `col_count` parameters in `gr.Dataframe` have been restructured to provide more flexibility and clarity. The tuple format for specifying fixed/dynamic behavior has been replaced with separate parameters for initial counts and limits.

**In Gradio 5.x:**
- `row_count: int | tuple[int, str]` - Could be an int or tuple like `(5, "fixed")` or `(5, "dynamic")`
- `col_count: int | tuple[int, str] | None` - Could be an int or tuple like `(3, "fixed")` or `(3, "dynamic")`

**In Gradio 6.x:**
- `row_count: int | None` - Just the initial number of rows to display
- `row_limits: tuple[int | None, int | None] | None` - Tuple specifying (min_rows, max_rows) constraints
- `column_count: int | None` - The initial number of columns to display
- `column_limits: tuple[int | None, int | None] | None` - Tuple specifying (min_columns, max_columns) constraints

**Before (Gradio 5.x):**

```python
import gradio as gr

# Fixed number of rows (users can't add/remove rows)
df = gr.Dataframe(row_count=(5, "fixed"), col_count=(3, "dynamic"))
```

Or with dynamic rows:

```python
# Dynamic rows (users can add/remove rows)
df = gr.Dataframe(row_count=(5, "dynamic"), col_count=(3, "fixed"))
```

Or with just integers (defaults to dynamic):

```python
df = gr.Dataframe(row_count=5, col_count=3)
```

**After (Gradio 6.x):**

```python
import gradio as gr

# Fixed number of rows (users can't add/remove rows)
df = gr.Dataframe(row_count=5, row_limits=(5, 5), column_count=3, column_limits=None)
```

Or with dynamic rows (users can add/remove rows):

```python
# Dynamic rows with no limits
df = gr.Dataframe(row_count=5, row_limits=None, column_count=3, column_limits=None)
```

Or with min/max constraints:

```python
# Rows between 3 and 10, columns between 2 and 5
df = gr.Dataframe(row_count=5, row_limits=(3, 10), column_count=3, column_limits=(2, 5))
```

**Migration examples:**

- `row_count=(5, "fixed")` â†’ `row_count=5, row_limits=(5, 5)`
- `row_count=(5, "dynamic")` â†’ `row_count=5, row_limits=None`
- `row_count=5` â†’ `row_count=5, row_limits=None` (same behavior)
- `col_count=(3, "fixed")` â†’ `column_count=3, column_limits=(3, 3)`
- `col_count=(3, "dynamic")` â†’ `column_count=3, column_limits=None`
- `col_count=3` â†’ `column_count=3, column_limits=None` (same behavior)

### `allow_tags=True` is now the default for `gr.Chatbot`

Due to the rise in LLMs returning HTML, markdown tags, and custom tags (such as `<thinking>` tags), the default value of `allow_tags` in `gr.Chatbot` has changed from `False` to `True` in Gradio 6.

**In Gradio 5.x:**
- `allow_tags=False` was the default
- All HTML and custom tags were sanitized/removed from chatbot messages (unless explicitly allowed)

**In Gradio 6.x:**
- `allow_tags=True` is the default
- All custom tags (non-standard HTML tags) are preserved in chatbot messages
- Standard HTML tags are still sanitized for security unless `sanitize_html=False`

**Before (Gradio 5.x):**

```python
import gradio as gr

chatbot = gr.Chatbot()
```

This would remove all tags from messages, including custom tags like `<thinking>`.

**After (Gradio 6.x):**

```python
import gradio as gr

chatbot = gr.Chatbot()
```

This will now preserve custom tags like `<thinking>` in the messages.

**To maintain the old behavior:**

If you want to continue removing all tags from chatbot messages (the old default behavior), explicitly set `allow_tags=False`:

```python
import gradio as gr

chatbot = gr.Chatbot(allow_tags=False)
```

**Note:** You can also specify a list of specific tags to allow:

```python
chatbot = gr.Chatbot(allow_tags=["thinking", "tool_call"])
```

This will only preserve `<thinking>` and `<tool_call>` tags while removing all other custom tags.



### Other removed component parameters

Several component parameters have been removed in Gradio 6.0. These parameters were previously deprecated and have now been fully removed.

#### `gr.Chatbot` removed parameters

**`bubble_full_width`** - This parameter has been removed as it no longer has any effect.


**`resizeable`** - This parameter (with the typo) has been removed. Use `resizable` instead.

**Before (Gradio 5.x):**
```python
chatbot = gr.Chatbot(resizeable=True)
```

**After (Gradio 6.x):**
```python
chatbot = gr.Chatbot(resizable=True)
```

**`show_copy_button`, `show_copy_all_button`, `show_share_button`** - These parameters have been removed. Use the `buttons` parameter instead.

**Before (Gradio 5.x):**
```python
chatbot = gr.Chatbot(show_copy_button=True, show_copy_all_button=True, show_share_button=True)
```

**After (Gradio 6.x):**
```python
chatbot = gr.Chatbot(buttons=["copy", "copy_all", "share"])
```

#### `gr.Audio` / `WaveformOptions` removed parameters

**`show_controls`** - This parameter in `WaveformOptions` has been removed. Use `show_recording_waveform` instead.

**Before (Gradio 5.x):**
```python
audio = gr.Audio(
    waveform_options=gr.WaveformOptions(show_controls=False)
)
```

**After (Gradio 6.x):**
```python
audio = gr.Audio(
    waveform_options=gr.WaveformOptions(show_recording_waveform=False)
)
```

**`min_length` and `max_length`** - These parameters have been removed. Use validators instead.

**Before (Gradio 5.x):**
```python
audio = gr.Audio(min_length=1, max_length=10)
```

**After (Gradio 6.x):**
```python
audio = gr.Audio(
    validator=lambda audio: gr.validators.is_audio_correct_length(audio, min_length=1, max_length=10)
)
```

**`show_download_button`, `show_share_button`** - These parameters have been removed. Use the `buttons` parameter instead.

**Before (Gradio 5.x):**
```python
audio = gr.Audio(show_download_button=True, show_share_button=True)
```

**After (Gradio 6.x):**
```python
audio = gr.Audio(buttons=["download", "share"])
```

**Note:** For components where `show_share_button` had a default of `None` (which would show the button on Spaces), you can use `buttons=["share"]` to always show it, or omit it from the list to hide it.

#### `gr.Image` removed parameters

**`mirror_webcam`** - This parameter has been removed. Use `webcam_options` with `gr.WebcamOptions` instead.

**Before (Gradio 5.x):**
```python
image = gr.Image(mirror_webcam=True)
```

**After (Gradio 6.x):**
```python
image = gr.Image(webcam_options=gr.WebcamOptions(mirror=True))
```

**`webcam_constraints`** - This parameter has been removed. Use `webcam_options` with `gr.WebcamOptions` instead.

**Before (Gradio 5.x):**
```python
image = gr.Image(webcam_constraints={"facingMode": "user"})
```

**After (Gradio 6.x):**
```python
image = gr.Image(webcam_options=gr.WebcamOptions(constraints={"facingMode": "user"}))
```

**`show_download_button`, `show_share_button`, `show_fullscreen_button`** - These parameters have been removed. Use the `buttons` parameter instead.

**Before (Gradio 5.x):**
```python
image = gr.Image(show_download_button=True, show_share_button=True, show_fullscreen_button=True)
```

**After (Gradio 6.x):**
```python
image = gr.Image(buttons=["download", "share", "fullscreen"])
```

#### `gr.Video` removed parameters

**`mirror_webcam`** - This parameter has been removed. Use `webcam_options` with `gr.WebcamOptions` instead.

**Before (Gradio 5.x):**
```python
video = gr.Video(mirror_webcam=True)
```

**After (Gradio 6.x):**
```python
video = gr.Video(webcam_options=gr.WebcamOptions(mirror=True))
```

**`webcam_constraints`** - This parameter has been removed. Use `webcam_options` with `gr.WebcamOptions` instead.

**Before (Gradio 5.x):**
```python
video = gr.Video(webcam_constraints={"facingMode": "user"})
```

**After (Gradio 6.x):**
```python
video = gr.Video(webcam_options=gr.WebcamOptions(constraints={"facingMode": "user"}))
```

**`min_length` and `max_length`** - These parameters have been removed. Use validators instead.

**Before (Gradio 5.x):**
```python
video = gr.Video(min_length=1, max_length=10)
```

**After (Gradio 6.x):**
```python
video = gr.Video(
    validator=lambda video: gr.validators.is_video_correct_length(video, min_length=1, max_length=10)
)
```

**`show_download_button`, `show_share_button`** - These parameters have been removed. Use the `buttons` parameter instead.

**Before (Gradio 5.x):**
```python
video = gr.Video(show_download_button=True, show_share_button=True)
```

**After (Gradio 6.x):**
```python
video = gr.Video(buttons=["download", "share"])
```

#### `gr.ImageEditor` removed parameters

**`crop_size`** - This parameter has been removed. Use `canvas_size` instead.

**Before (Gradio 5.x):**
```python
editor = gr.ImageEditor(crop_size=(512, 512))
```

**After (Gradio 6.x):**
```python
editor = gr.ImageEditor(canvas_size=(512, 512))
```

#### Removed components

**`gr.LogoutButton`** - This component has been removed. Use `gr.LoginButton` instead, which handles both login and logout processes.

**Before (Gradio 5.x):**
```python
logout_btn = gr.LogoutButton()
```

**After (Gradio 6.x):**
```python
login_btn = gr.LoginButton()
```

#### Native plot components removed parameters

The following parameters have been removed from `gr.LinePlot`, `gr.BarPlot`, and `gr.ScatterPlot`:

- `overlay_point` - This parameter has been removed.
- `width` - This parameter has been removed. Use CSS styling or container width instead.
- `stroke_dash` - This parameter has been removed.
- `interactive` - This parameter has been removed.
- `show_actions_button` - This parameter has been removed.
- `color_legend_title` - This parameter has been removed. Use `color_title` instead.
- `show_fullscreen_button`, `show_export_button` - These parameters have been removed. Use the `buttons` parameter instead.

**Before (Gradio 5.x):**
```python
plot = gr.LinePlot(
    value=data,
    x="date",
    y="downloads",
    overlay_point=True,
    width=900,
    show_fullscreen_button=True,
    show_export_button=True
)
```

**After (Gradio 6.x):**
```python
plot = gr.LinePlot(
    value=data,
    x="date",
    y="downloads",
    buttons=["fullscreen", "export"]
)
```

**Note:** For `color_legend_title`, use `color_title` instead:

**Before (Gradio 5.x):**
```python
plot = gr.ScatterPlot(color_legend_title="Category")
```

**After (Gradio 6.x):**
```python
plot = gr.ScatterPlot(color_title="Category")
```

#### `gr.Textbox` removed parameters

**`show_copy_button`** - This parameter has been removed. Use the `buttons` parameter instead.

**Before (Gradio 5.x):**
```python
text = gr.Textbox(show_copy_button=True)
```

**After (Gradio 6.x):**
```python
text = gr.Textbox(buttons=["copy"])
```

#### `gr.Markdown` removed parameters

**`show_copy_button`** - This parameter has been removed. Use the `buttons` parameter instead.

**Before (Gradio 5.x):**
```python
markdown = gr.Markdown(show_copy_button=True)
```

**After (Gradio 6.x):**
```python
markdown = gr.Markdown(buttons=["copy"])
```

#### `gr.Dataframe` removed parameters

**`show_copy_button`, `show_fullscreen_button`** - These parameters have been removed. Use the `buttons` parameter instead.

**Before (Gradio 5.x):**
```python
df = gr.Dataframe(show_copy_button=True, show_fullscreen_button=True)
```

**After (Gradio 6.x):**
```python
df = gr.Dataframe(buttons=["copy", "fullscreen"])
```

#### `gr.Slider` removed parameters

**`show_reset_button`** - This parameter has been removed. Use the `buttons` parameter instead.

**Before (Gradio 5.x):**
```python
slider = gr.Slider(show_reset_button=True)
```

**After (Gradio 6.x):**
```python
slider = gr.Slider(buttons=["reset"])
```


## CLI Changes

### `gradio sketch` command removed

The `gradio sketch` command-line tool has been deprecated and completely removed in Gradio 6. This tool was used to create Gradio apps through a visual interface.

**In Gradio 5.x:**
- You could run `gradio sketch` to launch an interactive GUI for building Gradio apps
- The tool would generate Python code visually

**In Gradio 6.x:**
- The `gradio sketch` command has been removed
- Running `gradio sketch` will raise a `DeprecationWarning`

## Python Client Changes

### `hf_token` parameter renamed to `token` in `Client`

The `hf_token` parameter in the `Client` class has been renamed to `token` for consistency and simplicity.

**Before (Gradio 5.x):**

```python
from gradio_client import Client

client = Client("abidlabs/my-private-space", hf_token="hf_...")
```

**After (Gradio 6.x):**

```python
from gradio_client import Client

client = Client("abidlabs/my-private-space", token="hf_...")
```

### `deploy_discord` method deprecated

The `deploy_discord` method in the `Client` class has been deprecated and will be removed in Gradio 6.0. This method was used to deploy Gradio apps as Discord bots.

**Before (Gradio 5.x):**

```python
from gradio_client import Client

client = Client("username/space-name")
client.deploy_discord(discord_bot_token="...")
```

**After (Gradio 6.x):**

The `deploy_discord` method is no longer available. Please see the [documentation on creating a Discord bot with Gradio](https://www.gradio.app/guides/creating-a-discord-bot-from-a-gradio-app) for alternative approaches.

### `AppError` now subclasses `Exception` instead of `ValueError`

The `AppError` exception class in the Python client now subclasses `Exception` directly instead of `ValueError`. This is a breaking change if you have code that specifically catches `ValueError` to handle `AppError` instances.

**Before (Gradio 5.x):**

```python
from gradio_client import Client
from gradio_client.exceptions import AppError

try:
    client = Client("username/space-name")
    result = client.predict("/predict", inputs)
except ValueError as e:
    # This would catch AppError in Gradio 5.x
    print(f"Error: {e}")
```

**After (Gradio 6.x):**

```python
from gradio_client import Client
from gradio_client.exceptions import AppError

try:
    client = Client("username/space-name")
    result = client.predict("/predict", inputs)
except AppError as e:
    # Explicitly catch AppError
    print(f"App error: {e}")
except ValueError as e:
    # This will no longer catch AppError
    print(f"Value error: {e}")
```

---

<!-- Source: guides/11_other-tutorials/Gradio-and-Comet.md -->
# Using Gradio and Comet

Tags: COMET, SPACES
Contributed by the Comet team

## Introduction

In this guide we will demonstrate some of the ways you can use Gradio with Comet. We will cover the basics of using Comet with Gradio and show you some of the ways that you can leverage Gradio's advanced features such as [Embedding with iFrames](https://www.gradio.app/guides/sharing-your-app/#embedding-with-iframes) and [State](https://www.gradio.app/docs/#state) to build some amazing model evaluation workflows.

Here is a list of the topics covered in this guide.

1. Logging Gradio UI's to your Comet Experiments
2. Embedding Gradio Applications directly into your Comet Projects
3. Embedding Hugging Face Spaces directly into your Comet Projects
4. Logging Model Inferences from your Gradio Application to Comet

## What is Comet?

[Comet](https://www.comet.com?utm_source=gradio&utm_medium=referral&utm_campaign=gradio-integration&utm_content=gradio-docs) is an MLOps Platform that is designed to help Data Scientists and Teams build better models faster! Comet provides tooling to Track, Explain, Manage, and Monitor your models in a single place! It works with Jupyter Notebooks and Scripts and most importantly it's 100% free!

## Setup

First, install the dependencies needed to run these examples

```shell
pip install comet_ml torch torchvision transformers gradio shap requests Pillow
```

Next, you will need to [sign up for a Comet Account](https://www.comet.com/signup?utm_source=gradio&utm_medium=referral&utm_campaign=gradio-integration&utm_content=gradio-docs). Once you have your account set up, [grab your API Key](https://www.comet.com/docs/v2/guides/getting-started/quickstart/#get-an-api-key?utm_source=gradio&utm_medium=referral&utm_campaign=gradio-integration&utm_content=gradio-docs) and configure your Comet credentials

If you're running these examples as a script, you can either export your credentials as environment variables

```shell
export COMET_API_KEY="<Your API Key>"
export COMET_WORKSPACE="<Your Workspace Name>"
export COMET_PROJECT_NAME="<Your Project Name>"
```

or set them in a `.comet.config` file in your working directory. You file should be formatted in the following way.

```shell
[comet]
api_key=<Your API Key>
workspace=<Your Workspace Name>
project_name=<Your Project Name>
```

If you are using the provided Colab Notebooks to run these examples, please run the cell with the following snippet before starting the Gradio UI. Running this cell allows you to interactively add your API key to the notebook.

```python
import comet_ml
comet_ml.init()
```

## 1. Logging Gradio UI's to your Comet Experiments

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/comet-examples/blob/master/integrations/model-evaluation/gradio/notebooks/Gradio_and_Comet.ipynb)

In this example, we will go over how to log your Gradio Applications to Comet and interact with them using the Gradio Custom Panel.

Let's start by building a simple Image Classification example using `resnet18`.

```python
import comet_ml

import requests
import torch
from PIL import Image
from torchvision import transforms

torch.hub.download_url_to_file("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")

if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

model = torch.hub.load("pytorch/vision:v0.6.0", "resnet18", pretrained=True).eval()
model = model.to(device)

# Download human-readable labels for ImageNet.
response = requests.get("https://git.io/JJkYN")
labels = response.text.split("\n")


def predict(inp):
    inp = Image.fromarray(inp.astype("uint8"), "RGB")
    inp = transforms.ToTensor()(inp).unsqueeze(0)
    with torch.no_grad():
        prediction = torch.nn.functional.softmax(model(inp.to(device))[0], dim=0)
    return {labels[i]: float(prediction[i]) for i in range(1000)}


inputs = gr.Image()
outputs = gr.Label(num_top_classes=3)

io = gr.Interface(
    fn=predict, inputs=inputs, outputs=outputs, examples=["dog.jpg"]
)
io.launch(inline=False, share=True)

experiment = comet_ml.Experiment()
experiment.add_tag("image-classifier")

io.integrate(comet_ml=experiment)
```

The last line in this snippet will log the URL of the Gradio Application to your Comet Experiment. You can find the URL in the Text Tab of your Experiment.

<video width="560" height="315" controls>
    <source src="https://user-images.githubusercontent.com/7529846/214328034-09369d4d-8b94-4c4a-aa3c-25e3ed8394c4.mp4"></source>
</video>

Add the Gradio Panel to your Experiment to interact with your application.

<video width="560" height="315" controls>
    <source src="https://user-images.githubusercontent.com/7529846/214328194-95987f83-c180-4929-9bed-c8a0d3563ed7.mp4"></source>
</video>

## 2. Embedding Gradio Applications directly into your Comet Projects

<iframe width="560" height="315" src="https://www.youtube.com/embed/KZnpH7msPq0?start=9" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

If you are permanently hosting your Gradio application, you can embed the UI using the Gradio Panel Extended custom Panel.

Go to your Comet Project page, and head over to the Panels tab. Click the `+ Add` button to bring up the Panels search page.

<img width="560" alt="adding-panels" src="https://user-images.githubusercontent.com/7529846/214329314-70a3ff3d-27fb-408c-a4d1-4b58892a3854.jpeg">

Next, search for Gradio Panel Extended in the Public Panels section and click `Add`.

<img width="560" alt="gradio-panel-extended" src="https://user-images.githubusercontent.com/7529846/214325577-43226119-0292-46be-a62a-0c7a80646ebb.png">

Once you have added your Panel, click `Edit` to access to the Panel Options page and paste in the URL of your Gradio application.

![Edit-Gradio-Panel-Options](https://user-images.githubusercontent.com/7529846/214573001-23814b5a-ca65-4ace-a8a5-b27cdda70f7a.gif)

<img width="560" alt="Edit-Gradio-Panel-URL" src="https://user-images.githubusercontent.com/7529846/214334843-870fe726-0aa1-4b21-bbc6-0c48f56c48d8.png">

## 3. Embedding Hugging Face Spaces directly into your Comet Projects

<iframe width="560" height="315" src="https://www.youtube.com/embed/KZnpH7msPq0?start=107" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

You can also embed Gradio Applications that are hosted on Hugging Faces Spaces into your Comet Projects using the Hugging Face Spaces Panel.

Go to your Comet Project page, and head over to the Panels tab. Click the `+ Add` button to bring up the Panels search page. Next, search for the Hugging Face Spaces Panel in the Public Panels section and click `Add`.

<img width="560" height="315" alt="huggingface-spaces-panel" src="https://user-images.githubusercontent.com/7529846/214325606-99aa3af3-b284-4026-b423-d3d238797e12.png">

Once you have added your Panel, click Edit to access to the Panel Options page and paste in the path of your Hugging Face Space e.g. `pytorch/ResNet`

<img width="560" height="315" alt="Edit-HF-Space" src="https://user-images.githubusercontent.com/7529846/214335868-c6f25dee-13db-4388-bcf5-65194f850b02.png">

## 4. Logging Model Inferences to Comet

<iframe width="560" height="315" src="https://www.youtube.com/embed/KZnpH7msPq0?start=176" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/comet-examples/blob/master/integrations/model-evaluation/gradio/notebooks/Logging_Model_Inferences_with_Comet_and_Gradio.ipynb)

In the previous examples, we demonstrated the various ways in which you can interact with a Gradio application through the Comet UI. Additionally, you can also log model inferences, such as SHAP plots, from your Gradio application to Comet.

In the following snippet, we're going to log inferences from a Text Generation model. We can persist an Experiment across multiple inference calls using Gradio's [State](https://www.gradio.app/docs/#state) object. This will allow you to log multiple inferences from a model to a single Experiment.

```python
import comet_ml
import gradio as gr
import shap
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

MODEL_NAME = "gpt2"

model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

# set model decoder to true
model.config.is_decoder = True
# set text-generation params under task_specific_params
model.config.task_specific_params["text-generation"] = {
    "do_sample": True,
    "max_length": 50,
    "temperature": 0.7,
    "top_k": 50,
    "no_repeat_ngram_size": 2,
}
model = model.to(device)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
explainer = shap.Explainer(model, tokenizer)


def start_experiment():
    """Returns an APIExperiment object that is thread safe
    and can be used to log inferences to a single Experiment
    """
    try:
        api = comet_ml.API()
        workspace = api.get_default_workspace()
        project_name = comet_ml.config.get_config()["comet.project_name"]

        experiment = comet_ml.APIExperiment(
            workspace=workspace, project_name=project_name
        )
        experiment.log_other("Created from", "gradio-inference")

        message = f"Started Experiment: [{experiment.name}]({experiment.url})"

        return (experiment, message)

    except Exception as e:
        return None, None


def predict(text, state, message):
    experiment = state

    shap_values = explainer([text])
    plot = shap.plots.text(shap_values, display=False)

    if experiment is not None:
        experiment.log_other("message", message)
        experiment.log_html(plot)

    return plot


with gr.Blocks() as demo:
    start_experiment_btn = gr.Button("Start New Experiment")
    experiment_status = gr.Markdown()

    # Log a message to the Experiment to provide more context
    experiment_message = gr.Textbox(label="Experiment Message")
    experiment = gr.State()

    input_text = gr.Textbox(label="Input Text", lines=5, interactive=True)
    submit_btn = gr.Button("Submit")

    output = gr.HTML(interactive=True)

    start_experiment_btn.click(
        start_experiment, outputs=[experiment, experiment_status]
    )
    submit_btn.click(
        predict, inputs=[input_text, experiment, experiment_message], outputs=[output]
    )
```

Inferences from this snippet will be saved in the HTML tab of your experiment.

<video width="560" height="315" controls>
    <source src="https://user-images.githubusercontent.com/7529846/214328610-466e5c81-4814-49b9-887c-065aca14dd30.mp4"></source>
</video>

## Conclusion

We hope you found this guide useful and that it provides some inspiration to help you build awesome model evaluation workflows with Comet and Gradio.

## How to contribute Gradio demos on HF spaces on the Comet organization

- Create an account on Hugging Face [here](https://huggingface.co/join).
- Add Gradio Demo under your username, see this [course](https://huggingface.co/course/chapter9/4?fw=pt) for setting up Gradio Demo on Hugging Face.
- Request to join the Comet organization [here](https://huggingface.co/Comet).

## Additional Resources

- [Comet Documentation](https://www.comet.com/docs/v2/?utm_source=gradio&utm_medium=referral&utm_campaign=gradio-integration&utm_content=gradio-docs)

---

<!-- Source: guides/11_other-tutorials/Gradio-and-ONNX-on-Hugging-Face.md -->
# Gradio and ONNX on Hugging Face

Related spaces: https://huggingface.co/spaces/onnx/EfficientNet-Lite4
Tags: ONNX, SPACES
Contributed by Gradio and the <a href="https://onnx.ai/">ONNX</a> team

## Introduction

In this Guide, we'll walk you through:

- Introduction of ONNX, ONNX model zoo, Gradio, and Hugging Face Spaces
- How to setup a Gradio demo for EfficientNet-Lite4
- How to contribute your own Gradio demos for the ONNX organization on Hugging Face

Here's an [example](https://onnx-efficientnet-lite4.hf.space/) of an ONNX model.

## What is the ONNX Model Zoo?

Open Neural Network Exchange ([ONNX](https://onnx.ai/)) is an open standard format for representing machine learning models. ONNX is supported by a community of partners who have implemented it in many frameworks and tools. For example, if you have trained a model in TensorFlow or PyTorch, you can convert it to ONNX easily, and from there run it on a variety of devices using an engine/compiler like ONNX Runtime.

The [ONNX Model Zoo](https://github.com/onnx/models) is a collection of pre-trained, state-of-the-art models in the ONNX format contributed by community members. Accompanying each model are Jupyter notebooks for model training and running inference with the trained model. The notebooks are written in Python and include links to the training dataset as well as references to the original paper that describes the model architecture.

## What are Hugging Face Spaces & Gradio?

### Gradio

Gradio lets users demo their machine learning models as a web app all in python code. Gradio wraps a python function into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.

Get started [here](https://gradio.app/getting_started)

### Hugging Face Spaces

Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).

### Hugging Face Models

Hugging Face Model Hub also supports ONNX models and ONNX models can be filtered through the [ONNX tag](https://huggingface.co/models?library=onnx&sort=downloads)

## How did Hugging Face help the ONNX Model Zoo?

There are a lot of Jupyter notebooks in the ONNX Model Zoo for users to test models. Previously, users needed to download the models themselves and run those notebooks locally for testing. With Hugging Face, the testing process can be much simpler and more user-friendly. Users can easily try certain ONNX Model Zoo model on Hugging Face Spaces and run a quick demo powered by Gradio with ONNX Runtime, all on cloud without downloading anything locally. Note, there are various runtimes for ONNX, e.g., [ONNX Runtime](https://github.com/microsoft/onnxruntime), [MXNet](https://github.com/apache/incubator-mxnet).

## What is the role of ONNX Runtime?

ONNX Runtime is a cross-platform inference and training machine-learning accelerator. It makes live Gradio demos with ONNX Model Zoo model on Hugging Face possible.

ONNX Runtime inference can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. For more information please see the [official website](https://onnxruntime.ai/).

## Setting up a Gradio Demo for EfficientNet-Lite4

EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite models. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU. To learn more read the [model card](https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4)

Here we walk through setting up a example demo for EfficientNet-Lite4 using Gradio

First we import our dependencies and download and load the efficientnet-lite4 model from the onnx model zoo. Then load the labels from the labels_map.txt file. We then setup our preprocessing functions, load the model for inference, and setup the inference function. Finally, the inference function is wrapped into a gradio interface for a user to interact with. See the full code below.

```python
import numpy as np
import math
import matplotlib.pyplot as plt
import cv2
import json
import gradio as gr
from huggingface_hub import hf_hub_download
from onnx import hub
import onnxruntime as ort

# loads ONNX model from ONNX Model Zoo
model = hub.load("efficientnet-lite4")
# loads the labels text file
labels = json.load(open("labels_map.txt", "r"))

# sets image file dimensions to 224x224 by resizing and cropping image from center
def pre_process_edgetpu(img, dims):
    output_height, output_width, _ = dims
    img = resize_with_aspectratio(img, output_height, output_width, inter_pol=cv2.INTER_LINEAR)
    img = center_crop(img, output_height, output_width)
    img = np.asarray(img, dtype='float32')
    # converts jpg pixel value from [0 - 255] to float array [-1.0 - 1.0]
    img -= [127.0, 127.0, 127.0]
    img /= [128.0, 128.0, 128.0]
    return img

# resizes the image with a proportional scale
def resize_with_aspectratio(img, out_height, out_width, scale=87.5, inter_pol=cv2.INTER_LINEAR):
    height, width, _ = img.shape
    new_height = int(100. * out_height / scale)
    new_width = int(100. * out_width / scale)
    if height > width:
        w = new_width
        h = int(new_height * height / width)
    else:
        h = new_height
        w = int(new_width * width / height)
    img = cv2.resize(img, (w, h), interpolation=inter_pol)
    return img

# crops the image around the center based on given height and width
def center_crop(img, out_height, out_width):
    height, width, _ = img.shape
    left = int((width - out_width) / 2)
    right = int((width + out_width) / 2)
    top = int((height - out_height) / 2)
    bottom = int((height + out_height) / 2)
    img = img[top:bottom, left:right]
    return img


sess = ort.InferenceSession(model)

def inference(img):
  img = cv2.imread(img)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

  img = pre_process_edgetpu(img, (224, 224, 3))

  img_batch = np.expand_dims(img, axis=0)

  results = sess.run(["Softmax:0"], {"images:0": img_batch})[0]
  result = reversed(results[0].argsort()[-5:])
  resultdic = {}
  for r in result:
      resultdic[labels[str(r)]] = float(results[0][r])
  return resultdic

title = "EfficientNet-Lite4"
description = "EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite model. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU."
examples = [['catonnx.jpg']]
gr.Interface(inference, gr.Image(type="filepath"), "label", title=title, description=description, examples=examples).launch()
```

## How to contribute Gradio demos on HF spaces using ONNX models

- Add model to the [onnx model zoo](https://github.com/onnx/models/blob/main/.github/PULL_REQUEST_TEMPLATE.md)
- Create an account on Hugging Face [here](https://huggingface.co/join).
- See list of models left to add to ONNX organization, please refer to the table with the [Models list](https://github.com/onnx/models#models)
- Add Gradio Demo under your username, see this [blog post](https://huggingface.co/blog/gradio-spaces) for setting up Gradio Demo on Hugging Face.
- Request to join ONNX Organization [here](https://huggingface.co/onnx).
- Once approved transfer model from your username to ONNX organization
- Add a badge for model in model table, see examples in [Models list](https://github.com/onnx/models#models)

---

<!-- Source: guides/11_other-tutorials/Gradio-and-Wandb-Integration.md -->
# Gradio and W&B Integration

Related spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN
Tags: WANDB, SPACES
Contributed by Gradio team

## Introduction

In this Guide, we'll walk you through:

- Introduction of Gradio, and Hugging Face Spaces, and Wandb
- How to setup a Gradio demo using the Wandb integration for JoJoGAN
- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face


## What is Wandb?

Weights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:

<img alt="Screen Shot 2022-08-01 at 5 54 59 PM" src="https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png">

## What are Hugging Face Spaces & Gradio?

### Gradio

Gradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model's inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.

Get started [here](https://gradio.app/getting_started)

### Hugging Face Spaces

Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).

## Setting up a Gradio Demo for JoJoGAN

Now, let's walk you through how to do this on your own. We'll make the assumption that you're new to W&B and Gradio for the purposes of this tutorial.

Let's get started!

1. Create a W&B account

   Follow [these quick instructions](https://app.wandb.ai/login) to create your free account if you donâ€™t have one already. It shouldn't take more than a couple minutes. Once you're done (or if you've already got an account), next, we'll run a quick colab.

2. Open Colab Install Gradio and W&B

   We'll be following along with the colab provided in the JoJoGAN repo with some minor modifications to use Wandb and Gradio more effectively.

   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mchong6/JoJoGAN/blob/main/stylize.ipynb)

   Install Gradio and Wandb at the top:

   ```sh
   pip install gradio wandb
   ```

3. Finetune StyleGAN and W&B experiment tracking

   This next step will open a W&B dashboard to track your experiments and a gradio panel showing pretrained models to choose from a drop down menu from a Gradio Demo hosted on Huggingface Spaces. Here's the code you need for that:

   ```python
   alpha =  1.0
   alpha = 1-alpha

   preserve_color = True
   num_iter = 100
   log_interval = 50

   samples = []
   column_names = ["Reference (y)", "Style Code(w)", "Real Face Image(x)"]

   wandb.init(project="JoJoGAN")
   config = wandb.config
   config.num_iter = num_iter
   config.preserve_color = preserve_color
   wandb.log(
   {"Style reference": [wandb.Image(transforms.ToPILImage()(target_im))]},
   step=0)

   # load discriminator for perceptual loss
   discriminator = Discriminator(1024, 2).eval().to(device)
   ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)
   discriminator.load_state_dict(ckpt["d"], strict=False)

   # reset generator
   del generator
   generator = deepcopy(original_generator)

   g_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))

   # Which layers to swap for generating a family of plausible real images -> fake image
   if preserve_color:
       id_swap = [9,11,15,16,17]
   else:
       id_swap = list(range(7, generator.n_latent))

   for idx in tqdm(range(num_iter)):
       mean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)
       in_latent = latents.clone()
       in_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]

       img = generator(in_latent, input_is_latent=True)

       with torch.no_grad():
           real_feat = discriminator(targets)
       fake_feat = discriminator(img)

       loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)

       wandb.log({"loss": loss}, step=idx)
       if idx % log_interval == 0:
           generator.eval()
           my_sample = generator(my_w, input_is_latent=True)
           generator.train()
           my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))
           wandb.log(
           {"Current stylization": [wandb.Image(my_sample)]},
           step=idx)
       table_data = [
               wandb.Image(transforms.ToPILImage()(target_im)),
               wandb.Image(img),
               wandb.Image(my_sample),
           ]
       samples.append(table_data)

       g_optim.zero_grad()
       loss.backward()
       g_optim.step()

   out_table = wandb.Table(data=samples, columns=column_names)
   wandb.log({"Current Samples": out_table})
   ```
4. Save, Download, and Load Model

    Here's how to save and download your model.

   ```python
   from PIL import Image
   import torch
   torch.backends.cudnn.benchmark = True
   from torchvision import transforms, utils
   from util import *
   import math
   import random
   import numpy as np
   from torch import nn, autograd, optim
   from torch.nn import functional as F
   from tqdm import tqdm
   import lpips
   from model import *
   from e4e_projection import projection as e4e_projection
   
   from copy import deepcopy
   import imageio
   
   import os
   import sys
   import torchvision.transforms as transforms
   from argparse import Namespace
   from e4e.models.psp import pSp
   from util import *
   from huggingface_hub import hf_hub_download
   from google.colab import files
   
   torch.save({"g": generator.state_dict()}, "your-model-name.pt")
   
   files.download('your-model-name.pt')
   
   latent_dim = 512
   device="cuda"
   model_path_s = hf_hub_download(repo_id="akhaliq/jojogan-stylegan2-ffhq-config-f", filename="stylegan2-ffhq-config-f.pt")
   original_generator = Generator(1024, latent_dim, 8, 2).to(device)
   ckpt = torch.load(model_path_s, map_location=lambda storage, loc: storage)
   original_generator.load_state_dict(ckpt["g_ema"], strict=False)
   mean_latent = original_generator.mean_latent(10000)
   
   generator = deepcopy(original_generator)
   
   ckpt = torch.load("/content/JoJoGAN/your-model-name.pt", map_location=lambda storage, loc: storage)
   generator.load_state_dict(ckpt["g"], strict=False)
   generator.eval()
   
   plt.rcParams['figure.dpi'] = 150
   
   transform = transforms.Compose(
       [
           transforms.Resize((1024, 1024)),
           transforms.ToTensor(),
           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
       ]
   )
   
   def inference(img):
       img.save('out.jpg')
       aligned_face = align_face('out.jpg')
   
       my_w = e4e_projection(aligned_face, "out.pt", device).unsqueeze(0)
       with torch.no_grad():
           my_sample = generator(my_w, input_is_latent=True)
   
       npimage = my_sample[0].cpu().permute(1, 2, 0).detach().numpy()
       imageio.imwrite('filename.jpeg', npimage)
       return 'filename.jpeg'
   ````

5. Build a Gradio Demo

   ```python
   import gradio as gr
   
   title = "JoJoGAN"
   description = "Gradio Demo for JoJoGAN: One Shot Face Stylization. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below."
   
   demo = gr.Interface(
       inference,
       gr.Image(type="pil"),
       gr.Image(type="file"),
       title=title,
       description=description
   )
   
   demo.launch(share=True)
   ```

6. Integrate Gradio into your W&B Dashboard

   The last stepâ€”integrating your Gradio demo with your W&B dashboardâ€”is just one extra line:

   ```python
   demo.integrate(wandb=wandb)
   ```

   Once you call integrate, a demo will be created and you can integrate it into your dashboard or report.

   Outside of W&B with Web components, using the `gradio-app` tags, anyone can embed Gradio demos on HF spaces directly into their blogs, websites, documentation, etc.:
   
   ```html
   <gradio-app space="akhaliq/JoJoGAN"> </gradio-app>
   ```

7. (Optional) Embed W&B plots in your Gradio App

   It's also possible to embed W&B plots within Gradio apps. To do so, you can create a W&B Report of your plots and
   embed them within your Gradio app within a `gr.HTML` block.

   The Report will need to be public and you will need to wrap the URL within an iFrame like this:

   ```python
   import gradio as gr
   
   def wandb_report(url):
       iframe = f'<iframe src={url} style="border:none;height:1024px;width:100%">'
       return gr.HTML(iframe)
   
   with gr.Blocks() as demo:
       report_url = 'https://wandb.ai/_scott/pytorch-sweeps-demo/reports/loss-22-10-07-16-00-17---VmlldzoyNzU2NzAx'
       report = wandb_report(report_url)
   
   demo.launch(share=True)
   ```

## Conclusion

We hope you enjoyed this brief demo of embedding a Gradio demo to a W&B report! Thanks for making it to the end. To recap:

- Only one single reference image is needed for fine-tuning JoJoGAN which usually takes about 1 minute on a GPU in colab. After training, style can be applied to any input image. Read more in the paper.

- W&B tracks experiments with just a few lines of code added to a colab and you can visualize, sort, and understand your experiments in a single, centralized dashboard.

- Gradio, meanwhile, demos the model in a user friendly interface to share anywhere on the web.

## How to contribute Gradio demos on HF spaces on the Wandb organization

- Create an account on Hugging Face [here](https://huggingface.co/join).
- Add Gradio Demo under your username, see this [course](https://huggingface.co/course/chapter9/4?fw=pt) for setting up Gradio Demo on Hugging Face.
- Request to join wandb organization [here](https://huggingface.co/wandb).
- Once approved transfer model from your username to Wandb organization

---

<!-- Source: guides/11_other-tutorials/how-to-use-3D-model-component.md -->
# How to Use the 3D Model Component

Related spaces: https://huggingface.co/spaces/gradio/Model3D, https://huggingface.co/spaces/gradio/PIFu-Clothed-Human-Digitization, https://huggingface.co/spaces/gradio/dpt-depth-estimation-3d-obj
Tags: VISION, IMAGE

## Introduction

3D models are becoming more popular in machine learning and make for some of the most fun demos to experiment with. Using `gradio`, you can easily build a demo of your 3D image model and share it with anyone. The Gradio 3D Model component accepts 3 file types including: _.obj_, _.glb_, & _.gltf_.

This guide will show you how to build a demo for your 3D image model in a few lines of code; like the one below. Play around with 3D object by clicking around, dragging and zooming:

<gradio-app space="gradio/Model3D"> </gradio-app>

### Prerequisites

Make sure you have the `gradio` Python package already [installed](https://gradio.app/guides/quickstart).

## Taking a Look at the Code

Let's take a look at how to create the minimal interface above. The prediction function in this case will just return the original 3D model mesh, but you can change this function to run inference on your machine learning model. We'll take a look at more complex examples below.

```python
import gradio as gr
import os


def load_mesh(mesh_file_name):
    return mesh_file_name


demo = gr.Interface(
    fn=load_mesh,
    inputs=gr.Model3D(),
    outputs=gr.Model3D(
            clear_color=[0.0, 0.0, 0.0, 0.0],  label="3D Model"),
    examples=[
        [os.path.join(os.path.dirname(__file__), "files/Bunny.obj")],
        [os.path.join(os.path.dirname(__file__), "files/Duck.glb")],
        [os.path.join(os.path.dirname(__file__), "files/Fox.gltf")],
        [os.path.join(os.path.dirname(__file__), "files/face.obj")],
    ],
)

if __name__ == "__main__":
    demo.launch()
```

Let's break down the code above:

`load_mesh`: This is our 'prediction' function and for simplicity, this function will take in the 3D model mesh and return it.

Creating the Interface:

- `fn`: the prediction function that is used when the user clicks submit. In our case this is the `load_mesh` function.
- `inputs`: create a model3D input component. The input expects an uploaded file as a {str} filepath.
- `outputs`: create a model3D output component. The output component also expects a file as a {str} filepath.
  - `clear_color`: this is the background color of the 3D model canvas. Expects RGBa values.
  - `label`: the label that appears on the top left of the component.
- `examples`: list of 3D model files. The 3D model component can accept _.obj_, _.glb_, & _.gltf_ file types.
- `cache_examples`: saves the predicted output for the examples, to save time on inference.

## Exploring a more complex Model3D Demo:

Below is a demo that uses the DPT model to predict the depth of an image and then uses 3D Point Cloud to create a 3D object. Take a look at the [app.py](https://huggingface.co/spaces/gradio/dpt-depth-estimation-3d-obj/blob/main/app.py) file for a peek into the code and the model prediction function.
<gradio-app space="gradio/dpt-depth-estimation-3d-obj"> </gradio-app>

---

And you're done! That's all the code you need to build an interface for your Model3D model. Here are some references that you may find useful:

- Gradio's ["Getting Started" guide](https://gradio.app/getting_started/)
- The first [3D Model Demo](https://huggingface.co/spaces/gradio/Model3D) and [complete code](https://huggingface.co/spaces/gradio/Model3D/tree/main) (on Hugging Face Spaces)

---

<!-- Source: guides/11_other-tutorials/image-classification-in-pytorch.md -->
# Image Classification in PyTorch

Related spaces: https://huggingface.co/spaces/abidlabs/pytorch-image-classifier, https://huggingface.co/spaces/pytorch/ResNet, https://huggingface.co/spaces/pytorch/ResNext, https://huggingface.co/spaces/pytorch/SqueezeNet
Tags: VISION, RESNET, PYTORCH

## Introduction

Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from autonomous vehicles to medical imaging.

Such models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like the demo on the bottom of the page.

Let's get started!

### Prerequisites

Make sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained image classification model, so you should also have `torch` installed.

## Step 1 â€” Setting up the Image Classification Model

First, we will need an image classification model. For this tutorial, we will use a pretrained Resnet-18 model, as it is easily downloadable from [PyTorch Hub](https://pytorch.org/hub/pytorch_vision_resnet/). You can use a different pretrained model or train your own.

```python
import torch

model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()
```

Because we will be using the model for inference, we have called the `.eval()` method.

## Step 2 â€” Defining a `predict` function

Next, we will need to define a function that takes in the _user input_, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).

In the case of our pretrained model, it will look like this:

```python
import requests
from PIL import Image
from torchvision import transforms

# Download human-readable labels for ImageNet.
response = requests.get("https://git.io/JJkYN")
labels = response.text.split("\n")

def predict(inp):
  inp = transforms.ToTensor()(inp).unsqueeze(0)
  with torch.no_grad():
    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)
    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}
  return confidences
```

Let's break this down. The function takes one parameter:

- `inp`: the input image as a `PIL` image

Then, the function converts the image to a PIL Image and then eventually a PyTorch `tensor`, passes it through the model, and returns:

- `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities

## Step 3 â€” Creating a Gradio Interface

Now that we have our predictive function set up, we can create a Gradio Interface around it.

In this case, the input component is a drag-and-drop image component. To create this input, we use `Image(type="pil")` which creates the component and handles the preprocessing to convert that to a `PIL` image.

The output component will be a `Label`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images by constructing it as `Label(num_top_classes=3)`.

Finally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:

```python
import gradio as gr

gr.Interface(fn=predict,
             inputs=gr.Image(type="pil"),
             outputs=gr.Label(num_top_classes=3),
             examples=["lion.jpg", "cheetah.jpg"]).launch()
```

This produces the following interface, which you can try right here in your browser (try uploading your own examples!):

<gradio-app space="gradio/pytorch-image-classifier">


---

And you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!

---

<!-- Source: guides/11_other-tutorials/image-classification-with-vision-transformers.md -->
# Image Classification with Vision Transformers

Related spaces: https://huggingface.co/spaces/abidlabs/vision-transformer
Tags: VISION, TRANSFORMERS, HUB

## Introduction

Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.

State-of-the-art image classifiers are based on the _transformers_ architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like the demo on the bottom of the page.

Let's get started!

### Prerequisites

Make sure you have the `gradio` Python package already [installed](/getting_started).

## Step 1 â€” Choosing a Vision Image Classification Model

First, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.

Expand the Tasks category on the left sidebar and select "Image Classification" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.

At the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.

## Step 2 â€” Loading the Vision Transformer Model with Gradio

When using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing.
All of these are automatically inferred from the model tags.

Besides the import statement, it only takes a single line of Python to load and launch the demo.

We use the `gr.Interface.load()` method and pass in the path to the model including the `huggingface/` to designate that it is from the Hugging Face Hub.

```python
import gradio as gr

gr.Interface.load(
             "huggingface/google/vit-base-patch16-224",
             examples=["alligator.jpg", "laptop.jpg"]).launch()
```

Notice that we have added one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples.

This produces the following interface, which you can try right here in your browser. When you input an image, it is automatically preprocessed and sent to the Hugging Face Hub API, where it is passed through the model and returned as a human-interpretable prediction. Try uploading your own image!

<gradio-app space="gradio/vision-transformer">

---

And you're done! In one line of code, you have built a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!

---

<!-- Source: guides/11_other-tutorials/installing-gradio-in-a-virtual-environment.md -->
# Installing Gradio in a Virtual Environment

Tags: INSTALLATION

In this guide, we will describe step-by-step how to install `gradio` within a virtual environment. This guide will cover both Windows and MacOS/Linux systems.

## Virtual Environments

A virtual environment in Python is a self-contained directory that holds a Python installation for a particular version of Python, along with a number of additional packages. This environment is isolated from the main Python installation and other virtual environments. Each environment can have its own independent set of installed Python packages, which allows you to maintain different versions of libraries for different projects without conflicts.


Using virtual environments ensures that you can work on multiple Python projects on the same machine without any conflicts. This is particularly useful when different projects require different versions of the same library. It also simplifies dependency management and enhances reproducibility, as you can easily share the requirements of your project with others.


## Installing Gradio on Windows

To install Gradio on a Windows system in a virtual environment, follow these steps:

1. **Install Python**: Ensure you have Python 3.10 or higher installed. You can download it from [python.org](https://www.python.org/). You can verify the installation by running `python --version` or `python3 --version` in Command Prompt.


2. **Create a Virtual Environment**:
   Open Command Prompt and navigate to your project directory. Then create a virtual environment using the following command:

   ```bash
   python -m venv gradio-env
   ```

   This command creates a new directory `gradio-env` in your project folder, containing a fresh Python installation.

3. **Activate the Virtual Environment**:
   To activate the virtual environment, run:

   ```bash
   .\gradio-env\Scripts\activate
   ```

   Your command prompt should now indicate that you are working inside `gradio-env`. Note: you can choose a different name than `gradio-env` for your virtual environment in this step.


4. **Install Gradio**:
   Now, you can install Gradio using pip:

   ```bash
   pip install gradio
   ```

5. **Verification**:
   To verify the installation, run `python` and then type:

   ```python
   import gradio as gr
   print(gr.__version__)
   ```

   This will display the installed version of Gradio.

## Installing Gradio on MacOS/Linux

The installation steps on MacOS and Linux are similar to Windows but with some differences in commands.

1. **Install Python**:
   Python usually comes pre-installed on MacOS and most Linux distributions. You can verify the installation by running `python --version` in the terminal (note that depending on how Python is installed, you might have to use `python3` instead of `python` throughout these steps). 
   
   Ensure you have Python 3.10 or higher installed. If you do not have it installed, you can download it from [python.org](https://www.python.org/). 

2. **Create a Virtual Environment**:
   Open Terminal and navigate to your project directory. Then create a virtual environment using:

   ```bash
   python -m venv gradio-env
   ```

   Note: you can choose a different name than `gradio-env` for your virtual environment in this step.

3. **Activate the Virtual Environment**:
   To activate the virtual environment on MacOS/Linux, use:

   ```bash
   source gradio-env/bin/activate
   ```

4. **Install Gradio**:
   With the virtual environment activated, install Gradio using pip:

   ```bash
   pip install gradio
   ```

5. **Verification**:
   To verify the installation, run `python` and then type:

   ```python
   import gradio as gr
   print(gr.__version__)
   ```

   This will display the installed version of Gradio.

By following these steps, you can successfully install Gradio in a virtual environment on your operating system, ensuring a clean and managed workspace for your Python projects.

---

<!-- Source: guides/11_other-tutorials/named-entity-recognition.md -->
# Named-Entity Recognition

Related spaces: https://huggingface.co/spaces/rajistics/biobert_ner_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial_Analyst_AI
Tags: NER, TEXT, HIGHLIGHT

## Introduction

Named-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or "token") into different categories, such as names of people or names of locations, or different parts of speech.

For example, given the sentence:

> Does Chicago have any Pakistani restaurants?

A named-entity recognition algorithm may identify:

- "Chicago" as a **location**
- "Pakistani" as an **ethnicity**

and so on.

Using `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team.

Here is an example of a demo that you'll be able to build:

$demo_ner_pipeline

This tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!

### Prerequisites

Make sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained named-entity recognition model. You can use your own, while in this tutorial, we will use one from the `transformers` library.

### Approach 1: List of Entity Dictionaries

Many named-entity recognition models output a list of dictionaries. Each dictionary consists of an _entity_, a "start" index, and an "end" index. This is, for example, how NER models in the `transformers` library operate:

```py
from transformers import pipeline
ner_pipeline = pipeline("ner")
ner_pipeline("Does Chicago have any Pakistani restaurants")
```

Output:

```bash
[{'entity': 'I-LOC',
  'score': 0.9988978,
  'index': 2,
  'word': 'Chicago',
  'start': 5,
  'end': 12},
 {'entity': 'I-MISC',
  'score': 0.9958592,
  'index': 5,
  'word': 'Pakistani',
  'start': 22,
  'end': 31}]
```

If you have such a model, it is very easy to hook it up to Gradio's `HighlightedText` component. All you need to do is pass in this **list of entities**, along with the **original text** to the model, together as dictionary, with the keys being `"entities"` and `"text"` respectively.

Here is a complete example:

$code_ner_pipeline
$demo_ner_pipeline

### Approach 2: List of Tuples

An alternative way to pass data into the `HighlightedText` component is a list of tuples. The first element of each tuple should be the word or words that are being classified into a particular entity. The second element should be the entity label (or `None` if they should be unlabeled). The `HighlightedText` component automatically strings together the words and labels to display the entities.

In some cases, this can be easier than the first approach. Here is a demo showing this approach using Spacy's parts-of-speech tagger:

$code_text_analysis
$demo_text_analysis

---

And you're done! That's all you need to know to build a web-based GUI for your NER model.

Fun tip: you can share your NER demo instantly with others simply by setting `share=True` in `launch()`.

---

<!-- Source: guides/11_other-tutorials/plot-component-for-maps.md -->
# How to Use the Plot Component for Maps

Tags: PLOTS, MAPS

## Introduction

This guide explains how you can use Gradio to plot geographical data on a map using the `gradio.Plot` component. The Gradio `Plot` component works with Matplotlib, Bokeh and Plotly. Plotly is what we will be working with in this guide. Plotly allows developers to easily create all sorts of maps with their geographical data. Take a look [here](https://plotly.com/python/maps/) for some examples.

## Overview

We will be using the New York City Airbnb dataset, which is hosted on kaggle [here](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data). I've uploaded it to the Hugging Face Hub as a dataset [here](https://huggingface.co/datasets/gradio/NYC-Airbnb-Open-Data) for easier use and download. Using this data we will plot Airbnb locations on a map output and allow filtering based on price and location. Below is the demo that we will be building. âš¡ï¸

$demo_map_airbnb

## Step 1 - Loading CSV data ðŸ’¾

Let's start by loading the Airbnb NYC data from the Hugging Face Hub.

```python
from datasets import load_dataset

dataset = load_dataset("gradio/NYC-Airbnb-Open-Data", split="train")
df = dataset.to_pandas()

def filter_map(min_price, max_price, boroughs):
    new_df = df[(df['neighbourhood_group'].isin(boroughs)) &
            (df['price'] > min_price) & (df['price'] < max_price)]
    names = new_df["name"].tolist()
    prices = new_df["price"].tolist()
    text_list = [(names[i], prices[i]) for i in range(0, len(names))]
```

In the code above, we first load the csv data into a pandas dataframe. Let's begin by defining a function that we will use as the prediction function for the gradio app. This function will accept the minimum price and maximum price range as well as the list of boroughs to filter the resulting map. We can use the passed in values (`min_price`, `max_price`, and list of `boroughs`) to filter the dataframe and create `new_df`. Next we will create `text_list` of the names and prices of each Airbnb to use as labels on the map.

## Step 2 - Map Figure ðŸŒ

Plotly makes it easy to work with maps. Let's take a look below how we can create a map figure.

```python
import plotly.graph_objects as go

fig = go.Figure(go.Scattermapbox(
            customdata=text_list,
            lat=new_df['latitude'].tolist(),
            lon=new_df['longitude'].tolist(),
            mode='markers',
            marker=go.scattermapbox.Marker(
                size=6
            ),
            hoverinfo="text",
            hovertemplate='<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}'
        ))

fig.update_layout(
    mapbox_style="open-street-map",
    hovermode='closest',
    mapbox=dict(
        bearing=0,
        center=go.layout.mapbox.Center(
            lat=40.67,
            lon=-73.90
        ),
        pitch=0,
        zoom=9
    ),
)
```

Above, we create a scatter plot on mapbox by passing it our list of latitudes and longitudes to plot markers. We also pass in our custom data of names and prices for additional info to appear on every marker we hover over. Next we use `update_layout` to specify other map settings such as zoom, and centering.

More info [here](https://plotly.com/python/scattermapbox/) on scatter plots using Mapbox and Plotly.

## Step 3 - Gradio App âš¡ï¸

We will use two `gr.Number` components and a `gr.CheckboxGroup` to allow users of our app to specify price ranges and borough locations. We will then use the `gr.Plot` component as an output for our Plotly + Mapbox map we created earlier.

```python
with gr.Blocks() as demo:
    with gr.Column():
        with gr.Row():
            min_price = gr.Number(value=250, label="Minimum Price")
            max_price = gr.Number(value=1000, label="Maximum Price")
        boroughs = gr.CheckboxGroup(choices=["Queens", "Brooklyn", "Manhattan", "Bronx", "Staten Island"], value=["Queens", "Brooklyn"], label="Select Boroughs:")
        btn = gr.Button(value="Update Filter")
        map = gr.Plot()
    demo.load(filter_map, [min_price, max_price, boroughs], map)
    btn.click(filter_map, [min_price, max_price, boroughs], map)
```

We layout these components using the `gr.Column` and `gr.Row` and we'll also add event triggers for when the demo first loads and when our "Update Filter" button is clicked in order to trigger the map to update with our new filters.

This is what the full demo code looks like:

$code_map_airbnb

## Step 4 - Deployment ðŸ¤—

If you run the code above, your app will start running locally.
You can even get a temporary shareable link by passing the `share=True` parameter to `launch`.

But what if you want to a permanent deployment solution?
Let's deploy our Gradio app to the free HuggingFace Spaces platform.

If you haven't used Spaces before, follow the previous guide [here](/using_hugging_face_integrations).

## Conclusion ðŸŽ‰

And you're all done! That's all the code you need to build a map demo.

Here's a link to the demo [Map demo](https://huggingface.co/spaces/gradio/map_airbnb) and [complete code](https://huggingface.co/spaces/gradio/map_airbnb/blob/main/run.py) (on Hugging Face Spaces)

---

<!-- Source: guides/11_other-tutorials/running-background-tasks.md -->
# Running Background Tasks

Related spaces: https://huggingface.co/spaces/freddyaboulton/gradio-google-forms
Tags: TASKS, SCHEDULED, TABULAR, DATA

## Introduction

This guide explains how you can run background tasks from your gradio app.
Background tasks are operations that you'd like to perform outside the request-response
lifecycle of your app either once or on a periodic schedule.
Examples of background tasks include periodically synchronizing data to an external database or
sending a report of model predictions via email.

## Overview

We will be creating a simple "Google-forms-style" application to gather feedback from users of the gradio library.
We will use a local sqlite database to store our data, but we will periodically synchronize the state of the database
with a [HuggingFace Dataset](https://huggingface.co/datasets) so that our user reviews are always backed up.
The synchronization will happen in a background task running every 60 seconds.

At the end of the demo, you'll have a fully working application like this one:

<gradio-app space="freddyaboulton/gradio-google-forms"> </gradio-app>

## Step 1 - Write your database logic ðŸ’¾

Our application will store the name of the reviewer, their rating of gradio on a scale of 1 to 5, as well as
any comments they want to share about the library. Let's write some code that creates a database table to
store this data. We'll also write some functions to insert a review into that table and fetch the latest 10 reviews.

We're going to use the `sqlite3` library to connect to our sqlite database but gradio will work with any library.

The code will look like this:

```python
DB_FILE = "./reviews.db"
db = sqlite3.connect(DB_FILE)

# Create table if it doesn't already exist
try:
    db.execute("SELECT * FROM reviews").fetchall()
    db.close()
except sqlite3.OperationalError:
    db.execute(
        '''
        CREATE TABLE reviews (id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
                              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
                              name TEXT, review INTEGER, comments TEXT)
        ''')
    db.commit()
    db.close()

def get_latest_reviews(db: sqlite3.Connection):
    reviews = db.execute("SELECT * FROM reviews ORDER BY id DESC limit 10").fetchall()
    total_reviews = db.execute("Select COUNT(id) from reviews").fetchone()[0]
    reviews = pd.DataFrame(reviews, columns=["id", "date_created", "name", "review", "comments"])
    return reviews, total_reviews


def add_review(name: str, review: int, comments: str):
    db = sqlite3.connect(DB_FILE)
    cursor = db.cursor()
    cursor.execute("INSERT INTO reviews(name, review, comments) VALUES(?,?,?)", [name, review, comments])
    db.commit()
    reviews, total_reviews = get_latest_reviews(db)
    db.close()
    return reviews, total_reviews
```

Let's also write a function to load the latest reviews when the gradio application loads:

```python
def load_data():
    db = sqlite3.connect(DB_FILE)
    reviews, total_reviews = get_latest_reviews(db)
    db.close()
    return reviews, total_reviews
```

## Step 2 - Create a gradio app âš¡

Now that we have our database logic defined, we can use gradio create a dynamic web page to ask our users for feedback!

```python
with gr.Blocks() as demo:
    with gr.Row():
        with gr.Column():
            name = gr.Textbox(label="Name", placeholder="What is your name?")
            review = gr.Radio(label="How satisfied are you with using gradio?", choices=[1, 2, 3, 4, 5])
            comments = gr.Textbox(label="Comments", lines=10, placeholder="Do you have any feedback on gradio?")
            submit = gr.Button(value="Submit Feedback")
        with gr.Column():
            data = gr.Dataframe(label="Most recently created 10 rows")
            count = gr.Number(label="Total number of reviews")
    submit.click(add_review, [name, review, comments], [data, count])
    demo.load(load_data, None, [data, count])
```

## Step 3 - Synchronize with HuggingFace Datasets ðŸ¤—

We could call `demo.launch()` after step 2 and have a fully functioning application. However,
our data would be stored locally on our machine. If the sqlite file were accidentally deleted, we'd lose all of our reviews!
Let's back up our data to a dataset on the HuggingFace hub.

Create a dataset [here](https://huggingface.co/datasets) before proceeding.

Now at the **top** of our script, we'll use the [huggingface hub client library](https://huggingface.co/docs/huggingface_hub/index)
to connect to our dataset and pull the latest backup.

```python
TOKEN = os.environ.get('HUB_TOKEN')
repo = huggingface_hub.Repository(
    local_dir="data",
    repo_type="dataset",
    clone_from="<name-of-your-dataset>",
    use_auth_token=TOKEN
)
repo.git_pull()

shutil.copyfile("./data/reviews.db", DB_FILE)
```

Note that you'll have to get an access token from the "Settings" tab of your HuggingFace for the above code to work.
In the script, the token is securely accessed via an environment variable.

![access_token](https://github.com/gradio-app/gradio/blob/main/guides/assets/access_token.png?raw=true)

Now we will create a background task to synch our local database to the dataset hub every 60 seconds.
We will use the [AdvancedPythonScheduler](https://apscheduler.readthedocs.io/en/3.x/) to handle the scheduling.
However, this is not the only task scheduling library available. Feel free to use whatever you are comfortable with.

The function to back up our data will look like this:

```python
from apscheduler.schedulers.background import BackgroundScheduler

def backup_db():
    shutil.copyfile(DB_FILE, "./data/reviews.db")
    db = sqlite3.connect(DB_FILE)
    reviews = db.execute("SELECT * FROM reviews").fetchall()
    pd.DataFrame(reviews).to_csv("./data/reviews.csv", index=False)
    print("updating db")
    repo.push_to_hub(blocking=False, commit_message=f"Updating data at {datetime.datetime.now()}")


scheduler = BackgroundScheduler()
scheduler.add_job(func=backup_db, trigger="interval", seconds=60)
scheduler.start()
```

## Step 4 (Bonus) - Deployment to HuggingFace Spaces

You can use the HuggingFace [Spaces](https://huggingface.co/spaces) platform to deploy this application for free âœ¨

If you haven't used Spaces before, follow the previous guide [here](/using_hugging_face_integrations).
You will have to use the `HUB_TOKEN` environment variable as a secret in the Guides.

## Conclusion

Congratulations! You know how to run background tasks from your gradio app on a schedule â²ï¸.

Checkout the application running on Spaces [here](https://huggingface.co/spaces/freddyaboulton/gradio-google-forms).
The complete code is [here](https://huggingface.co/spaces/freddyaboulton/gradio-google-forms/blob/main/app.py)

---

<!-- Source: guides/11_other-tutorials/running-gradio-on-your-web-server-with-nginx.md -->
# Running a Gradio App on your Web Server with Nginx

Tags: DEPLOYMENT, WEB SERVER, NGINX

## Introduction

Gradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines. Gradio apps can be deployed on [Hugging Face Spaces](https://hf.space) for free.

In some cases though, you might want to deploy a Gradio app on your own web server. You might already be using [Nginx](https://www.nginx.com/), a highly performant web server, to serve your website (say `https://www.example.com`), and you want to attach Gradio to a specific subpath on your website (e.g. `https://www.example.com/gradio-demo`).

In this Guide, we will guide you through the process of running a Gradio app behind Nginx on your own web server to achieve this.

**Prerequisites**

1. A Linux web server with [Nginx installed](https://www.nginx.com/blog/setting-up-nginx/) and [Gradio installed](/quickstart)
2. A working Gradio app saved as a python file on your web server

## Editing your Nginx configuration file

1. Start by editing the Nginx configuration file on your web server. By default, this is located at: `/etc/nginx/nginx.conf`

In the `http` block, add the following line to include server block configurations from a separate file:

```bash
include /etc/nginx/sites-enabled/*;
```

2. Create a new file in the `/etc/nginx/sites-available` directory (create the directory if it does not already exist), using a filename that represents your app, for example: `sudo nano /etc/nginx/sites-available/my_gradio_app`

3. Paste the following into your file editor:

```bash
server {
    listen 80;
    server_name example.com www.example.com;  # Change this to your domain name

    location /gradio-demo/ {  # Change this if you'd like to server your Gradio app on a different path
        proxy_pass http://127.0.0.1:7860/; # Change this if your Gradio app will be running on a different port
        proxy_buffering off;
        proxy_redirect off;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-Host $host;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```


Tip: Setting the `X-Forwarded-Host` and `X-Forwarded-Proto` headers is important as Gradio uses these, in conjunction with the `root_path` parameter discussed below, to construct the public URL that your app is being served on. Gradio uses the public URL to fetch various static assets. If these headers are not set, your Gradio app may load in a broken state.

*Note:* The `$host` variable does not include the host port. If you are serving your Gradio application on a raw IP address and port, you should use the `$http_host` variable instead, in these lines:

```bash
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-Host $host;
```

## Run your Gradio app on your web server

1. Before you launch your Gradio app, you'll need to set the `root_path` to be the same as the subpath that you specified in your nginx configuration. This is necessary for Gradio to run on any subpath besides the root of the domain.

    *Note:* Instead of a subpath, you can also provide a complete URL for `root_path` (beginning with `http` or `https`) in which case the `root_path` is treated as an absolute URL instead of a URL suffix (but in this case, you'll need to update the `root_path` if the domain changes).

Here's a simple example of a Gradio app with a custom `root_path` corresponding to the Nginx configuration above.

```python
import gradio as gr
import time

def test(x):
time.sleep(4)
return x

gr.Interface(test, "textbox", "textbox").queue().launch(root_path="/gradio-demo")
```

2. Start a `tmux` session by typing `tmux` and pressing enter (optional)

It's recommended that you run your Gradio app in a `tmux` session so that you can keep it running in the background easily

3. Then, start your Gradio app. Simply type in `python` followed by the name of your Gradio python file. By default, your app will run on `localhost:7860`, but if it starts on a different port, you will need to update the nginx configuration file above.

## Restart Nginx

1. If you are in a tmux session, exit by typing CTRL+B (or CMD+B), followed by the "D" key.

2. Finally, restart nginx by running `sudo systemctl restart nginx`.

And that's it! If you visit `https://example.com/gradio-demo` on your browser, you should see your Gradio app running there

---

<!-- Source: guides/11_other-tutorials/setting-up-a-demo-for-maximum-performance.md -->
# Setting Up a Demo for Maximum Performance

Tags: CONCURRENCY, LATENCY, PERFORMANCE

Let's say that your Gradio demo goes _viral_ on social media -- you have lots of users trying it out simultaneously, and you want to provide your users with the best possible experience or, in other words, minimize the amount of time that each user has to wait in the queue to see their prediction.

How can you configure your Gradio demo to handle the most traffic? In this Guide, we dive into some of the parameters of Gradio's `.queue()` method as well as some other related parameters, and discuss how to set these parameters in a way that allows you to serve lots of users simultaneously with minimal latency.

This is an advanced guide, so make sure you know the basics of Gradio already, such as [how to create and launch a Gradio Interface](https://gradio.app/guides/quickstart/). Most of the information in this Guide is relevant whether you are hosting your demo on [Hugging Face Spaces](https://hf.space) or on your own server.

## Overview of Gradio's Queueing System

By default, every Gradio demo includes a built-in queuing system that scales to thousands of requests. When a user of your app submits a request (i.e. submits an input to your function), Gradio adds the request to the queue, and requests are processed in order, generally speaking (this is not exactly true, as discussed below). When the user's request has finished processing, the Gradio server returns the result back to the user using server-side events (SSE). The SSE protocol has several advantages over simply using HTTP POST requests: 

(1) They do not time out -- most browsers raise a timeout error if they do not get a response to a POST request after a short period of time (e.g. 1 min). This can be a problem if your inference function takes longer than 1 minute to run or if many people are trying out your demo at the same time, resulting in increased latency.

(2) They allow the server to send multiple updates to the frontend. This means, for example, that the server can send a real-time ETA of how long your prediction will take to complete.

To configure the queue, simply call the `.queue()` method before launching an `Interface`, `TabbedInterface`, `ChatInterface` or any `Blocks`. Here's an example:

```py
import gradio as gr

app = gr.Interface(lambda x:x, "image", "image")
app.queue()  # <-- Sets up a queue with default parameters
app.launch()
```

**How Requests are Processed from the Queue**

When a Gradio server is launched, a pool of threads is used to execute requests from the queue. By default, the maximum size of this thread pool is `40` (which is the default inherited from FastAPI, on which the Gradio server is based). However, this does *not* mean that 40 requests are always processed in parallel from the queue. 

Instead, Gradio uses a **single-function-single-worker** model by default. This means that each worker thread is only assigned a single function from among all of the functions that could be part of your Gradio app. This ensures that you do not see, for example, out-of-memory errors, due to multiple workers calling a machine learning model at the same time. Suppose you have 3 functions in your Gradio app: A, B, and C. And you see the following sequence of 7 requests come in from users using your app:

```
1 2 3 4 5 6 7
-------------
A B A A C B A
```

Initially, 3 workers will get dispatched to handle requests 1, 2, and 5 (corresponding to functions: A, B, C). As soon as any of these workers finish, they will start processing the next function in the queue of the same function type, e.g. the worker that finished processing request 1 will start processing request 3, and so on.

If you want to change this behavior, there are several parameters that can be used to configure the queue and help reduce latency. Let's go through them one-by-one.


### The `default_concurrency_limit` parameter in `queue()`

The first parameter we will explore is the `default_concurrency_limit` parameter in `queue()`. This controls how many workers can execute the same event. By default, this is set to `1`, but you can set it to a higher integer: `2`, `10`, or even `None` (in the last case, there is no limit besides the total number of available workers). 

This is useful, for example, if your Gradio app does not call any resource-intensive functions. If your app only queries external APIs, then you can set the `default_concurrency_limit` much higher. Increasing this parameter can **linearly multiply the capacity of your server to handle requests**.

So why not set this parameter much higher all the time? Keep in mind that since requests are processed in parallel, each request will consume memory to store the data and weights for processing. This means that you might get out-of-memory errors if you increase the `default_concurrency_limit` too high. You may also start to get diminishing returns if the `default_concurrency_limit` is too high because of costs of switching between different worker threads.

**Recommendation**: Increase the `default_concurrency_limit` parameter as high as you can while you continue to see performance gains or until you hit memory limits on your machine. You can [read about Hugging Face Spaces machine specs here](https://huggingface.co/docs/hub/spaces-overview).


### The `concurrency_limit` parameter in events

You can also set the number of requests that can be processed in parallel for each event individually. These take priority over the  `default_concurrency_limit` parameter described previously.

To do this, set the `concurrency_limit` parameter of any event listener, e.g. `btn.click(..., concurrency_limit=20)` or in the `Interface` or `ChatInterface` classes: e.g. `gr.Interface(..., concurrency_limit=20)`. By default, this parameter is set to the global `default_concurrency_limit`.


### The `max_threads` parameter in `launch()`

If your demo uses non-async functions, e.g. `def` instead of `async def`, they will be run in a threadpool. This threadpool has a size of 40 meaning that only 40 threads can be created to run your non-async functions. If you are running into this limit, you can increase the threadpool size with `max_threads`. The default value is 40.

Tip: You should use async functions whenever possible to increase the number of concurrent requests your app can handle. Quick functions that are not CPU-bound are good candidates to be written as `async`. This [guide](https://fastapi.tiangolo.com/async/) is a good primer on the concept.


### The `max_size` parameter in `queue()`

A more blunt way to reduce the wait times is simply to prevent too many people from joining the queue in the first place. You can set the maximum number of requests that the queue processes using the `max_size` parameter of `queue()`. If a request arrives when the queue is already of the maximum size, it will not be allowed to join the queue and instead, the user will receive an error saying that the queue is full and to try again. By default, `max_size=None`, meaning that there is no limit to the number of users that can join the queue.

Paradoxically, setting a `max_size` can often improve user experience because it prevents users from being dissuaded by very long queue wait times. Users who are more interested and invested in your demo will keep trying to join the queue, and will be able to get their results faster.

**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your expectations of how long users might be willing to wait for a prediction.

### The `max_batch_size` parameter in events

Another way to increase the parallelism of your Gradio demo is to write your function so that it can accept **batches** of inputs. Most deep learning models can process batches of samples more efficiently than processing individual samples.

If you write your function to process a batch of samples, Gradio will automatically batch incoming requests together and pass them into your function as a batch of samples. You need to set `batch` to `True` (by default it is `False`) and set a `max_batch_size` (by default it is `4`) based on the maximum number of samples your function is able to handle. These two parameters can be passed into `gr.Interface()` or to an event in Blocks such as `.click()`.

While setting a batch is conceptually similar to having workers process requests in parallel, it is often _faster_ than setting the `concurrency_count` for deep learning models. The downside is that you might need to adapt your function a little bit to accept batches of samples instead of individual samples.

Here's an example of a function that does _not_ accept a batch of inputs -- it processes a single input at a time:

```py
import time

def trim_words(word, length):
    return word[:int(length)]

```

Here's the same function rewritten to take in a batch of samples:

```py
import time

def trim_words(words, lengths):
    trimmed_words = []
    for w, l in zip(words, lengths):
        trimmed_words.append(w[:int(l)])
    return [trimmed_words]

```

The second function can be used with `batch=True` and an appropriate `max_batch_size` parameter.

**Recommendation**: If possible, write your function to accept batches of samples, and then set `batch` to `True` and the `max_batch_size` as high as possible based on your machine's memory limits.

## Upgrading your Hardware (GPUs, TPUs, etc.)

If you have done everything above, and your demo is still not fast enough, you can upgrade the hardware that your model is running on. Changing the model from running on CPUs to running on GPUs will usually provide a 10x-50x increase in inference time for deep learning models.

It is particularly straightforward to upgrade your Hardware on Hugging Face Spaces. Simply click on the "Settings" tab in your Space and choose the Space Hardware you'd like.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-gpu-settings.png)

While you might need to adapt portions of your machine learning inference code to run on a GPU (here's a [handy guide](https://cnvrg.io/pytorch-cuda/) if you are using PyTorch), Gradio is completely agnostic to the choice of hardware and will work completely fine if you use it with CPUs, GPUs, TPUs, or any other hardware!

Note: your GPU memory is different than your CPU memory, so if you upgrade your hardware,
you might need to adjust the value of the `default_concurrency_limit` parameter described above.

## Conclusion

Congratulations! You know how to set up a Gradio demo for maximum performance. Good luck on your next viral demo!

---

<!-- Source: guides/11_other-tutorials/styling-the-gradio-dataframe.md -->
# How to Style the Gradio Dataframe

Tags: DATAFRAME, STYLE, COLOR

## Introduction

Data visualization is a crucial aspect of data analysis and machine learning. The Gradio `DataFrame` component is a popular way to display tabular data within a web application. 

But what if you want to stylize the table of data? What if you want to add background colors, partially highlight cells, or change the display precision of numbers? This Guide is for you!



Let's dive in!

**Prerequisites**: We'll be using the `gradio.Blocks` class in our examples.
You can [read the Guide to Blocks first](https://gradio.app/blocks-and-event-listeners) if you are not already familiar with it. Also please make sure you are using the **latest version** version of Gradio: `pip install --upgrade gradio`.


## The Pandas `Styler`

The Gradio `DataFrame` component now supports values of the type `Styler` from the `pandas` class. This allows us to reuse the rich existing API and documentation of the `Styler` class instead of inventing a new style format on our own. Here's a complete example of how it looks:

```python
import pandas as pd 
import gradio as gr

# Creating a sample dataframe
df = pd.DataFrame({
    "A" : [14, 4, 5, 4, 1], 
    "B" : [5, 2, 54, 3, 2], 
    "C" : [20, 20, 7, 3, 8], 
    "D" : [14, 3, 6, 2, 6], 
    "E" : [23, 45, 64, 32, 23]
}) 

# Applying style to highlight the maximum value in each row
styler = df.style.highlight_max(color = 'lightgreen', axis = 0)

# Displaying the styled dataframe in Gradio
with gr.Blocks() as demo:
    gr.DataFrame(styler)
    
demo.launch()
```

The Styler class can be used to apply conditional formatting and styling to dataframes, making them more visually appealing and interpretable. You can highlight certain values, apply gradients, or even use custom CSS to style the DataFrame. The Styler object is applied to a DataFrame and it returns a new object with the relevant styling properties, which can then be previewed directly, or rendered dynamically in a Gradio interface.

To read more about the Styler object, read the official `pandas` documentation at: https://pandas.pydata.org/docs/user_guide/style.html

Below, we'll explore a few examples:

### Highlighting Cells

Ok, so let's revisit the previous example. We start by creating a `pd.DataFrame` object and then highlight the highest value in each row with a light green color:

```python
import pandas as pd 

# Creating a sample dataframe
df = pd.DataFrame({
    "A" : [14, 4, 5, 4, 1], 
    "B" : [5, 2, 54, 3, 2], 
    "C" : [20, 20, 7, 3, 8], 
    "D" : [14, 3, 6, 2, 6], 
    "E" : [23, 45, 64, 32, 23]
}) 

# Applying style to highlight the maximum value in each row
styler = df.style.highlight_max(color = 'lightgreen', axis = 0)
```

Now, we simply pass this object into the Gradio `DataFrame` and we can visualize our colorful table of data in 4 lines of python:

```python
import gradio as gr

with gr.Blocks() as demo:
    gr.Dataframe(styler)
    
demo.launch()
```

Here's how it looks:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/df-highlight.png)

### Font Colors

Apart from highlighting cells, you might want to color specific text within the cells. Here's how you can change text colors for certain columns:

```python
import pandas as pd 
import gradio as gr

# Creating a sample dataframe
df = pd.DataFrame({
    "A" : [14, 4, 5, 4, 1], 
    "B" : [5, 2, 54, 3, 2], 
    "C" : [20, 20, 7, 3, 8], 
    "D" : [14, 3, 6, 2, 6], 
    "E" : [23, 45, 64, 32, 23]
}) 

# Function to apply text color
def highlight_cols(x): 
    df = x.copy() 
    df.loc[:, :] = 'color: purple'
    df[['B', 'C', 'E']] = 'color: green'
    return df 

# Applying the style function
s = df.style.apply(highlight_cols, axis = None)

# Displaying the styled dataframe in Gradio
with gr.Blocks() as demo:
    gr.DataFrame(s)
    
demo.launch()
```

In this script, we define a custom function highlight_cols that changes the text color to purple for all cells, but overrides this for columns B, C, and E with green. Here's how it looks:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/df-color.png)

### Display Precision 

Sometimes, the data you are dealing with might have long floating numbers, and you may want to display only a fixed number of decimals for simplicity. The pandas Styler object allows you to format the precision of numbers displayed. Here's how you can do this:

```python
import pandas as pd
import gradio as gr

# Creating a sample dataframe with floating numbers
df = pd.DataFrame({
    "A" : [14.12345, 4.23456, 5.34567, 4.45678, 1.56789], 
    "B" : [5.67891, 2.78912, 54.89123, 3.91234, 2.12345], 
    # ... other columns
}) 

# Setting the precision of numbers to 2 decimal places
s = df.style.format("{:.2f}")

# Displaying the styled dataframe in Gradio
with gr.Blocks() as demo:
    gr.DataFrame(s)
    
demo.launch()
```

In this script, the format method of the Styler object is used to set the precision of numbers to two decimal places. Much cleaner now:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/df-precision.png)



## Custom Styling

So far, we've been restricting ourselves to styling that is supported by the Pandas `Styler` class. But what if you want to create custom styles like partially highlighting cells based on their values:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/dataframe_custom_styling.png)


This isn't possible with `Styler`, but you can do this by creating your own **`styling`** array, which is a 2D array the same size and shape as your data. Each element in this list should be a CSS style string (e.g. `"background-color: green"`) that applies to the `<td>` element containing the cell value (or an empty string if no custom CSS should be applied). Similarly, you can create a **`display_value`** array which controls the value that is displayed in each cell (which can be different the underlying value which is the one that is used for searching/sorting).

Here's the complete code for how to can use custom styling with `gr.Dataframe` as in the screenshot above:

$code_dataframe_custom_styling


## Note about Interactivity

One thing to keep in mind is that the gradio `DataFrame` component only accepts custom styling objects when it is non-interactive (i.e. in "static" mode). If the `DataFrame` component is interactive, then the styling information is ignored and instead the raw table values are shown instead. 

The `DataFrame` component is by default non-interactive, unless it is used as an input to an event. In which case, you can force the component to be non-interactive by setting the `interactive` prop like this:

```python
c = gr.DataFrame(styler, interactive=False)
```

## Conclusion ðŸŽ‰

This is just a taste of what's possible using the `gradio.DataFrame` component with the `Styler` class from `pandas`. Try it out and let us know what you think!

---

<!-- Source: guides/11_other-tutorials/theming-guide.md -->
# Theming

Tags: THEMES

## Introduction

Gradio features a built-in theming engine that lets you customize the look and feel of your app. You can choose from a variety of themes, or create your own. To do so, pass the `theme=` kwarg to the `launch()` method of `Blocks` or `Interface`. For example:

```python
with gr.Blocks() as demo:
    ... # your code here
demo.launch(theme=gr.themes.Soft())
    ...
```

<div class="wrapper">
<iframe
	src="https://gradio-theme-soft.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

Gradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. These are:


* `gr.themes.Base()` - the `"base"` theme sets the primary color to blue but otherwise has minimal styling, making it particularly useful as a base for creating new, custom themes.
* `gr.themes.Default()` - the `"default"` Gradio 5 theme, with a vibrant orange primary color and gray secondary color.
* `gr.themes.Origin()` - the `"origin"` theme is most similar to Gradio 4 styling. Colors, especially in light mode, are more subdued than the Gradio 5 default theme.
* `gr.themes.Citrus()` - the `"citrus"` theme uses a yellow primary color, highlights form elements that are in focus, and includes fun 3D effects when buttons are clicked.
* `gr.themes.Monochrome()` - the `"monochrome"` theme uses a black primary and white secondary color, and uses serif-style fonts, giving the appearance of a black-and-white newspaper. 
* `gr.themes.Soft()` - the `"soft"` theme uses a purple primary color and white secondary color. It also increases the border radius around buttons and form elements and highlights labels.
* `gr.themes.Glass()` - the `"glass"` theme has a blue primary color and a transclucent gray secondary color. The theme also uses vertical gradients to create a glassy effect.
* `gr.themes.Ocean()` - the `"ocean"` theme has a blue-green primary color and gray secondary color. The theme also uses horizontal gradients, especially for buttons and some form elements.


Each of these themes set values for hundreds of CSS variables. You can use prebuilt themes as a starting point for your own custom themes, or you can create your own themes from scratch. Let's take a look at each approach.

## Using the Theme Builder

The easiest way to build a theme is using the Theme Builder. To launch the Theme Builder locally, run the following code:

```python
import gradio as gr

gr.themes.builder()
```

$demo_theme_builder

You can use the Theme Builder running on Spaces above, though it runs much faster when you launch it locally via `gr.themes.builder()`.

As you edit the values in the Theme Builder, the app will preview updates in real time. You can download the code to generate the theme you've created so you can use it in any Gradio app.

In the rest of the guide, we will cover building themes programmatically.

## Extending Themes via the Constructor

Although each theme has hundreds of CSS variables, the values for most these variables are drawn from 8 core variables which can be set through the constructor of each prebuilt theme. Modifying these 8 arguments allows you to quickly change the look and feel of your app.

### Core Colors

The first 3 constructor arguments set the colors of the theme and are `gradio.themes.Color` objects. Internally, these Color objects hold brightness values for the palette of a single hue, ranging from 50, 100, 200..., 800, 900, 950. Other CSS variables are derived from these 3 colors.

The 3 color constructor arguments are:

- `primary_hue`: This is the color draws attention in your theme. In the default theme, this is set to `gradio.themes.colors.orange`.
- `secondary_hue`: This is the color that is used for secondary elements in your theme. In the default theme, this is set to `gradio.themes.colors.blue`.
- `neutral_hue`: This is the color that is used for text and other neutral elements in your theme. In the default theme, this is set to `gradio.themes.colors.gray`.

You could modify these values using their string shortcuts, such as

```python
with gr.Blocks() as demo:
    ... # your code here
demo.launch(theme=gr.themes.Default(primary_hue="red", secondary_hue="pink"))
    ...
```

or you could use the `Color` objects directly, like this:

```python
with gr.Blocks() as demo:
    ... # your code here
demo.launch(theme=gr.themes.Default(primary_hue=gr.themes.colors.red, secondary_hue=gr.themes.colors.pink))
```

<div class="wrapper">
<iframe
	src="https://gradio-theme-extended-step-1.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

Predefined colors are:

- `slate`
- `gray`
- `zinc`
- `neutral`
- `stone`
- `red`
- `orange`
- `amber`
- `yellow`
- `lime`
- `green`
- `emerald`
- `teal`
- `cyan`
- `sky`
- `blue`
- `indigo`
- `violet`
- `purple`
- `fuchsia`
- `pink`
- `rose`

You could also create your own custom `Color` objects and pass them in.

### Core Sizing

The next 3 constructor arguments set the sizing of the theme and are `gradio.themes.Size` objects. Internally, these Size objects hold pixel size values that range from `xxs` to `xxl`. Other CSS variables are derived from these 3 sizes.

- `spacing_size`: This sets the padding within and spacing between elements. In the default theme, this is set to `gradio.themes.sizes.spacing_md`.
- `radius_size`: This sets the roundedness of corners of elements. In the default theme, this is set to `gradio.themes.sizes.radius_md`.
- `text_size`: This sets the font size of text. In the default theme, this is set to `gradio.themes.sizes.text_md`.

You could modify these values using their string shortcuts, such as

```python
with gr.Blocks() as demo:
    ... # your code here
demo.launch(theme=gr.themes.Default(spacing_size="sm", radius_size="none"))
    ...
```

or you could use the `Size` objects directly, like this:

```python
with gr.Blocks() as demo:
    ... # your code here
demo.launch(theme=gr.themes.Default(spacing_size=gr.themes.sizes.spacing_sm, radius_size=gr.themes.sizes.radius_none))
    ...
```

<div class="wrapper">
<iframe
	src="https://gradio-theme-extended-step-2.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

The predefined size objects are:

- `radius_none`
- `radius_sm`
- `radius_md`
- `radius_lg`
- `spacing_sm`
- `spacing_md`
- `spacing_lg`
- `text_sm`
- `text_md`
- `text_lg`

You could also create your own custom `Size` objects and pass them in.

### Core Fonts

The final 2 constructor arguments set the fonts of the theme. You can pass a list of fonts to each of these arguments to specify fallbacks. If you provide a string, it will be loaded as a system font. If you provide a `gradio.themes.GoogleFont`, the font will be loaded from Google Fonts.

- `font`: This sets the primary font of the theme. In the default theme, this is set to `gradio.themes.GoogleFont("IBM Plex Sans")`.
- `font_mono`: This sets the monospace font of the theme. In the default theme, this is set to `gradio.themes.GoogleFont("IBM Plex Mono")`.

You could modify these values such as the following:

```python
with gr.Blocks() as demo:
    ... # your code here
demo.launch(theme=gr.themes.Default(font=[gr.themes.GoogleFont("Inconsolata"), "Arial", "sans-serif"]))
    ...
```

<div class="wrapper">
<iframe
	src="https://gradio-theme-extended-step-3.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

## Extending Themes via `.set()`

You can also modify the values of CSS variables after the theme has been loaded. To do so, use the `.set()` method of the theme object to get access to the CSS variables. For example:

```python
theme = gr.themes.Default(primary_hue="blue").set(
    loader_color="#FF0000",
    slider_color="#FF0000",
)

with gr.Blocks() as demo:
    ... # your code here
demo.launch(theme=theme)
```

In the example above, we've set the `loader_color` and `slider_color` variables to `#FF0000`, despite the overall `primary_color` using the blue color palette. You can set any CSS variable that is defined in the theme in this manner.

Your IDE type hinting should help you navigate these variables. Since there are so many CSS variables, let's take a look at how these variables are named and organized.

### CSS Variable Naming Conventions

CSS variable names can get quite long, like `button_primary_background_fill_hover_dark`! However they follow a common naming convention that makes it easy to understand what they do and to find the variable you're looking for. Separated by underscores, the variable name is made up of:

1. The target element, such as `button`, `slider`, or `block`.
2. The target element type or sub-element, such as `button_primary`, or `block_label`.
3. The property, such as `button_primary_background_fill`, or `block_label_border_width`.
4. Any relevant state, such as `button_primary_background_fill_hover`.
5. If the value is different in dark mode, the suffix `_dark`. For example, `input_border_color_focus_dark`.

Of course, many CSS variable names are shorter than this, such as `table_border_color`, or `input_shadow`.

### CSS Variable Organization

Though there are hundreds of CSS variables, they do not all have to have individual values. They draw their values by referencing a set of core variables and referencing each other. This allows us to only have to modify a few variables to change the look and feel of the entire theme, while also getting finer control of individual elements that we may want to modify.

#### Referencing Core Variables

To reference one of the core constructor variables, precede the variable name with an asterisk. To reference a core color, use the `*primary_`, `*secondary_`, or `*neutral_` prefix, followed by the brightness value. For example:

```python
theme = gr.themes.Default(primary_hue="blue").set(
    button_primary_background_fill="*primary_200",
    button_primary_background_fill_hover="*primary_300",
)
```

In the example above, we've set the `button_primary_background_fill` and `button_primary_background_fill_hover` variables to `*primary_200` and `*primary_300`. These variables will be set to the 200 and 300 brightness values of the blue primary color palette, respectively.

Similarly, to reference a core size, use the `*spacing_`, `*radius_`, or `*text_` prefix, followed by the size value. For example:

```python
theme = gr.themes.Default(radius_size="md").set(
    button_primary_border_radius="*radius_xl",
)
```

In the example above, we've set the `button_primary_border_radius` variable to `*radius_xl`. This variable will be set to the `xl` setting of the medium radius size range.

#### Referencing Other Variables

Variables can also reference each other. For example, look at the example below:

```python
theme = gr.themes.Default().set(
    button_primary_background_fill="#FF0000",
    button_primary_background_fill_hover="#FF0000",
    button_primary_border="#FF0000",
)
```

Having to set these values to a common color is a bit tedious. Instead, we can reference the `button_primary_background_fill` variable in the `button_primary_background_fill_hover` and `button_primary_border` variables, using a `*` prefix.

```python
theme = gr.themes.Default().set(
    button_primary_background_fill="#FF0000",
    button_primary_background_fill_hover="*button_primary_background_fill",
    button_primary_border="*button_primary_background_fill",
)
```

Now, if we change the `button_primary_background_fill` variable, the `button_primary_background_fill_hover` and `button_primary_border` variables will automatically update as well.

This is particularly useful if you intend to share your theme - it makes it easy to modify the theme without having to change every variable.

Note that dark mode variables automatically reference each other. For example:

```python
theme = gr.themes.Default().set(
    button_primary_background_fill="#FF0000",
    button_primary_background_fill_dark="#AAAAAA",
    button_primary_border="*button_primary_background_fill",
    button_primary_border_dark="*button_primary_background_fill_dark",
)
```

`button_primary_border_dark` will draw its value from `button_primary_background_fill_dark`, because dark mode always draw from the dark version of the variable.

## Creating a Full Theme

Let's say you want to create a theme from scratch! We'll go through it step by step - you can also see the source of prebuilt themes in the gradio source repo for reference - [here's the source](https://github.com/gradio-app/gradio/blob/main/gradio/themes/monochrome.py) for the Monochrome theme.

Our new theme class will inherit from `gradio.themes.Base`, a theme that sets a lot of convenient defaults. Let's make a simple demo that creates a dummy theme called Seafoam, and make a simple app that uses it.

$code_theme_new_step_1

<div class="wrapper">
<iframe
	src="https://gradio-theme-new-step-1.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

The Base theme is very barebones, and uses `gr.themes.Blue` as it primary color - you'll note the primary button and the loading animation are both blue as a result. Let's change the defaults core arguments of our app. We'll overwrite the constructor and pass new defaults for the core constructor arguments.

We'll use `gr.themes.Emerald` as our primary color, and set secondary and neutral hues to `gr.themes.Blue`. We'll make our text larger using `text_lg`. We'll use `Quicksand` as our default font, loaded from Google Fonts.

$code_theme_new_step_2

<div class="wrapper">
<iframe
	src="https://gradio-theme-new-step-2.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

See how the primary button and the loading animation are now green? These CSS variables are tied to the `primary_hue` variable.

Let's modify the theme a bit more directly. We'll call the `set()` method to overwrite CSS variable values explicitly. We can use any CSS logic, and reference our core constructor arguments using the `*` prefix.

$code_theme_new_step_3

<div class="wrapper">
<iframe
	src="https://gradio-theme-new-step-3.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

Look how fun our theme looks now! With just a few variable changes, our theme looks completely different.

You may find it helpful to explore the [source code of the other prebuilt themes](https://github.com/gradio-app/gradio/blob/main/gradio/themes) to see how they modified the base theme. You can also find your browser's Inspector useful to select elements from the UI and see what CSS variables are being used in the styles panel.

## Sharing Themes

Once you have created a theme, you can upload it to the HuggingFace Hub to let others view it, use it, and build off of it!

### Uploading a Theme

There are two ways to upload a theme, via the theme class instance or the command line. We will cover both of them with the previously created `seafoam` theme.

- Via the class instance

Each theme instance has a method called `push_to_hub` we can use to upload a theme to the HuggingFace hub.

```python
seafoam.push_to_hub(repo_name="seafoam",
                    version="0.0.1",
					token="<token>")
```

- Via the command line

First save the theme to disk

```python
seafoam.dump(filename="seafoam.json")
```

Then use the `upload_theme` command:

```bash
upload_theme\
"seafoam.json"\
"seafoam"\
--version "0.0.1"\
--token "<token>"
```

In order to upload a theme, you must have a HuggingFace account and pass your [Access Token](https://huggingface.co/docs/huggingface_hub/quick-start#login)
as the `token` argument. However, if you log in via the [HuggingFace command line](https://huggingface.co/docs/huggingface_hub/quick-start#login) (which comes installed with `gradio`),
you can omit the `token` argument.

The `version` argument lets you specify a valid [semantic version](https://www.geeksforgeeks.org/introduction-semantic-versioning/) string for your theme.
That way your users are able to specify which version of your theme they want to use in their apps. This also lets you publish updates to your theme without worrying
about changing how previously created apps look. The `version` argument is optional. If omitted, the next patch version is automatically applied.

### Theme Previews

By calling `push_to_hub` or `upload_theme`, the theme assets will be stored in a [HuggingFace space](https://huggingface.co/docs/hub/spaces-overview).

For example, the theme preview for the calm seafoam theme is here: [calm seafoam preview](https://huggingface.co/spaces/shivalikasingh/calm_seafoam).

<div class="wrapper">
<iframe
	src="https://shivalikasingh-calm-seafoam.hf.space/?__theme=light"
	frameborder="0"
></iframe>
</div>

### Discovering Themes

The [Theme Gallery](https://huggingface.co/spaces/gradio/theme-gallery) shows all the public gradio themes. After publishing your theme,
it will automatically show up in the theme gallery after a couple of minutes.

You can sort the themes by the number of likes on the space and from most to least recently created as well as toggling themes between light and dark mode.

<div class="wrapper">
<iframe
	src="https://gradio-theme-gallery.static.hf.space"
	frameborder="0"
></iframe>
</div>

### Downloading

To use a theme from the hub, use the `from_hub` method on the `ThemeClass` and pass it to your app:

```python
my_theme = gr.Theme.from_hub("gradio/seafoam")

with gr.Blocks() as demo:
    ... # your code here
demo.launch(theme=my_theme)
```

You can also pass the theme string directly to the `launch()` method of `Blocks` or `Interface` (e.g. `demo.launch(theme="gradio/seafoam")`)

You can pin your app to an upstream theme version by using semantic versioning expressions.

For example, the following would ensure the theme we load from the `seafoam` repo was between versions `0.0.1` and `0.1.0`:

```python
with gr.Blocks() as demo:
    ... # your code here
demo.launch(theme="gradio/seafoam@>=0.0.1,<0.1.0")
    ....
```

Enjoy creating your own themes! If you make one you're proud of, please share it with the world by uploading it to the hub!
If you tag us on [Twitter](https://twitter.com/gradio) we can give your theme a shout out!

<style>
.wrapper {
    position: relative;
    padding-bottom: 56.25%;
    padding-top: 25px;
    height: 0;
}
.wrapper iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}
</style>

---

<!-- Source: guides/11_other-tutorials/understanding-gradio-share-links.md -->
# Share Links and Share Servers

You may already know that you can share any Gradio app that you build by setting `share=True` in the `.launch()` method. In other words, if you do:

```py
import gradio as gr

with gr.Blocks() as demo:
    ...

demo.launch(share=True)
```

This creates a publicly accessible **share link** (which looks like: `https://xxxxx.gradio.live`) to your Gradio application immediately, letting you share your app with anyone (while keeping the code and model running in your local environment). The link is created on Gradio's **share server**, which does not host your Gradio app, but instead creates a _tunnel_ to your locally-running Gradio app. 

This is particlarly useful when you are prototyping and want to get immediate feedback on your machine learning app, without having to deal with the hassle of hosting or deploying your application.

<video controls>
  <source src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/share-links.mov" type="video/mp4">
</video>

At any given time, more than 5,000 Gradio apps are being shared through share links. But how is this link created, and how can you create your own share server? Read on!

### Fast Reverse Proxy (FRP)

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/frp-gradio-diagram.svg)

Gradio share links are powered by Fast Reverse Proxy (FRP), an [open-source tunneling solution](https://github.com/huggingface/frp). Here's how it works:

When you create a Gradio app with `share=True`, the FRP Client is automatically downloaded to your local machine (if not already installed). This client establishes a secure TLS tunnel to Gradio's Share Server, which hosts the FRP Server component capable of handling thousands of simultaneous connections.

Once the tunnel is established, Gradio's Share Server exposes your locally-running application to the internet under a unique domain in the format `xxxxx.gradio.live`. This entire process happens in the background, when you launch a Gradio app with `share=True`.

Next, we'll dive deeper into both the FRP Client and FRP Server, as they are used in Gradio.

### FRP Client

We use a [modified version of the FRP Client](https://github.com/huggingface/frp/tree/tls/client), which runs on your machine. We package binaries for the most common operating systems, and the FRP Client for your system is downloaded the first time you create a share link on your machine.

**Code**:
* The complete Go code for the client can be found [in this directory](https://github.com/huggingface/frp/tree/tls/client).
* We use this [Make script](https://github.com/huggingface/frp/blob/tls/Makefile) to package the Go code into binaries for each operating system.

**Troubleshooting**: Some antivirus programs (notably Windows Defender) block the download of the FRP Client. In this case, you'll see a message with details on how to install the file manually, something like:

```
Could not create share link. Missing file: /Users/.../frpc_darwin_arm64_v0.3. 

Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: 

1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_darwin_arm64
2. Rename the downloaded file to: frpc_darwin_arm64_v0.3
3. Move the file to this location: /Users/...
```

If this does not work, you may need to [whitelist this file with your antivirus](https://www.makeuseof.com/how-to-whitelist-files-windows-defender/) in order to use the share links.

### FRP Server

Gradio runs a share server, which is a modified version of the FRP server. This server handles the public-facing side of the tunnel, receiving incoming connections from the internet and routing them to the appropriate FRP client running on your local machine.

The official Gradio share server is hosted at `gradio.live`, and we make our best effort to keep it running reliably at all times. This is the server that's used by default when you set `share=True` in your Gradio applications. You can check the current operational status of the official Gradio share server at [https://status.gradio.app/](https://status.gradio.app/). 

If you prefer, you can also host your own FRP server. This gives you complete control over the tunneling infrastructure and can be useful for enterprise deployments or situations where you need custom domains or additional security measures, or if you want to avoid the 72 hour timeout that is in place for links created through Gradio's official share server. Here are the instructions for running your own [Gradio Share Server](https://github.com/huggingface/frp?tab=readme-ov-file#why-run-your-own-share-server).


**Code**:
* The complete Go code for the client can be found [in this directory](https://github.com/huggingface/frp/tree/dev/server).
* The Dockerfile to launch [the FRP Server](https://github.com/huggingface/frp/blob/dev/dockerfiles/Dockerfile-for-frps) can be found here.

**Troubleshooting**: Gradio's Share Server may occasionally go down, despite our best effort to keep it running. If the [status page](https://status.gradio.app/) shows that the Gradio server is down, we'll work on fixing it, no need to create an issue!

---

<!-- Source: guides/11_other-tutorials/using-flagging.md -->
# Using Flagging

Related spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flagging-basic
Tags: FLAGGING, DATA

## Introduction

When you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these "hard" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.

Gradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.

## The **Flag** button in `gradio.Interface`

Flagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.

There are [four parameters](https://gradio.app/docs/interface#initialization) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.

- `flagging_mode`: this parameter can be set to either `"manual"` (default), `"auto"`, or `"never"`.
  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.
  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.
  - `never`: users will not see a button to flag, and no sample will be flagged.
- `flagging_options`: this parameter can be either `None` (default) or a list of strings.
  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.
  - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `["Incorrect", "Ambiguous"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `flagging_mode` is `"manual"`.
  - The chosen option is then logged along with the input and output.
- `flagging_dir`: this parameter takes a string.
  - It represents what to name the directory where flagged data is stored.
- `flagging_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class
  - Using this parameter allows you to write custom code that gets run when the flag button is clicked
  - By default, this is set to an instance of `gr.JSONLogger`

## What happens to flagged data?

Within the directory provided by the `flagging_dir` argument, a JSON file will log the flagged data.

Here's an example: The code below creates the calculator interface embedded below it:

```python
import gradio as gr


def calculator(num1, operation, num2):
    if operation == "add":
        return num1 + num2
    elif operation == "subtract":
        return num1 - num2
    elif operation == "multiply":
        return num1 * num2
    elif operation == "divide":
        return num1 / num2


iface = gr.Interface(
    calculator,
    ["number", gr.Radio(["add", "subtract", "multiply", "divide"]), "number"],
    "number",
    flagging_mode="manual"
)

iface.launch()
```

<gradio-app space="gradio/calculator-flagging-basic"></gradio-app>

When you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged.

```directory
+-- flagged/
|   +-- logs.csv
```

_flagged/logs.csv_

```csv
num1,operation,num2,Output,timestamp
5,add,7,12,2022-01-31 11:40:51.093412
6,subtract,1.5,4.5,2022-01-31 03:25:32.023542
```

If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure.

```directory
+-- flagged/
|   +-- logs.csv
|   +-- image/
|   |   +-- 0.png
|   |   +-- 1.png
|   +-- Output/
|   |   +-- 0.png
|   |   +-- 1.png
```

_flagged/logs.csv_

```csv
im,Output timestamp
im/0.png,Output/0.png,2022-02-04 19:49:58.026963
im/1.png,Output/1.png,2022-02-02 10:40:51.093412
```

If you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV.

If we go back to the calculator example, the following code will create the interface embedded below it.

```python
iface = gr.Interface(
    calculator,
    ["number", gr.Radio(["add", "subtract", "multiply", "divide"]), "number"],
    "number",
    flagging_mode="manual",
    flagging_options=["wrong sign", "off by one", "other"]
)

iface.launch()
```

<gradio-app space="gradio/calculator-flagging-options"></gradio-app>

When users click the flag button, the csv file will now include a column indicating the selected option.

_flagged/logs.csv_

```csv
num1,operation,num2,Output,flag,timestamp
5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412
6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512
```

## Flagging with Blocks

What about if you are using `gradio.Blocks`? On one hand, you have even more flexibility
with Blocks -- you can write whatever Python code you want to run when a button is clicked,
and assign that using the built-in events in Blocks.

At the same time, you might want to use an existing `FlaggingCallback` to avoid writing extra code.
This requires two steps:

1. You have to run your callback's `.setup()` somewhere in the code prior to the
   first time you flag data
2. When the flagging button is clicked, then you trigger the callback's `.flag()` method,
   making sure to collect the arguments correctly and disabling the typical preprocessing.

Here is an example with an image sepia filter Blocks demo that lets you flag
data using the default `CSVLogger`:

$code_blocks_flag
$demo_blocks_flag

## Privacy

Important Note: please make sure your users understand when the data they submit is being saved, and what you plan on doing with it. This is especially important when you use `flagging_mode=auto` (when all of the data submitted through the demo is being flagged)

### That's all! Happy building :)

---

<!-- Source: guides/11_other-tutorials/using-gradio-for-tabular-workflows.md -->
# Using Gradio for Tabular Data Science Workflows

Related spaces: https://huggingface.co/spaces/scikit-learn/gradio-skops-integration, https://huggingface.co/spaces/scikit-learn/tabular-playground, https://huggingface.co/spaces/merve/gradio-analysis-dashboard

## Introduction

Tabular data science is the most widely used domain of machine learning, with problems ranging from customer segmentation to churn prediction. Throughout various stages of the tabular data science workflow, communicating your work to stakeholders or clients can be cumbersome; which prevents data scientists from focusing on what matters, such as data analysis and model building. Data scientists can end up spending hours building a dashboard that takes in dataframe and returning plots, or returning a prediction or plot of clusters in a dataset. In this guide, we'll go through how to use `gradio` to improve your data science workflows. We will also talk about how to use `gradio` and [skops](https://skops.readthedocs.io/en/stable/) to build interfaces with only one line of code!

### Prerequisites

Make sure you have the `gradio` Python package already [installed](/getting_started).

## Let's Create a Simple Interface!

We will take a look at how we can create a simple UI that predicts failures based on product information.

```python
import gradio as gr
import pandas as pd
import joblib
import datasets


inputs = [gr.Dataframe(row_count = (2, "dynamic"), col_count=(4,"dynamic"), label="Input Data", interactive=1)]

outputs = [gr.Dataframe(row_count = (2, "dynamic"), col_count=(1, "fixed"), label="Predictions", headers=["Failures"])]

model = joblib.load("model.pkl")

# we will give our dataframe as example
df = datasets.load_dataset("merve/supersoaker-failures")
df = df["train"].to_pandas()

def infer(input_dataframe):
  return pd.DataFrame(model.predict(input_dataframe))

gr.Interface(fn = infer, inputs = inputs, outputs = outputs, examples = [[df.head(2)]]).launch()
```

Let's break down above code.

- `fn`: the inference function that takes input dataframe and returns predictions.
- `inputs`: the component we take our input with. We define our input as dataframe with 2 rows and 4 columns, which initially will look like an empty dataframe with the aforementioned shape. When the `row_count` is set to `dynamic`, you don't have to rely on the dataset you're inputting to pre-defined component.
- `outputs`: The dataframe component that stores outputs. This UI can take single or multiple samples to infer, and returns 0 or 1 for each sample in one column, so we give `row_count` as 2 and `col_count` as 1 above. `headers` is a list made of header names for dataframe.
- `examples`: You can either pass the input by dragging and dropping a CSV file, or a pandas DataFrame through examples, which headers will be automatically taken by the interface.

We will now create an example for a minimal data visualization dashboard. You can find a more comprehensive version in the related Spaces.

<gradio-app space="gradio/tabular-playground"></gradio-app>

```python
import gradio as gr
import pandas as pd
import datasets
import seaborn as sns
import matplotlib.pyplot as plt

df = datasets.load_dataset("merve/supersoaker-failures")
df = df["train"].to_pandas()
df.dropna(axis=0, inplace=True)

def plot(df):
  plt.scatter(df.measurement_13, df.measurement_15, c = df.loading,alpha=0.5)
  plt.savefig("scatter.png")
  df['failure'].value_counts().plot(kind='bar')
  plt.savefig("bar.png")
  sns.heatmap(df.select_dtypes(include="number").corr())
  plt.savefig("corr.png")
  plots = ["corr.png","scatter.png", "bar.png"]
  return plots

inputs = [gr.Dataframe(label="Supersoaker Production Data")]
outputs = [gr.Gallery(label="Profiling Dashboard", columns=(1,3))]

gr.Interface(plot, inputs=inputs, outputs=outputs, examples=[df.head(100)], title="Supersoaker Failures Analysis Dashboard").launch()
```

<gradio-app space="gradio/gradio-analysis-dashboard-minimal"></gradio-app>

We will use the same dataset we used to train our model, but we will make a dashboard to visualize it this time.

- `fn`: The function that will create plots based on data.
- `inputs`: We use the same `Dataframe` component we used above.
- `outputs`: The `Gallery` component is used to keep our visualizations.
- `examples`: We will have the dataset itself as the example.

## Easily load tabular data interfaces with one line of code using skops

`skops` is a library built on top of `huggingface_hub` and `sklearn`. With the recent `gradio` integration of `skops`, you can build tabular data interfaces with one line of code!

```python
import gradio as gr

# title and description are optional
title = "Supersoaker Defective Product Prediction"
description = "This model predicts Supersoaker production line failures. Drag and drop any slice from dataset or edit values as you wish in below dataframe component."

gr.load("huggingface/scikit-learn/tabular-playground", title=title, description=description).launch()
```

<gradio-app space="gradio/gradio-skops-integration"></gradio-app>

`sklearn` models pushed to Hugging Face Hub using `skops` include a `config.json` file that contains an example input with column names, the task being solved (that can either be `tabular-classification` or `tabular-regression`). From the task type, `gradio` constructs the `Interface` and consumes column names and the example input to build it. You can [refer to skops documentation on hosting models on Hub](https://skops.readthedocs.io/en/latest/auto_examples/plot_hf_hub.html#sphx-glr-auto-examples-plot-hf-hub-py) to learn how to push your models to Hub using `skops`.

---

<!-- Source: guides/11_other-tutorials/using-gradio-in-other-programming-languages.md -->
# Using Gradio in Other Programming Languages

The core `gradio` library is a Python library. But you can also use `gradio` to create UIs around programs written in other languages, thanks to Python's ability to interface with external processes. Using Python's `subprocess` module, you can call programs written in C++, Rust, or virtually any other language, allowing `gradio` to become a flexible UI layer for non-Python applications.

In this post, we'll walk through how to integrate `gradio` with C++ and Rust, using Python's `subprocess` module to invoke code written in these languages. We'll also discuss how to use Gradio with R, which is even easier, thanks to the [reticulate](https://rstudio.github.io/reticulate/) R package, which makes it possible to install and import Python modules in R.

## Using Gradio with C++

Letâ€™s start with a simple example of integrating a C++ program into a Gradio app. Suppose we have the following C++ program that adds two numbers:

```cpp
// add.cpp
#include <iostream>

int main() {
    double a, b;
    std::cin >> a >> b;
    std::cout << a + b << std::endl;
    return 0;
}
```

This program reads two numbers from standard input, adds them, and outputs the result.

We can build a Gradio interface around this C++ program using Python's `subprocess` module. Hereâ€™s the corresponding Python code:

```python
import gradio as gr
import subprocess

def add_numbers(a, b):
    process = subprocess.Popen(
        ['./add'], 
        stdin=subprocess.PIPE, 
        stdout=subprocess.PIPE, 
        stderr=subprocess.PIPE
    )
    output, error = process.communicate(input=f"{a} {b}\n".encode())
    
    if error:
        return f"Error: {error.decode()}"
    return float(output.decode().strip())

demo = gr.Interface(
    fn=add_numbers, 
    inputs=[gr.Number(label="Number 1"), gr.Number(label="Number 2")], 
    outputs=gr.Textbox(label="Result")
)

demo.launch()
```

Here, `subprocess.Popen` is used to execute the compiled C++ program (`add`), pass the input values, and capture the output. You can compile the C++ program by running:

```bash
g++ -o add add.cpp
```

This example shows how easy it is to call C++ from Python using `subprocess` and build a Gradio interface around it.

## Using Gradio with Rust

Now, letâ€™s move to another example: calling a Rust program to apply a sepia filter to an image. The Rust code could look something like this:

```rust
// sepia.rs
extern crate image;

use image::{GenericImageView, ImageBuffer, Rgba};

fn sepia_filter(input: &str, output: &str) {
    let img = image::open(input).unwrap();
    let (width, height) = img.dimensions();
    let mut img_buf = ImageBuffer::new(width, height);

    for (x, y, pixel) in img.pixels() {
        let (r, g, b, a) = (pixel[0] as f32, pixel[1] as f32, pixel[2] as f32, pixel[3]);
        let tr = (0.393 * r + 0.769 * g + 0.189 * b).min(255.0);
        let tg = (0.349 * r + 0.686 * g + 0.168 * b).min(255.0);
        let tb = (0.272 * r + 0.534 * g + 0.131 * b).min(255.0);
        img_buf.put_pixel(x, y, Rgba([tr as u8, tg as u8, tb as u8, a]));
    }

    img_buf.save(output).unwrap();
}

fn main() {
    let args: Vec<String> = std::env::args().collect();
    if args.len() != 3 {
        eprintln!("Usage: sepia <input_file> <output_file>");
        return;
    }
    sepia_filter(&args[1], &args[2]);
}
```

This Rust program applies a sepia filter to an image. It takes two command-line arguments: the input image path and the output image path. You can compile this program using:

```bash
cargo build --release
```

Now, we can call this Rust program from Python and use Gradio to build the interface:

```python
import gradio as gr
import subprocess

def apply_sepia(input_path):
    output_path = "output.png"
    
    process = subprocess.Popen(
        ['./target/release/sepia', input_path, output_path], 
        stdout=subprocess.PIPE, 
        stderr=subprocess.PIPE
    )
    process.wait()
    
    return output_path

demo = gr.Interface(
    fn=apply_sepia, 
    inputs=gr.Image(type="filepath", label="Input Image"), 
    outputs=gr.Image(label="Sepia Image")
)

demo.launch()
```

Here, when a user uploads an image and clicks submit, Gradio calls the Rust binary (`sepia`) to process the image, and returns the sepia-filtered output to Gradio.

This setup showcases how you can integrate performance-critical or specialized code written in Rust into a Gradio interface.

## Using Gradio with R (via `reticulate`)

Integrating Gradio with R is particularly straightforward thanks to the `reticulate` package, which allows you to run Python code directly in R. Letâ€™s walk through an example of using Gradio in R. 

**Installation**

First, you need to install the `reticulate` package in R:

```r
install.packages("reticulate")
```


Once installed, you can use the package to run Gradio directly from within an R script.


```r
library(reticulate)

py_install("gradio", pip = TRUE)

gr <- import("gradio") # import gradio as gr
```

**Building a Gradio Application**

With gradio installed and imported, we now have access to gradio's app building methods. Let's build a simple app for an R function that returns a greeting

```r
greeting <- \(name) paste("Hello", name)

app <- gr$Interface(
  fn = greeting,
  inputs = gr$Text(label = "Name"),
  outputs = gr$Text(label = "Greeting"),
  title = "Hello! &#128515 &#128075"
)

app$launch(server_name = "localhost", 
           server_port = as.integer(3000))
```

Credit to [@IfeanyiIdiaye](https://github.com/Ifeanyi55) for contributing this section. You can see more examples [here](https://github.com/Ifeanyi55/Gradio-in-R/tree/main/Code), including using Gradio Blocks to build a machine learning application in R.

---

<!-- Source: guides/11_other-tutorials/wrapping-layouts.md -->
# Wrapping Layouts

Tags: LAYOUTS

## Introduction

Gradio features [blocks](https://www.gradio.app/docs/blocks) to easily layout applications. To use this feature, you need to stack or nest layout components and create a hierarchy with them. This isn't difficult to implement and maintain for small projects, but after the project gets more complex, this component hierarchy becomes difficult to maintain and reuse.

In this guide, we are going to explore how we can wrap the layout classes to create more maintainable and easy-to-read applications without sacrificing flexibility.

## Example

We are going to follow the implementation from this Huggingface Space example:

<gradio-app
space="gradio/wrapping-layouts">
</gradio-app>

## Implementation

The wrapping utility has two important classes. The first one is the ```LayoutBase``` class and the other one is the ```Application``` class.

We are going to look at the ```render``` and ```attach_event``` functions of them for brevity. You can look at the full implementation from [the example code](https://huggingface.co/spaces/WoWoWoWololo/wrapping-layouts/blob/main/app.py).

So let's start with the ```LayoutBase``` class.

### LayoutBase Class

1. Render Function

    Let's look at the ```render``` function in the ```LayoutBase``` class:

```python
# other LayoutBase implementations

def render(self) -> None:
    with self.main_layout:
        for renderable in self.renderables:
            renderable.render()

    self.main_layout.render()
```
This is a little confusing at first but if you consider the default implementation you can understand it easily.
Let's look at an example:

In the default implementation, this is what we're doing:

```python
with Row():
    left_textbox = Textbox(value="left_textbox")
    right_textbox = Textbox(value="right_textbox")
```

Now, pay attention to the Textbox variables. These variables' ```render``` parameter is true by default. So as we use the ```with``` syntax and create these variables, they are calling the ```render``` function under the ```with``` syntax.

We know the render function is called in the constructor with the implementation from the ```gradio.blocks.Block``` class:

```python
class Block:
    # constructor parameters are omitted for brevity
    def __init__(self, ...):
        # other assign functions 

        if render:
            self.render()
```

So our implementation looks like this:

```python
# self.main_layout -> Row()
with self.main_layout:
    left_textbox.render()
    right_textbox.render()
```

What this means is by calling the components' render functions under the ```with``` syntax, we are actually simulating the default implementation.

So now let's consider two nested ```with```s to see how the outer one works. For this, let's expand our example with the ```Tab``` component:

```python
with Tab():
    with Row():
        first_textbox = Textbox(value="first_textbox")
        second_textbox = Textbox(value="second_textbox")
```

Pay attention to the Row and Tab components this time. We have created the Textbox variables above and added them to Row with the ```with``` syntax. Now we need to add the Row component to the Tab component. You can see that the Row component is created with default parameters, so its render parameter is true, that's why the render function is going to be executed under the Tab component's ```with``` syntax.

To mimic this implementation, we need to call the ```render``` function of the ```main_layout``` variable after the ```with``` syntax of the ```main_layout``` variable.

So the implementation looks like this:

```python
with tab_main_layout:
    with row_main_layout:
        first_textbox.render()
        second_textbox.render()

    row_main_layout.render()

tab_main_layout.render()
```

The default implementation and our implementation are the same, but we are using the render function ourselves. So it requires a little work.

Now, let's take a look at the ```attach_event``` function.

2. Attach Event Function

    The function is left as not implemented because it is specific to the class, so each class has to implement its `attach_event` function.

```python
    # other LayoutBase implementations

    def attach_event(self, block_dict: Dict[str, Block]) -> None:
        raise NotImplementedError
```

Check out the ```block_dict``` variable in the ```Application``` class's ```attach_event``` function.

### Application Class

1. Render Function

```python
    # other Application implementations

    def _render(self):
        with self.app:
            for child in self.children:
                child.render()

        self.app.render()
```

From the explanation of the ```LayoutBase``` class's ```render``` function, we can understand the ```child.render``` part.

So let's look at the bottom part, why are we calling the ```app``` variable's ```render``` function? It's important to call this function because if we look at the implementation in the ```gradio.blocks.Blocks``` class, we can see that it is adding the components and event functions into the root component. To put it another way, it is creating and structuring the gradio application.

2. Attach Event Function

    Let's see how we can attach events to components:

```python
    # other Application implementations

    def _attach_event(self):
        block_dict: Dict[str, Block] = {}

        for child in self.children:
            block_dict.update(child.global_children_dict)

        with self.app:
            for child in self.children:
                try:
                    child.attach_event(block_dict=block_dict)
                except NotImplementedError:
                    print(f"{child.name}'s attach_event is not implemented")
```

You can see why the ```global_children_list``` is used in the ```LayoutBase``` class from the example code. With this, all the components in the application are gathered into one dictionary, so the component can access all the components with their names.

The ```with``` syntax is used here again to attach events to components. If we look at the ```__exit__``` function in the ```gradio.blocks.Blocks``` class, we can see that it is calling the ```attach_load_events``` function which is used for setting event triggers to components. So we have to use the ```with``` syntax to trigger the ```__exit__``` function.

Of course, we can call ```attach_load_events``` without using the ```with``` syntax, but the function needs a ```Context.root_block```, and it is set in the ```__enter__``` function. So we used the ```with``` syntax here rather than calling the function ourselves.

## Conclusion

In this guide, we saw

- How we can wrap the layouts
- How components are rendered
- How we can structure our application with wrapped layout classes

Because the classes used in this guide are used for demonstration purposes, they may still not be totally optimized or modular. But that would make the guide much longer!

I hope this guide helps you gain another view of the layout classes and gives you an idea about how you can use them for your needs. See the full implementation of our example [here](https://huggingface.co/spaces/WoWoWoWololo/wrapping-layouts/blob/main/app.py).

---

<!-- Source: guides/cn/04_integrating-other-frameworks/Gradio-and-Comet.md -->
# ä½¿ç”¨ Gradio å’Œ Comet

Tags: COMET, SPACES
ç”± Comet å›¢é˜Ÿè´¡çŒ®

## ä»‹ç»

åœ¨è¿™ä¸ªæŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºæ‚¨å¯ä»¥å¦‚ä½•ä½¿ç”¨ Gradio å’Œ Cometã€‚æˆ‘ä»¬å°†ä»‹ç»ä½¿ç”¨ Comet å’Œ Gradio çš„åŸºæœ¬çŸ¥è¯†ï¼Œå¹¶å‘æ‚¨å±•ç¤ºå¦‚ä½•åˆ©ç”¨ Gradio çš„é«˜çº§åŠŸèƒ½ï¼Œå¦‚ [ä½¿ç”¨ iFrames è¿›è¡ŒåµŒå…¥](https://www.gradio.app/sharing-your-app/#embedding-with-iframes) å’Œ [çŠ¶æ€](https://www.gradio.app/docs/#state) æ¥æž„å»ºä¸€äº›ä»¤äººæƒŠå¹çš„æ¨¡åž‹è¯„ä¼°å·¥ä½œæµç¨‹ã€‚

ä¸‹é¢æ˜¯æœ¬æŒ‡å—æ¶µç›–çš„ä¸»é¢˜åˆ—è¡¨ã€‚

1. å°† Gradio UI è®°å½•åˆ°æ‚¨çš„ Comet å®žéªŒä¸­
2. ç›´æŽ¥å°† Gradio åº”ç”¨ç¨‹åºåµŒå…¥åˆ°æ‚¨çš„ Comet é¡¹ç›®ä¸­
3. ç›´æŽ¥å°† Hugging Face Spaces åµŒå…¥åˆ°æ‚¨çš„ Comet é¡¹ç›®ä¸­
4. å°† Gradio åº”ç”¨ç¨‹åºçš„æ¨¡åž‹æŽ¨ç†è®°å½•åˆ° Comet ä¸­

## ä»€ä¹ˆæ˜¯ Cometï¼Ÿ

[Comet](https://www.comet.com?utm_source=gradio&utm_medium=referral&utm_campaign=gradio-integration&utm_content=gradio-docs) æ˜¯ä¸€ä¸ª MLOps å¹³å°ï¼Œæ—¨åœ¨å¸®åŠ©æ•°æ®ç§‘å­¦å®¶å’Œå›¢é˜Ÿæ›´å¿«åœ°æž„å»ºæ›´å¥½çš„æ¨¡åž‹ï¼Comet æä¾›å·¥å…·æ¥è·Ÿè¸ªã€è§£é‡Šã€ç®¡ç†å’Œç›‘æŽ§æ‚¨çš„æ¨¡åž‹ï¼Œé›†ä¸­åœ¨ä¸€ä¸ªåœ°æ–¹ï¼å®ƒå¯ä»¥ä¸Ž Jupyter ç¬”è®°æœ¬å’Œè„šæœ¬é…åˆä½¿ç”¨ï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œå®ƒæ˜¯ 100% å…è´¹çš„ï¼

## è®¾ç½®

é¦–å…ˆï¼Œå®‰è£…è¿è¡Œè¿™äº›ç¤ºä¾‹æ‰€éœ€çš„ä¾èµ–é¡¹

```shell
pip install comet_ml torch torchvision transformers gradio shap requests Pillow
```

æŽ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦[æ³¨å†Œä¸€ä¸ª Comet è´¦æˆ·](https://www.comet.com/signup?utm_source=gradio&utm_medium=referral&utm_campaign=gradio-integration&utm_content=gradio-docs)ã€‚ä¸€æ—¦æ‚¨è®¾ç½®äº†æ‚¨çš„è´¦æˆ·ï¼Œ[èŽ·å–æ‚¨çš„ API å¯†é’¥](https://www.comet.com/docs/v2/guides/getting-started/quickstart/#get-an-api-key?utm_source=gradio&utm_medium=referral&utm_campaign=gradio-integration&utm_content=gradio-docs) å¹¶é…ç½®æ‚¨çš„ Comet å‡­æ®

å¦‚æžœæ‚¨å°†è¿™äº›ç¤ºä¾‹ä½œä¸ºè„šæœ¬è¿è¡Œï¼Œæ‚¨å¯ä»¥å°†æ‚¨çš„å‡­æ®å¯¼å‡ºä¸ºçŽ¯å¢ƒå˜é‡

```shell
export COMET_API_KEY="<æ‚¨çš„ API å¯†é’¥>"
export COMET_WORKSPACE="<æ‚¨çš„å·¥ä½œç©ºé—´åç§°>"
export COMET_PROJECT_NAME="<æ‚¨çš„é¡¹ç›®åç§°>"
```

æˆ–è€…å°†å®ƒä»¬è®¾ç½®åœ¨æ‚¨çš„å·¥ä½œç›®å½•ä¸­çš„ `.comet.config` æ–‡ä»¶ä¸­ã€‚æ‚¨çš„æ–‡ä»¶åº”æŒ‰ä»¥ä¸‹æ–¹å¼æ ¼å¼åŒ–ã€‚

```shell
[comet]
api_key=<æ‚¨çš„ API å¯†é’¥>
workspace=<æ‚¨çš„å·¥ä½œç©ºé—´åç§°>
project_name=<æ‚¨çš„é¡¹ç›®åç§°>
```

å¦‚æžœæ‚¨ä½¿ç”¨æä¾›çš„ Colab Notebooks è¿è¡Œè¿™äº›ç¤ºä¾‹ï¼Œè¯·åœ¨å¼€å§‹ Gradio UI ä¹‹å‰è¿è¡Œå¸¦æœ‰ä»¥ä¸‹ç‰‡æ®µçš„å•å…ƒæ ¼ã€‚è¿è¡Œæ­¤å•å…ƒæ ¼å¯ä»¥è®©æ‚¨äº¤äº’å¼åœ°å°† API å¯†é’¥æ·»åŠ åˆ°ç¬”è®°æœ¬ä¸­ã€‚

```python
import comet_ml
comet_ml.init()
```

## 1. å°† Gradio UI è®°å½•åˆ°æ‚¨çš„ Comet å®žéªŒä¸­

[![åœ¨ Colab ä¸­æ‰“å¼€](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/comet-examples/blob/master/integrations/model-evaluation/gradio/notebooks/Gradio_and_Comet.ipynb)

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•å°†æ‚¨çš„ Gradio åº”ç”¨ç¨‹åºè®°å½•åˆ° Cometï¼Œå¹¶ä½¿ç”¨ Gradio è‡ªå®šä¹‰é¢æ¿ä¸Žå…¶è¿›è¡Œäº¤äº’ã€‚

æˆ‘ä»¬å…ˆé€šè¿‡ä½¿ç”¨ `resnet18` æž„å»ºä¸€ä¸ªç®€å•çš„å›¾åƒåˆ†ç±»ç¤ºä¾‹ã€‚

```python
import comet_ml

import requests
import torch
from PIL import Image
from torchvision import transforms

torch.hub.download_url_to_file("https://github.com/pytorch/hub/raw/master/images/dog.jpg", "dog.jpg")

if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

model = torch.hub.load("pytorch/vision:v0.6.0", "resnet18", pretrained=True).eval()
model = model.to(device)

# ä¸º ImageNet ä¸‹è½½å¯è¯»çš„æ ‡ç­¾ã€‚
response = requests.get("https://git.io/JJkYN")
labels = response.text.split("\n")


def predict(inp):
    inp = Image.fromarray(inp.astype("uint8"), "RGB")
    inp = transforms.ToTensor()(inp).unsqueeze(0)
    with torch.no_grad():
        prediction = torch.nn.functional.softmax(model(inp.to(device))[0], dim=0)
    return {labels[i]: float(prediction[i]) for i in range(1000)}


inputs = gr.Image()
outputs = gr.Label(num_top_classes=3)

io = gr.Interface(
    fn=predict, inputs=inputs, outputs=outputs, examples=["dog.jpg"]
)
io.launch(inline=False, share=True)

experiment = comet_ml.Experiment()
experiment.add_tag("image-classifier")

io.integrate(comet_ml=experiment)
```

æ­¤ç‰‡æ®µä¸­çš„æœ€åŽä¸€è¡Œå°†å°† Gradio åº”ç”¨ç¨‹åºçš„ URL è®°å½•åˆ°æ‚¨çš„ Comet å®žéªŒä¸­ã€‚æ‚¨å¯ä»¥åœ¨å®žéªŒçš„æ–‡æœ¬é€‰é¡¹å¡ä¸­æ‰¾åˆ°è¯¥ URLã€‚

<video width="560" height="315" controls>
    <source src="https://user-images.githubusercontent.com/7529846/214328034-09369d4d-8b94-4c4a-aa3c-25e3ed8394c4.mp4"></source>
</video>

å°† Gradio é¢æ¿æ·»åŠ åˆ°æ‚¨çš„å®žéªŒä¸­ï¼Œä¸Žåº”ç”¨ç¨‹åºè¿›è¡Œäº¤äº’ã€‚

<video width="560" height="315" controls>
    <source src="https://user-images.githubusercontent.com/7529846/214328194-95987f83-c180-4929-9bed-c8a0d3563ed7.mp4"></source>
</video>

## 2. ç›´æŽ¥å°† Gradio åº”ç”¨ç¨‹åºåµŒå…¥åˆ°æ‚¨çš„ Comet é¡¹ç›®ä¸­

<iframe width="560" height="315" src="https://www.youtube.com/embed/KZnpH7msPq0?start=9" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

å¦‚æžœæ‚¨è¦é•¿æœŸæ‰˜ç®¡ Gradio åº”ç”¨ç¨‹åºï¼Œå¯ä»¥ä½¿ç”¨ Gradio Panel Extended è‡ªå®šä¹‰é¢æ¿è¿›è¡ŒåµŒå…¥ UIã€‚

è½¬åˆ°æ‚¨çš„ Comet é¡¹ç›®é¡µé¢ï¼Œè½¬åˆ°é¢æ¿é€‰é¡¹å¡ã€‚å•å‡»â€œ+ æ·»åŠ â€æŒ‰é’®ä»¥æ‰“å¼€é¢æ¿æœç´¢é¡µé¢ã€‚

<img width="560" alt="adding-panels" src="https://user-images.githubusercontent.com/7529846/214329314-70a3ff3d-27fb-408c-a4d1-4b58892a3854.jpeg">

æŽ¥ä¸‹æ¥ï¼Œåœ¨å…¬å…±é¢æ¿éƒ¨åˆ†æœç´¢ Gradio Panel Extended å¹¶å•å‡»â€œæ·»åŠ â€ã€‚

<img width="560" alt="gradio-panel-extended" src="https://user-images.githubusercontent.com/7529846/214325577-43226119-0292-46be-a62a-0c7a80646ebb.png">

æ·»åŠ é¢æ¿åŽï¼Œå•å‡»â€œç¼–è¾‘â€ä»¥è®¿é—®é¢æ¿é€‰é¡¹é¡µé¢ï¼Œå¹¶ç²˜è´´æ‚¨çš„ Gradio åº”ç”¨ç¨‹åºçš„ URLã€‚

![Edit-Gradio-Panel-Options](https://user-images.githubusercontent.com/7529846/214573001-23814b5a-ca65-4ace-a8a5-b27cdda70f7a.gif)

<img width="560" alt="Edit-Gradio-Panel-URL" src="https://user-images.githubusercontent.com/7529846/214334843-870fe726-0aa1-4b21-bbc6-0c48f56c48d8.png">

## 3. ç›´æŽ¥å°† Hugging Face Spaces åµŒå…¥åˆ°æ‚¨çš„ Comet é¡¹ç›®ä¸­

<iframe width="560" height="315" src="https://www.youtube.com/embed/KZnpH7msPq0?start=107" title="YouTube è§†é¢‘æ’­æ”¾å™¨ " frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ Hugging Face Spaces é¢æ¿å°†æ‰˜ç®¡åœ¨ Hugging Faces Spaces ä¸­çš„ Gradio åº”ç”¨ç¨‹åºåµŒå…¥åˆ°æ‚¨çš„ Comet é¡¹ç›®ä¸­ã€‚

è½¬åˆ° Comet é¡¹ç›®é¡µé¢ï¼Œè½¬åˆ°é¢æ¿é€‰é¡¹å¡ã€‚å•å‡»â€œ+æ·»åŠ â€æŒ‰é’®ä»¥æ‰“å¼€é¢æ¿æœç´¢é¡µé¢ã€‚ç„¶åŽï¼Œåœ¨å…¬å…±é¢æ¿éƒ¨åˆ†æœç´¢ Hugging Face Spaces é¢æ¿å¹¶å•å‡»â€œæ·»åŠ â€ã€‚

<img width="560" height="315" alt="huggingface-spaces-panel" src="https://user-images.githubusercontent.com/7529846/214325606-99aa3af3-b284-4026-b423-d3d238797e12.png">

æ·»åŠ é¢æ¿åŽï¼Œå•å‡»â€œç¼–è¾‘â€ä»¥è®¿é—®é¢æ¿é€‰é¡¹é¡µé¢ï¼Œå¹¶ç²˜è´´æ‚¨çš„ Hugging Face Space è·¯å¾„ï¼Œä¾‹å¦‚ `pytorch/ResNet`

<img width="560" height="315" alt="Edit-HF-Space" src="https://user-images.githubusercontent.com/7529846/214335868-c6f25dee-13db-4388-bcf5-65194f850b02.png">

## 4. è®°å½•æ¨¡åž‹æŽ¨æ–­ç»“æžœåˆ° Comet

<iframe width="560" height="315" src="https://www.youtube.com/embed/KZnpH7msPq0?start=176" title="YouTube è§†é¢‘æ’­æ”¾å™¨ " frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

[![åœ¨ Colab ä¸­æ‰“å¼€](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/comet-examples/blob/master/integrations/model-evaluation/gradio/notebooks/Logging_Model_Inferences_with_Comet_and_Gradio.ipynb)

åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æ¼”ç¤ºäº†é€šè¿‡ Comet UI ä¸Ž Gradio åº”ç”¨ç¨‹åºäº¤äº’çš„å„ç§æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ‚¨è¿˜å¯ä»¥å°† Gradio åº”ç”¨ç¨‹åºçš„æ¨¡åž‹æŽ¨æ–­ï¼ˆä¾‹å¦‚ SHAP å›¾ï¼‰è®°å½•åˆ° Comet ä¸­ã€‚

åœ¨ä»¥ä¸‹ä»£ç æ®µä¸­ï¼Œæˆ‘ä»¬å°†è®°å½•æ¥è‡ªæ–‡æœ¬ç”Ÿæˆæ¨¡åž‹çš„æŽ¨æ–­ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Gradio çš„[State](https://www.gradio.app/docs/#state)å¯¹è±¡åœ¨å¤šæ¬¡æŽ¨æ–­è°ƒç”¨ä¹‹é—´ä¿æŒå®žéªŒçš„æŒä¹…æ€§ã€‚è¿™å°†ä½¿æ‚¨èƒ½å¤Ÿå°†å¤šä¸ªæ¨¡åž‹æŽ¨æ–­è®°å½•åˆ°å•ä¸ªå®žéªŒä¸­ã€‚

```python
import comet_ml
import gradio as gr
import shap
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

MODEL_NAME = "gpt2"

model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

# set model decoder to true
model.config.is_decoder = True
# set text-generation params under task_specific_params
model.config.task_specific_params["text-generation"] = {
    "do_sample": True,
    "max_length": 50,
    "temperature": 0.7,
    "top_k": 50,
    "no_repeat_ngram_size": 2,
}
model = model.to(device)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
explainer = shap.Explainer(model, tokenizer)


def start_experiment():
    """Returns an APIExperiment object that is thread safe
    and can be used to log inferences to a single Experiment
    """
    try:
        api = comet_ml.API()
        workspace = api.get_default_workspace()
        project_name = comet_ml.config.get_config()["comet.project_name"]

        experiment = comet_ml.APIExperiment(
            workspace=workspace, project_name=project_name
        )
        experiment.log_other("Created from", "gradio-inference")

        message = f"Started Experiment: [{experiment.name}]({experiment.url})"
        return (experiment, message)

    except Exception as e:
        return None, None


def predict(text, state, message):
    experiment = state

    shap_values = explainer([text])
    plot = shap.plots.text(shap_values, display=False)

    if experiment is not None:
        experiment.log_other("message", message)
        experiment.log_html(plot)

    return plot


with gr.Blocks() as demo:
    start_experiment_btn = gr.Button("Start New Experiment")
    experiment_status = gr.Markdown()

    # Log a message to the Experiment to provide more context
    experiment_message = gr.Textbox(label="Experiment Message")
    experiment = gr.State()

    input_text = gr.Textbox(label="Input Text", lines=5, interactive=True)
    submit_btn = gr.Button("Submit")

    output = gr.HTML(interactive=True)

    start_experiment_btn.click(
        start_experiment, outputs=[experiment, experiment_status]
    )
    submit_btn.click(
        predict, inputs=[input_text, experiment, experiment_message], outputs=[output]
    )
```

è¯¥ä»£ç æ®µä¸­çš„æŽ¨æ–­ç»“æžœå°†ä¿å­˜åœ¨å®žéªŒçš„ HTML é€‰é¡¹å¡ä¸­ã€‚

<video width="560" height="315" controls>
    <source src="https://user-images.githubusercontent.com/7529846/214328610-466e5c81-4814-49b9-887c-065aca14dd30.mp4"></source>
</video>

## ç»“è®º

å¸Œæœ›æ‚¨å¯¹æœ¬æŒ‡å—æœ‰æ‰€è£¨ç›Šï¼Œå¹¶èƒ½ä¸ºæ‚¨æž„å»ºå‡ºè‰²çš„ Comet å’Œ Gradio æ¨¡åž‹è¯„ä¼°å·¥ä½œæµç¨‹æä¾›ä¸€äº›å¯ç¤ºã€‚

## å¦‚ä½•åœ¨ Comet ç»„ç»‡ä¸Šè´¡çŒ® Gradio æ¼”ç¤º

- åœ¨ Hugging Face ä¸Šåˆ›å»ºå¸å·[æ­¤å¤„](https://huggingface.co/join)ã€‚
- åœ¨ç”¨æˆ·åä¸‹æ·»åŠ  Gradio æ¼”ç¤ºï¼Œè¯·å‚é˜…[æ­¤å¤„](https://huggingface.co/course/chapter9/4?fw=pt)ä»¥è®¾ç½® Gradio æ¼”ç¤ºã€‚
- è¯·æ±‚åŠ å…¥ Comet ç»„ç»‡[æ­¤å¤„](https://huggingface.co/Comet)ã€‚

## æ›´å¤šèµ„æº

- [Comet æ–‡æ¡£](https://www.comet.com/docs/v2/?utm_source=gradio&utm_medium=referral&utm_campaign=gradio-integration&utm_content=gradio-docs)

---

<!-- Source: guides/cn/04_integrating-other-frameworks/Gradio-and-ONNX-on-Hugging-Face.md -->
# Gradio å’Œ ONNX åœ¨ Hugging Face ä¸Š

Related spaces: https://huggingface.co/spaces/onnx/EfficientNet-Lite4
Tags: ONNXï¼ŒSPACES
ç”± Gradio å’Œ <a href="https://onnx.ai/">ONNX</a> å›¢é˜Ÿè´¡çŒ®

## ä»‹ç»

åœ¨è¿™ä¸ªæŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ä¸ºæ‚¨ä»‹ç»ä»¥ä¸‹å†…å®¹ï¼š

- ONNXã€ONNX æ¨¡åž‹ä»“åº“ã€Gradio å’Œ Hugging Face Spaces çš„ä»‹ç»
- å¦‚ä½•ä¸º EfficientNet-Lite4 è®¾ç½® Gradio æ¼”ç¤º
- å¦‚ä½•ä¸º Hugging Face ä¸Šçš„ ONNX ç»„ç»‡è´¡çŒ®è‡ªå·±çš„ Gradio æ¼”ç¤º

ä¸‹é¢æ˜¯ä¸€ä¸ª ONNX æ¨¡åž‹çš„ç¤ºä¾‹ï¼šåœ¨ä¸‹é¢å°è¯• EfficientNet-Lite4 æ¼”ç¤ºã€‚

<iframe src="https://onnx-efficientnet-lite4.hf.space" frameBorder="0" height="810" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

## ONNX æ¨¡åž‹ä»“åº“æ˜¯ä»€ä¹ˆï¼Ÿ

Open Neural Network Exchangeï¼ˆ[ONNX](https://onnx.ai/)ï¼‰æ˜¯ä¸€ç§è¡¨ç¤ºæœºå™¨å­¦ä¹ æ¨¡åž‹çš„å¼€æ”¾æ ‡å‡†æ ¼å¼ã€‚ONNX ç”±ä¸€ä¸ªå®žçŽ°äº†è¯¥æ ¼å¼çš„åˆä½œä¼™ä¼´ç¤¾åŒºæ”¯æŒï¼Œè¯¥ç¤¾åŒºå°†å…¶å®žæ–½åˆ°è®¸å¤šæ¡†æž¶å’Œå·¥å…·ä¸­ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨åœ¨ TensorFlow æˆ– PyTorch ä¸­è®­ç»ƒäº†ä¸€ä¸ªæ¨¡åž‹ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å°†å…¶è½¬æ¢ä¸º ONNXï¼Œç„¶åŽä½¿ç”¨ç±»ä¼¼ ONNX Runtime çš„å¼•æ“Ž / ç¼–è¯‘å™¨åœ¨å„ç§è®¾å¤‡ä¸Šè¿è¡Œå®ƒã€‚

[ONNX æ¨¡åž‹ä»“åº“](https://github.com/onnx/models)æ˜¯ç”±ç¤¾åŒºæˆå‘˜è´¡çŒ®çš„ä¸€ç»„é¢„è®­ç»ƒçš„å…ˆè¿›æ¨¡åž‹ï¼Œæ ¼å¼ä¸º ONNXã€‚æ¯ä¸ªæ¨¡åž‹éƒ½é™„å¸¦äº†ç”¨äºŽæ¨¡åž‹è®­ç»ƒå’Œè¿è¡ŒæŽ¨ç†çš„ Jupyter ç¬”è®°æœ¬ã€‚è¿™äº›ç¬”è®°æœ¬ä»¥ Python ç¼–å†™ï¼Œå¹¶åŒ…å«åˆ°è®­ç»ƒæ•°æ®é›†çš„é“¾æŽ¥ï¼Œä»¥åŠæè¿°æ¨¡åž‹æž¶æž„çš„åŽŸå§‹è®ºæ–‡çš„å‚è€ƒæ–‡çŒ®ã€‚

## Hugging Face Spaces å’Œ Gradio æ˜¯ä»€ä¹ˆï¼Ÿ

### Gradio

Gradio å¯è®©ç”¨æˆ·ä½¿ç”¨ Python ä»£ç å°†å…¶æœºå™¨å­¦ä¹ æ¨¡åž‹æ¼”ç¤ºä¸º Web åº”ç”¨ç¨‹åºã€‚Gradio å°† Python å‡½æ•°å°è£…åˆ°ç”¨æˆ·ç•Œé¢ä¸­ï¼Œæ¼”ç¤ºå¯ä»¥åœ¨ jupyter ç¬”è®°æœ¬ã€colab ç¬”è®°æœ¬ä¸­å¯åŠ¨ï¼Œå¹¶å¯ä»¥åµŒå…¥åˆ°æ‚¨è‡ªå·±çš„ç½‘ç«™ä¸Šï¼Œå¹¶åœ¨ Hugging Face Spaces ä¸Šå…è´¹æ‰˜ç®¡ã€‚

åœ¨æ­¤å¤„å¼€å§‹[https://gradio.app/getting_started](https://gradio.app/getting_started)

### Hugging Face Spaces

Hugging Face Spaces æ˜¯ Gradio æ¼”ç¤ºçš„å…è´¹æ‰˜ç®¡é€‰é¡¹ã€‚Spaces æä¾›äº† 3 ç§ SDK é€‰é¡¹ï¼šGradioã€Streamlit å’Œé™æ€ HTML æ¼”ç¤ºã€‚Spaces å¯ä»¥æ˜¯å…¬å…±çš„æˆ–ç§æœ‰çš„ï¼Œå·¥ä½œæµç¨‹ä¸Ž github repos ç±»ä¼¼ã€‚ç›®å‰ Hugging Face ä¸Šæœ‰ 2000 å¤šä¸ª Spacesã€‚åœ¨æ­¤å¤„äº†è§£æ›´å¤šå…³äºŽ Spaces çš„ä¿¡æ¯[https://huggingface.co/spaces/launch](https://huggingface.co/spaces/launch)ã€‚

### Hugging Face æ¨¡åž‹

Hugging Face æ¨¡åž‹ä¸­å¿ƒè¿˜æ”¯æŒ ONNX æ¨¡åž‹ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡[ONNX æ ‡ç­¾](https://huggingface.co/models?library=onnx&sort=downloads)å¯¹ ONNX æ¨¡åž‹è¿›è¡Œç­›é€‰

## Hugging Face æ˜¯å¦‚ä½•å¸®åŠ© ONNX æ¨¡åž‹ä»“åº“çš„ï¼Ÿ

ONNX æ¨¡åž‹ä»“åº“ä¸­æœ‰è®¸å¤š Jupyter ç¬”è®°æœ¬ä¾›ç”¨æˆ·æµ‹è¯•æ¨¡åž‹ã€‚ä»¥å‰ï¼Œç”¨æˆ·éœ€è¦è‡ªå·±ä¸‹è½½æ¨¡åž‹å¹¶åœ¨æœ¬åœ°è¿è¡Œè¿™äº›ç¬”è®°æœ¬æµ‹è¯•ã€‚æœ‰äº† Hugging Faceï¼Œæµ‹è¯•è¿‡ç¨‹å¯ä»¥æ›´ç®€å•å’Œç”¨æˆ·å‹å¥½ã€‚ç”¨æˆ·å¯ä»¥åœ¨ Hugging Face Spaces ä¸Šè½»æ¾å°è¯• ONNX æ¨¡åž‹ä»“åº“ä¸­çš„æŸä¸ªæ¨¡åž‹ï¼Œå¹¶ä½¿ç”¨ ONNX Runtime è¿è¡Œç”± Gradio æä¾›æ”¯æŒçš„å¿«é€Ÿæ¼”ç¤ºï¼Œå…¨éƒ¨åœ¨äº‘ç«¯è¿›è¡Œï¼Œæ— éœ€åœ¨æœ¬åœ°ä¸‹è½½ä»»ä½•å†…å®¹ã€‚è¯·æ³¨æ„ï¼ŒONNX æœ‰å„ç§è¿è¡Œæ—¶ï¼Œä¾‹å¦‚[ONNX Runtime](https://github.com/microsoft/onnxruntime)ã€[MXNet](https://github.com/apache/incubator-mxnet)ç­‰

## ONNX Runtime çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

ONNX Runtime æ˜¯ä¸€ä¸ªè·¨å¹³å°çš„æŽ¨ç†å’Œè®­ç»ƒæœºå™¨å­¦ä¹ åŠ é€Ÿå™¨ã€‚å®ƒä½¿å¾—åœ¨ Hugging Face ä¸Šä½¿ç”¨ ONNX æ¨¡åž‹ä»“åº“ä¸­çš„æ¨¡åž‹è¿›è¡Œå®žæ—¶ Gradio æ¼”ç¤ºæˆä¸ºå¯èƒ½ã€‚

ONNX Runtime å¯ä»¥å®žçŽ°æ›´å¿«çš„å®¢æˆ·ä½“éªŒå’Œæ›´ä½Žçš„æˆæœ¬ï¼Œæ”¯æŒæ¥è‡ª PyTorch å’Œ TensorFlow/Keras ç­‰æ·±åº¦å­¦ä¹ æ¡†æž¶ä»¥åŠ scikit-learnã€LightGBMã€XGBoost ç­‰ä¼ ç»Ÿæœºå™¨å­¦ä¹ åº“çš„æ¨¡åž‹ã€‚ONNX Runtime ä¸Žä¸åŒçš„ç¡¬ä»¶ã€é©±åŠ¨ç¨‹åºå’Œæ“ä½œç³»ç»Ÿå…¼å®¹ï¼Œå¹¶é€šè¿‡åˆ©ç”¨é€‚ç”¨çš„ç¡¬ä»¶åŠ é€Ÿå™¨ä»¥åŠå›¾å½¢ä¼˜åŒ–å’Œè½¬æ¢æä¾›æœ€ä½³æ€§èƒ½ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[å®˜æ–¹ç½‘ç«™](https://onnxruntime.ai/)ã€‚

## ä¸º EfficientNet-Lite4 è®¾ç½® Gradio æ¼”ç¤º

EfficientNet-Lite 4 æ˜¯ EfficientNet-Lite ç³»åˆ—ä¸­æœ€å¤§å’Œæœ€å‡†ç¡®çš„æ¨¡åž‹ã€‚å®ƒæ˜¯ä¸€ä¸ªä»…ä½¿ç”¨æ•´æ•°é‡åŒ–çš„æ¨¡åž‹ï¼Œèƒ½å¤Ÿåœ¨æ‰€æœ‰ EfficientNet æ¨¡åž‹ä¸­æä¾›æœ€é«˜çš„å‡†ç¡®çŽ‡ã€‚åœ¨ Pixel 4 CPU ä¸Šä»¥å®žæ—¶æ–¹å¼è¿è¡Œï¼ˆä¾‹å¦‚ 30ms/ å›¾åƒï¼‰æ—¶ï¼Œå¯ä»¥å®žçŽ° 80.4ï¼…çš„ ImageNet top-1 å‡†ç¡®çŽ‡ã€‚è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·é˜…è¯»[æ¨¡åž‹å¡ç‰‡](https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4)

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Gradio ä¸º EfficientNet-Lite4 è®¾ç½®ç¤ºä¾‹æ¼”ç¤º

é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥æ‰€éœ€çš„ä¾èµ–é¡¹å¹¶ä¸‹è½½å’Œè½½å…¥æ¥è‡ª ONNX æ¨¡åž‹ä»“åº“çš„ efficientnet-lite4 æ¨¡åž‹ã€‚ç„¶åŽä»Ž labels_map.txt æ–‡ä»¶åŠ è½½æ ‡ç­¾ã€‚æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¾ç½®é¢„å¤„ç†å‡½æ•°ã€åŠ è½½ç”¨äºŽæŽ¨ç†çš„æ¨¡åž‹å¹¶è®¾ç½®æŽ¨ç†å‡½æ•°ã€‚æœ€åŽï¼Œå°†æŽ¨ç†å‡½æ•°å°è£…åˆ° Gradio æŽ¥å£ä¸­ï¼Œä¾›ç”¨æˆ·è¿›è¡Œäº¤äº’ã€‚ä¸‹é¢æ˜¯å®Œæ•´çš„ä»£ç ã€‚

```python
import numpy as np
import math
import matplotlib.pyplot as plt
import cv2
import json
import gradio as gr
from huggingface_hub import hf_hub_download
from onnx import hub
import onnxruntime as ort

# ä»ŽONNXæ¨¡åž‹ä»“åº“åŠ è½½ONNXæ¨¡åž‹
model = hub.load("efficientnet-lite4")
# åŠ è½½æ ‡ç­¾æ–‡æœ¬æ–‡ä»¶
labels = json.load(open("labels_map.txt", "r"))

# é€šè¿‡å°†å›¾åƒä»Žä¸­å¿ƒè°ƒæ•´å¤§å°å¹¶è£å‰ªåˆ°224x224æ¥è®¾ç½®å›¾åƒæ–‡ä»¶çš„å°ºå¯¸
def pre_process_edgetpu(img, dims):
    output_height, output_width, _ = dims
    img = resize_with_aspectratio(img, output_height, output_width, inter_pol=cv2.INTER_LINEAR)
    img = center_crop(img, output_height, output_width)
    img = np.asarray(img, dtype='float32')
    # å°†jpgåƒç´ å€¼ä»Ž[0 - 255]è½¬æ¢ä¸ºæµ®ç‚¹æ•°ç»„[-1.0 - 1.0]
    img -= [127.0, 127.0, 127.0]
    img /= [128.0, 128.0, 128.0]
    return img

# ä½¿ç”¨ç­‰æ¯”ä¾‹ç¼©æ”¾è°ƒæ•´å›¾åƒå°ºå¯¸
def resize_with_aspectratio(img, out_height, out_width, scale=87.5, inter_pol=cv2.INTER_LINEAR):
    height, width, _ = img.shape
    new_height = int(100. * out_height / scale)
    new_width = int(100. * out_width / scale)
    if height > width:
        w = new_width
        h = int(new_height * height / width)
    else:
        h = new_height
        w = int(new_width * width / height)
    img = cv2.resize(img, (w, h), interpolation=inter_pol)
    return img

# crops the image around the center based on given height and width
def center_crop(img, out_height, out_width):
    height, width, _ = img.shape
    left = int((width - out_width) / 2)
    right = int((width + out_width) / 2)
    top = int((height - out_height) / 2)
    bottom = int((height + out_height) / 2)
    img = img[top:bottom, left:right]
    return img


sess = ort.InferenceSession(model)

def inference(img):
  img = cv2.imread(img)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

  img = pre_process_edgetpu(img, (224, 224, 3))

  img_batch = np.expand_dims(img, axis=0)

  results = sess.run(["Softmax:0"], {"images:0": img_batch})[0]
  result = reversed(results[0].argsort()[-5:])
  resultdic = {}
  for r in result:
      resultdic[labels[str(r)]] = float(results[0][r])
  return resultdic

title = "EfficientNet-Lite4"
description = "EfficientNet-Lite 4æ˜¯æœ€å¤§çš„å˜ä½“ï¼Œä¹Ÿæ˜¯EfficientNet-Liteæ¨¡åž‹é›†åˆä¸­æœ€å‡†ç¡®çš„ã€‚å®ƒæ˜¯ä¸€ä¸ªä»…åŒ…å«æ•´æ•°çš„é‡åŒ–æ¨¡åž‹ï¼Œå…·æœ‰æ‰€æœ‰EfficientNetæ¨¡åž‹ä¸­æœ€é«˜çš„å‡†ç¡®åº¦ã€‚åœ¨Pixel 4 CPUä¸Šï¼Œå®ƒå®žçŽ°äº†80.4ï¼…çš„ImageNet top-1å‡†ç¡®åº¦ï¼ŒåŒæ—¶ä»ç„¶å¯ä»¥å®žæ—¶è¿è¡Œï¼ˆä¾‹å¦‚30ms/å›¾åƒï¼‰ã€‚"
examples = [['catonnx.jpg']]
gr.Interface(inference, gr.Image(type="filepath"), "label", title=title, description=description, examples=examples).launch()
```

## å¦‚ä½•ä½¿ç”¨ ONNX æ¨¡åž‹åœ¨ HF Spaces ä¸Šè´¡çŒ® Gradio æ¼”ç¤º

- å°†æ¨¡åž‹æ·»åŠ åˆ°[onnx model zoo](https://github.com/onnx/models/blob/main/.github/PULL_REQUEST_TEMPLATE.md)
- åœ¨ Hugging Face ä¸Šåˆ›å»ºä¸€ä¸ªè´¦å·[here](https://huggingface.co/join).
- è¦æŸ¥çœ‹è¿˜æœ‰å“ªäº›æ¨¡åž‹éœ€è¦æ·»åŠ åˆ° ONNX ç»„ç»‡ä¸­ï¼Œè¯·å‚é˜…[Models list](https://github.com/onnx/models#models)ä¸­çš„åˆ—è¡¨
- åœ¨æ‚¨çš„ç”¨æˆ·åä¸‹æ·»åŠ  Gradio Demoï¼Œè¯·å‚é˜…æ­¤[åšæ–‡](https://huggingface.co/blog/gradio-spaces)ä»¥åœ¨ Hugging Face ä¸Šè®¾ç½® Gradio Demoã€‚
- è¯·æ±‚åŠ å…¥ ONNX ç»„ç»‡[here](https://huggingface.co/onnx).
- ä¸€æ—¦èŽ·å‡†ï¼Œå°†æ¨¡åž‹ä»Žæ‚¨çš„ç”¨æˆ·åä¸‹è½¬ç§»åˆ° ONNX ç»„ç»‡
- åœ¨æ¨¡åž‹è¡¨ä¸­ä¸ºæ¨¡åž‹æ·»åŠ å¾½ç« ï¼Œåœ¨[Models list](https://github.com/onnx/models#models)ä¸­æŸ¥çœ‹ç¤ºä¾‹

---

<!-- Source: guides/cn/04_integrating-other-frameworks/Gradio-and-Wandb-Integration.md -->
# Gradio and W&B Integration

ç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/akhaliq/JoJoGAN
æ ‡ç­¾ï¼šWANDB, SPACES
ç”± Gradio å›¢é˜Ÿè´¡çŒ®

## ä»‹ç»

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å¯¼æ‚¨å®Œæˆä»¥ä¸‹å†…å®¹ï¼š

- Gradioã€Hugging Face Spaces å’Œ Wandb çš„ä»‹ç»
- å¦‚ä½•ä½¿ç”¨ Wandb é›†æˆä¸º JoJoGAN è®¾ç½® Gradio æ¼”ç¤º
- å¦‚ä½•åœ¨ Hugging Face çš„ Wandb ç»„ç»‡ä¸­è¿½è¸ªå®žéªŒå¹¶è´¡çŒ®æ‚¨è‡ªå·±çš„ Gradio æ¼”ç¤º

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ Wandb è·Ÿè¸ªè®­ç»ƒå’Œå®žéªŒçš„æ¨¡åž‹ç¤ºä¾‹ï¼Œè¯·åœ¨ä¸‹æ–¹å°è¯• JoJoGAN æ¼”ç¤ºã€‚

<iframe src="https://akhaliq-jojogan.hf.space" frameBorder="0" height="810" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

## ä»€ä¹ˆæ˜¯ Wandbï¼Ÿ

Weights and Biases (W&B) å…è®¸æ•°æ®ç§‘å­¦å®¶å’Œæœºå™¨å­¦ä¹ ç§‘å­¦å®¶åœ¨ä»Žè®­ç»ƒåˆ°ç”Ÿäº§çš„æ¯ä¸ªé˜¶æ®µè·Ÿè¸ªä»–ä»¬çš„æœºå™¨å­¦ä¹ å®žéªŒã€‚ä»»ä½•æŒ‡æ ‡éƒ½å¯ä»¥å¯¹æ ·æœ¬è¿›è¡Œèšåˆï¼Œå¹¶åœ¨å¯è‡ªå®šä¹‰å’Œå¯æœç´¢çš„ä»ªè¡¨æ¿ä¸­æ˜¾ç¤ºï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

<img alt="Screen Shot 2022-08-01 at 5 54 59 PM" src="https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png">

## ä»€ä¹ˆæ˜¯ Hugging Face Spaces å’Œ Gradioï¼Ÿ

### Gradio

Gradio è®©ç”¨æˆ·å¯ä»¥ä½¿ç”¨å‡ è¡Œ Python ä»£ç å°†å…¶æœºå™¨å­¦ä¹ æ¨¡åž‹æ¼”ç¤ºä¸º Web åº”ç”¨ç¨‹åºã€‚Gradio å°†ä»»ä½• Python å‡½æ•°ï¼ˆä¾‹å¦‚æœºå™¨å­¦ä¹ æ¨¡åž‹çš„æŽ¨æ–­å‡½æ•°ï¼‰åŒ…è£…æˆä¸€ä¸ªç”¨æˆ·ç•Œé¢ï¼Œè¿™äº›æ¼”ç¤ºå¯ä»¥åœ¨ jupyter ç¬”è®°æœ¬ã€colab ç¬”è®°æœ¬ä¸­å¯åŠ¨ï¼Œä¹Ÿå¯ä»¥åµŒå…¥åˆ°æ‚¨è‡ªå·±çš„ç½‘ç«™ä¸­ï¼Œå…è´¹æ‰˜ç®¡åœ¨ Hugging Face Spaces ä¸Šã€‚

åœ¨è¿™é‡Œå¼€å§‹ [here](https://gradio.app/getting_started)

### Hugging Face Spaces

Hugging Face Spaces æ˜¯ Gradio æ¼”ç¤ºçš„å…è´¹æ‰˜ç®¡é€‰é¡¹ã€‚Spaces æœ‰ 3 ä¸ª SDK é€‰é¡¹ï¼šGradioã€Streamlit å’Œé™æ€ HTML æ¼”ç¤ºã€‚Spaces å¯ä»¥æ˜¯å…¬å…±çš„æˆ–ç§æœ‰çš„ï¼Œå·¥ä½œæµç¨‹ç±»ä¼¼äºŽ github å­˜å‚¨åº“ã€‚ç›®å‰åœ¨ Hugging Face ä¸Šæœ‰ 2000 å¤šä¸ª Spacesã€‚äº†è§£æ›´å¤šå…³äºŽ Spaces çš„ä¿¡æ¯ [here](https://huggingface.co/spaces/launch)ã€‚

## ä¸º JoJoGAN è®¾ç½® Gradio æ¼”ç¤º

çŽ°åœ¨ï¼Œè®©æˆ‘ä»¬å¼•å¯¼æ‚¨å¦‚ä½•åœ¨è‡ªå·±çš„çŽ¯å¢ƒä¸­å®Œæˆæ­¤æ“ä½œã€‚æˆ‘ä»¬å‡è®¾æ‚¨å¯¹ W&B å’Œ Gradio è¿˜ä¸å¤ªäº†è§£ï¼Œåªæ˜¯ä¸ºäº†æœ¬æ•™ç¨‹çš„ç›®çš„ã€‚

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

1. åˆ›å»º W&B è´¦å·

   å¦‚æžœæ‚¨è¿˜æ²¡æœ‰ W&B è´¦å·ï¼Œè¯·æŒ‰ç…§[è¿™äº›å¿«é€Ÿè¯´æ˜Ž](https://app.wandb.ai/login)åˆ›å»ºå…è´¹è´¦å·ã€‚è¿™ä¸åº”è¯¥è¶…è¿‡å‡ åˆ†é’Ÿçš„æ—¶é—´ã€‚ä¸€æ—¦å®Œæˆï¼ˆæˆ–è€…å¦‚æžœæ‚¨å·²ç»æœ‰ä¸€ä¸ªè´¦æˆ·ï¼‰ï¼ŒæŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿è¡Œä¸€ä¸ªå¿«é€Ÿçš„ colabã€‚

2. æ‰“å¼€ Colab å®‰è£… Gradio å’Œ W&B

   æˆ‘ä»¬å°†æŒ‰ç…§ JoJoGAN å­˜å‚¨åº“ä¸­æä¾›çš„ colab è¿›è¡Œæ“ä½œï¼Œç¨ä½œä¿®æ”¹ä»¥æ›´æœ‰æ•ˆåœ°ä½¿ç”¨ Wandb å’Œ Gradioã€‚

   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mchong6/JoJoGAN/blob/main/stylize.ipynb)

   åœ¨é¡¶éƒ¨å®‰è£… Gradio å’Œ Wandb:

   ```sh

   pip install gradio wandb
   ```

3. å¾®è°ƒ StyleGAN å’Œ W&B å®žéªŒè·Ÿè¸ª

   ä¸‹ä¸€æ­¥å°†æ‰“å¼€ä¸€ä¸ª W&B ä»ªè¡¨æ¿ï¼Œä»¥è·Ÿè¸ªå®žéªŒï¼Œå¹¶æ˜¾ç¤ºä¸€ä¸ª Gradio æ¼”ç¤ºæä¾›çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼Œæ‚¨å¯ä»¥ä»Žä¸‹æ‹‰èœå•ä¸­é€‰æ‹©ã€‚è¿™æ˜¯æ‚¨éœ€è¦çš„ä»£ç ï¼š

   ```python

   alpha =  1.0
   alpha = 1-alpha

   preserve_color = True
   num_iter = 100
   log_interval = 50

   samples = []
      column_names = ["Reference (y)", "Style Code(w)", "Real Face Image(x)"]

   wandb.init(project="JoJoGAN")
   config = wandb.config
   config.num_iter = num_iter
   config.preserve_color = preserve_color
   wandb.log(
   {"Style reference": [wandb.Image(transforms.ToPILImage()(target_im))]},
   step=0)

   # åŠ è½½åˆ¤åˆ«å™¨ç”¨äºŽæ„ŸçŸ¥æŸå¤±
   discriminator = Discriminator(1024, 2).eval().to(device)
   ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)
   discriminator.load_state_dict(ckpt["d"], strict=False)

   # é‡ç½®ç”Ÿæˆå™¨
   del generator
   generator = deepcopy(original_generator)

   g_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))

   # ç”¨äºŽç”Ÿæˆä¸€æ—åˆç†çœŸå®žå›¾åƒ-> å‡å›¾åƒçš„æ›´æ¢å›¾å±‚
   if preserve_color:
       id_swap = [9,11,15,16,17]
   else:
       id_swap = list(range(7, generator.n_latent))

   for idx in tqdm(range(num_iter)):
       mean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)
       in_latent = latents.clone()
       in_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]

       img = generator(in_latent, input_is_latent=True)

       with torch.no_grad():
           real_feat = discriminator(targets)
       fake_feat = discriminator(img)

       loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)

       wandb.log({"loss": loss}, step=idx)
       if idx % log_interval == 0:
           generator.eval()
           my_sample = generator(my_w, input_is_latent=True)
           generator.train()
           my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))
           wandb.log(
           {"Current stylization": [wandb.Image(my_sample)]},
           step=idx)
       table_data = [
               wandb.Image(transforms.ToPILImage()(target_im)),
               wandb.Image(img),
               wandb.Image(my_sample),
           ]
       samples.append(table_data)

       g_optim.zero_grad()
       loss.backward()
       g_optim.step()

   out_table = wandb.Table(data=samples, columns=column_names)
   wandb.log({" å½“å‰æ ·æœ¬æ•° ": out_table})
   ```

4. ä¿å­˜ã€ä¸‹è½½å’ŒåŠ è½½æ¨¡åž‹

   ä»¥ä¸‹æ˜¯å¦‚ä½•ä¿å­˜å’Œä¸‹è½½æ‚¨çš„æ¨¡åž‹ã€‚

   ```python

   from PIL import Image
   import torch
   torch.backends.cudnn.benchmark = True
   from torchvision import transforms, utils
   from util import *
   import math
   import random
   import numpy as np
   from torch import nn, autograd, optim
   from torch.nn import functional as F
   from tqdm import tqdm
   import lpips
   from model import *
   from e4e_projection import projection as e4e_projection

   from copy import deepcopy
   import imageio

   import os
   import sys
   import torchvision.transforms as transforms
   from argparse import Namespace
   from e4e.models.psp import pSp
   from util import *
   from huggingface_hub import hf_hub_download
   from google.colab import files
   torch.save({"g": generator.state_dict()}, "your-model-name.pt")

   files.download('your-model-name.pt')

   latent_dim = 512
   device="cuda"
   model_path_s = hf_hub_download(repo_id="akhaliq/jojogan-stylegan2-ffhq-config-f", filename="stylegan2-ffhq-config-f.pt")
   original_generator = Generator(1024, latent_dim, 8, 2).to(device)
   ckpt = torch.load(model_path_s, map_location=lambda storage, loc: storage)
   original_generator.load_state_dict(ckpt["g_ema"], strict=False)
   mean_latent = original_generator.mean_latent(10000)

   generator = deepcopy(original_generator)

   ckpt = torch.load("/content/JoJoGAN/your-model-name.pt", map_location=lambda storage, loc: storage)
   generator.load_state_dict(ckpt["g"], strict=False)
   generator.eval()

   plt.rcParams['figure.dpi'] = 150

   transform = transforms.Compose(
       [
           transforms.Resize((1024, 1024)),
           transforms.ToTensor(),
           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
       ]
   )

   def inference(img):
       img.save('out.jpg')
       aligned_face = align_face('out.jpg')

       my_w = e4e_projection(aligned_face, "out.pt", device).unsqueeze(0)
       with torch.no_grad():
           my_sample = generator(my_w, input_is_latent=True)

       npimage = my_sample[0].cpu().permute(1, 2, 0).detach().numpy()
       imageio.imwrite('filename.jpeg', npimage)
       return 'filename.jpeg'
   ```

5. æž„å»º Gradio æ¼”ç¤º

   ```python

   import gradio as gr

   title = "JoJoGAN"
   description = "JoJoGAN çš„ Gradio æ¼”ç¤ºï¼šä¸€æ¬¡æ€§é¢éƒ¨é£Žæ ¼åŒ–ã€‚è¦ä½¿ç”¨å®ƒï¼Œåªéœ€ä¸Šä¼ æ‚¨çš„å›¾åƒï¼Œæˆ–å•å‡»ç¤ºä¾‹ä¹‹ä¸€åŠ è½½å®ƒä»¬ã€‚åœ¨ä¸‹é¢çš„é“¾æŽ¥ä¸­é˜…è¯»æ›´å¤šä¿¡æ¯ã€‚"

   demo = gr.Interface(
       inference,
       gr.Image(type="pil"),
       gr.Image(type=" æ–‡ä»¶ "),
       title=title,
       description=description
   )

   demo.launch(share=True)
   ```

6. å°† Gradio é›†æˆåˆ° W&B ä»ªè¡¨æ¿

   æœ€åŽä¸€æ­¥â€”â€”å°† Gradio æ¼”ç¤ºä¸Ž W&B ä»ªè¡¨æ¿é›†æˆï¼Œåªéœ€è¦ä¸€è¡Œé¢å¤–çš„ä»£ç  :

   ```python

   demo.integrate(wandb=wandb)
   ```

   è°ƒç”¨é›†æˆä¹‹åŽï¼Œå°†åˆ›å»ºä¸€ä¸ªæ¼”ç¤ºï¼Œæ‚¨å¯ä»¥å°†å…¶é›†æˆåˆ°ä»ªè¡¨æ¿æˆ–æŠ¥å‘Šä¸­

   åœ¨ W&B ä¹‹å¤–ï¼Œä½¿ç”¨ gradio-app æ ‡è®°å…è®¸ä»»ä½•äººç›´æŽ¥å°† Gradio æ¼”ç¤ºåµŒå…¥åˆ°å…¶åšå®¢ã€ç½‘ç«™ã€æ–‡æ¡£ç­‰ä¸­çš„ HF spaces ä¸Š :

   ```html
   &lt;gradio-app space="akhaliq/JoJoGAN"&gt; &lt;gradio-app&gt;
   ```

7.ï¼ˆå¯é€‰ï¼‰åœ¨ Gradio åº”ç”¨ç¨‹åºä¸­åµŒå…¥ W&B å›¾

    ä¹Ÿå¯ä»¥åœ¨ Gradio åº”ç”¨ç¨‹åºä¸­åµŒå…¥ W&B å›¾ã€‚ä¸ºæ­¤ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ª W&B æŠ¥å‘Šï¼Œå¹¶åœ¨ä¸€ä¸ª `gr.HTML` å—ä¸­å°†å…¶åµŒå…¥åˆ° Gradio åº”ç”¨ç¨‹åºä¸­ã€‚

    æŠ¥å‘Šéœ€è¦æ˜¯å…¬å¼€çš„ï¼Œæ‚¨éœ€è¦åœ¨ iFrame ä¸­åŒ…è£… URLï¼Œå¦‚ä¸‹æ‰€ç¤º :
    The Report will need to be public and you will need to wrap the URL within an iFrame like this:
    ```python

    import gradio as gr

    def wandb_report(url):
        iframe = f'&lt;iframe src={url} style="border:none;height:1024px;width:100%"&gt;'
        return gr.HTML(iframe)

    with gr.Blocks() as demo:
        report_url = 'https://wandb.ai/_scott/pytorch-sweeps-demo/reports/loss-22-10-07-16-00-17---VmlldzoyNzU2NzAx'
        report = wandb_report(report_url)

    demo.launch(share=True)
    ```

## ç»“è®º

å¸Œæœ›æ‚¨å–œæ¬¢æ­¤åµŒå…¥ Gradio æ¼”ç¤ºåˆ° W&B æŠ¥å‘Šçš„ç®€çŸ­æ¼”ç¤ºï¼æ„Ÿè°¢æ‚¨ä¸€ç›´é˜…è¯»åˆ°æœ€åŽã€‚å›žé¡¾ä¸€ä¸‹ :

- ä»…éœ€è¦ä¸€ä¸ªå•ä¸€å‚è€ƒå›¾åƒå³å¯å¯¹ JoJoGAN è¿›è¡Œå¾®è°ƒï¼Œé€šå¸¸åœ¨ GPU ä¸Šéœ€è¦çº¦ 1 åˆ†é’Ÿã€‚è®­ç»ƒå®ŒæˆåŽï¼Œå¯ä»¥å°†æ ·å¼åº”ç”¨äºŽä»»ä½•è¾“å…¥å›¾åƒã€‚åœ¨è®ºæ–‡ä¸­é˜…è¯»æ›´å¤šå†…å®¹ã€‚

- W&B å¯ä»¥é€šè¿‡æ·»åŠ å‡ è¡Œä»£ç æ¥è·Ÿè¸ªå®žéªŒï¼Œæ‚¨å¯ä»¥åœ¨å•ä¸ªé›†ä¸­çš„ä»ªè¡¨æ¿ä¸­å¯è§†åŒ–ã€æŽ’åºå’Œç†è§£æ‚¨çš„å®žéªŒã€‚

- Gradio åˆ™åœ¨ç”¨æˆ·å‹å¥½çš„ç•Œé¢ä¸­æ¼”ç¤ºæ¨¡åž‹ï¼Œå¯ä»¥åœ¨ç½‘ç»œä¸Šä»»ä½•åœ°æ–¹å…±äº«ã€‚

## å¦‚ä½•åœ¨ Wandb ç»„ç»‡çš„ HF spaces ä¸Š è´¡çŒ® Gradio æ¼”ç¤º

- åœ¨ Hugging Face ä¸Šåˆ›å»ºä¸€ä¸ªå¸æˆ·[æ­¤å¤„](https://huggingface.co/join)ã€‚
- åœ¨æ‚¨çš„ç”¨æˆ·åä¸‹æ·»åŠ  Gradio æ¼”ç¤ºï¼Œè¯·å‚é˜…[æ­¤æ•™ç¨‹](https://huggingface.co/course/chapter9/4?fw=pt) ä»¥åœ¨ Hugging Face ä¸Šè®¾ç½® Gradio æ¼”ç¤ºã€‚
- ç”³è¯·åŠ å…¥ wandb ç»„ç»‡[æ­¤å¤„](https://huggingface.co/wandb)ã€‚
- æ‰¹å‡†åŽï¼Œå°†æ¨¡åž‹ä»Žè‡ªå·±çš„ç”¨æˆ·åè½¬ç§»åˆ° Wandb ç»„ç»‡ä¸­ã€‚

---

<!-- Source: guides/cn/04_integrating-other-frameworks/image-classification-in-pytorch.md -->
# PyTorch å›¾åƒåˆ†ç±»

Related spaces: https://huggingface.co/spaces/abidlabs/pytorch-image-classifier, https://huggingface.co/spaces/pytorch/ResNet, https://huggingface.co/spaces/pytorch/ResNext, https://huggingface.co/spaces/pytorch/SqueezeNet
Tags: VISION, RESNET, PYTORCH

## ä»‹ç»

å›¾åƒåˆ†ç±»æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒä»»åŠ¡ã€‚æž„å»ºæ›´å¥½çš„åˆ†ç±»å™¨ä»¥åŒºåˆ†å›¾ç‰‡ä¸­å­˜åœ¨çš„ç‰©ä½“æ˜¯å½“å‰ç ”ç©¶çš„ä¸€ä¸ªçƒ­ç‚¹é¢†åŸŸï¼Œå› ä¸ºå®ƒçš„åº”ç”¨èŒƒå›´ä»Žè‡ªåŠ¨é©¾é©¶è½¦è¾†åˆ°åŒ»å­¦æˆåƒç­‰é¢†åŸŸéƒ½å¾ˆå¹¿æ³›ã€‚

è¿™æ ·çš„æ¨¡åž‹éžå¸¸é€‚åˆ Gradio çš„ _image_ è¾“å…¥ç»„ä»¶ï¼Œå› æ­¤åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Gradio æž„å»ºä¸€ä¸ªç”¨äºŽå›¾åƒåˆ†ç±»çš„ Web æ¼”ç¤ºã€‚æˆ‘ä»¬å°†èƒ½å¤Ÿåœ¨ Python ä¸­æž„å»ºæ•´ä¸ª Web åº”ç”¨ç¨‹åºï¼Œæ•ˆæžœå¦‚ä¸‹ï¼ˆè¯•è¯•å…¶ä¸­ä¸€ä¸ªç¤ºä¾‹ï¼ï¼‰:

<iframe src="https://abidlabs-pytorch-image-classifier.hf.space" frameBorder="0" height="660" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

### å…ˆå†³æ¡ä»¶

ç¡®ä¿æ‚¨å·²ç»[å®‰è£…](/getting_started)äº† `gradio` Python åŒ…ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„å›¾åƒåˆ†ç±»æ¨¡åž‹ï¼Œæ‰€ä»¥æ‚¨è¿˜åº”è¯¥å®‰è£…äº† `torch`ã€‚

## ç¬¬ä¸€æ­¥ - è®¾ç½®å›¾åƒåˆ†ç±»æ¨¡åž‹

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå›¾åƒåˆ†ç±»æ¨¡åž‹ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„ Resnet-18 æ¨¡åž‹ï¼Œå› ä¸ºå®ƒå¯ä»¥ä»Ž[PyTorch Hub](https://pytorch.org/hub/pytorch_vision_resnet/)è½»æ¾ä¸‹è½½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å…¶ä»–é¢„è®­ç»ƒæ¨¡åž‹æˆ–è®­ç»ƒè‡ªå·±çš„æ¨¡åž‹ã€‚

```python
import torch

model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()
```

ç”±äºŽæˆ‘ä»¬å°†ä½¿ç”¨æ¨¡åž‹è¿›è¡ŒæŽ¨æ–­ï¼Œæ‰€ä»¥æˆ‘ä»¬è°ƒç”¨äº† `.eval()` æ–¹æ³•ã€‚

## ç¬¬äºŒæ­¥ - å®šä¹‰ `predict` å‡½æ•°

æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°æŽ¥å—*ç”¨æˆ·è¾“å…¥*ï¼Œåœ¨æœ¬ç¤ºä¾‹ä¸­æ˜¯ä¸€å¼ å›¾ç‰‡ï¼Œå¹¶è¿”å›žé¢„æµ‹ç»“æžœã€‚é¢„æµ‹ç»“æžœåº”è¯¥ä»¥å­—å…¸çš„å½¢å¼è¿”å›žï¼Œå…¶ä¸­é”®æ˜¯ç±»åˆ«åç§°ï¼Œå€¼æ˜¯ç½®ä¿¡åº¦æ¦‚çŽ‡ã€‚æˆ‘ä»¬å°†ä»Žè¿™ä¸ª[text æ–‡ä»¶](https://git.io/JJkYN)ä¸­åŠ è½½ç±»åˆ«åç§°ã€‚

å¯¹äºŽæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼Œå®ƒçš„ä»£ç å¦‚ä¸‹ï¼š

```python
import requests
from PIL import Image
from torchvision import transforms

# ä¸‹è½½ImageNetçš„å¯è¯»æ ‡ç­¾ã€‚
response = requests.get("https://git.io/JJkYN")
labels = response.text.split("\n")

def predict(inp):
  inp = transforms.ToTensor()(inp).unsqueeze(0)
  with torch.no_grad():
    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)
    confidences = {labels[i]: float(prediction[i]) for i in range(1000)}
  return confidences
```

è®©æˆ‘ä»¬é€æ­¥æ¥çœ‹ä¸€ä¸‹è¿™æ®µä»£ç ã€‚è¯¥å‡½æ•°æŽ¥å—ä¸€ä¸ªå‚æ•°ï¼š

- `inp`ï¼šè¾“å…¥å›¾ç‰‡ï¼Œç±»åž‹ä¸º `PIL` å›¾åƒ

ç„¶åŽï¼Œè¯¥å‡½æ•°å°†å›¾åƒè½¬æ¢ä¸º PIL å›¾åƒï¼Œæœ€ç»ˆè½¬æ¢ä¸º PyTorch çš„ `tensor`ï¼Œå°†å…¶è¾“å…¥æ¨¡åž‹ï¼Œå¹¶è¿”å›žï¼š

- `confidences`ï¼šé¢„æµ‹ç»“æžœï¼Œä»¥å­—å…¸å½¢å¼è¡¨ç¤ºï¼Œå…¶ä¸­é”®æ˜¯ç±»åˆ«æ ‡ç­¾ï¼Œå€¼æ˜¯ç½®ä¿¡åº¦æ¦‚çŽ‡

## ç¬¬ä¸‰æ­¥ - åˆ›å»º Gradio ç•Œé¢

çŽ°åœ¨æˆ‘ä»¬å·²ç»è®¾ç½®å¥½äº†é¢„æµ‹å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ª Gradio ç•Œé¢ã€‚

åœ¨æœ¬ä¾‹ä¸­ï¼Œè¾“å…¥ç»„ä»¶æ˜¯ä¸€ä¸ªæ‹–æ”¾å›¾ç‰‡çš„ç»„ä»¶ã€‚ä¸ºäº†åˆ›å»ºè¿™ä¸ªè¾“å…¥ç»„ä»¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ `Image(type="pil")` æ¥åˆ›å»ºè¯¥ç»„ä»¶ï¼Œå¹¶å¤„ç†é¢„å¤„ç†æ“ä½œå°†å…¶è½¬æ¢ä¸º `PIL` å›¾åƒã€‚

è¾“å‡ºç»„ä»¶å°†æ˜¯ä¸€ä¸ª `Label`ï¼Œå®ƒä»¥è‰¯å¥½çš„å½¢å¼æ˜¾ç¤ºé¡¶éƒ¨æ ‡ç­¾ã€‚ç”±äºŽæˆ‘ä»¬ä¸æƒ³æ˜¾ç¤ºæ‰€æœ‰ 1000 ä¸ªç±»åˆ«æ ‡ç­¾ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å…¶å®šåˆ¶ä¸ºåªæ˜¾ç¤ºå‰ 3 ä¸ªæ ‡ç­¾ï¼Œæž„é€ ä¸º `Label(num_top_classes=3)`ã€‚

æœ€åŽï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ª `examples` å‚æ•°ï¼Œå…è®¸æˆ‘ä»¬é¢„å¡«ä¸€äº›é¢„å®šä¹‰çš„ç¤ºä¾‹åˆ°ç•Œé¢ä¸­ã€‚Gradio çš„ä»£ç å¦‚ä¸‹ï¼š

```python
import gradio as gr

gr.Interface(fn=predict,
             inputs=gr.Image(type="pil"),
             outputs=gr.Label(num_top_classes=3),
             examples=["lion.jpg", "cheetah.jpg"]).launch()
```

è¿™å°†äº§ç”Ÿä»¥ä¸‹ç•Œé¢ï¼Œæ‚¨å¯ä»¥åœ¨æµè§ˆå™¨ä¸­ç›´æŽ¥å°è¯•ï¼ˆè¯•è¯•ä¸Šä¼ è‡ªå·±çš„ç¤ºä¾‹å›¾ç‰‡ï¼ï¼‰ï¼š

<iframe src="https://abidlabs-pytorch-image-classifier.hf.space" frameBorder="0" height="660" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

---

å®Œæˆäº†ï¼è¿™å°±æ˜¯æž„å»ºå›¾åƒåˆ†ç±»å™¨ Web æ¼”ç¤ºæ‰€éœ€çš„æ‰€æœ‰ä»£ç ã€‚å¦‚æžœæ‚¨æƒ³ä¸Žä»–äººå…±äº«ï¼Œè¯·åœ¨ `launch()` æŽ¥å£æ—¶è®¾ç½® `share=True`ï¼

---

<!-- Source: guides/cn/04_integrating-other-frameworks/image-classification-in-tensorflow.md -->
# TensorFlow å’Œ Keras ä¸­çš„å›¾åƒåˆ†ç±»

ç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/abidlabs/keras-image-classifier
æ ‡ç­¾ï¼šVISION, MOBILENET, TENSORFLOW

## ç®€ä»‹

å›¾åƒåˆ†ç±»æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹æ ¸å¿ƒä»»åŠ¡ã€‚æž„å»ºæ›´å¥½çš„åˆ†ç±»å™¨æ¥è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“æ˜¯ä¸€ä¸ªç ”ç©¶çš„çƒ­ç‚¹é¢†åŸŸï¼Œå› ä¸ºå®ƒåœ¨äº¤é€šæŽ§åˆ¶ç³»ç»Ÿåˆ°å«æ˜Ÿæˆåƒç­‰åº”ç”¨ä¸­éƒ½æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚

è¿™æ ·çš„æ¨¡åž‹éžå¸¸é€‚åˆä¸Ž Gradio çš„ _image_ è¾“å…¥ç»„ä»¶ä¸€èµ·ä½¿ç”¨ï¼Œå› æ­¤åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Gradio æž„å»ºä¸€ä¸ªç”¨äºŽå›¾åƒåˆ†ç±»çš„ Web æ¼”ç¤ºã€‚æˆ‘ä»¬å¯ä»¥åœ¨ Python ä¸­æž„å»ºæ•´ä¸ª Web åº”ç”¨ç¨‹åºï¼Œå®ƒçš„ç•Œé¢å°†å¦‚ä¸‹æ‰€ç¤ºï¼ˆè¯•è¯•å…¶ä¸­ä¸€ä¸ªä¾‹å­ï¼ï¼‰ï¼š

<iframe src="https://abidlabs-keras-image-classifier.hf.space" frameBorder="0" height="660" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

### å…ˆå†³æ¡ä»¶

ç¡®ä¿æ‚¨å·²ç»[å®‰è£…](/getting_started)äº† `gradio` Python åŒ…ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„ Keras å›¾åƒåˆ†ç±»æ¨¡åž‹ï¼Œå› æ­¤æ‚¨è¿˜åº”è¯¥å®‰è£…äº† `tensorflow`ã€‚

## ç¬¬ä¸€æ­¥ â€”â€” è®¾ç½®å›¾åƒåˆ†ç±»æ¨¡åž‹

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå›¾åƒåˆ†ç±»æ¨¡åž‹ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„ Mobile Net æ¨¡åž‹ï¼Œå› ä¸ºå®ƒå¯ä»¥ä»Ž[Keras](https://keras.io/api/applications/mobilenet/)è½»æ¾ä¸‹è½½ã€‚æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–é¢„è®­ç»ƒæ¨¡åž‹æˆ–è®­ç»ƒè‡ªå·±çš„æ¨¡åž‹ã€‚

```python
import tensorflow as tf

inception_net = tf.keras.applications.MobileNetV2()
```

æ­¤è¡Œä»£ç å°†ä½¿ç”¨ Keras åº“è‡ªåŠ¨ä¸‹è½½ MobileNet æ¨¡åž‹å’Œæƒé‡ã€‚

## ç¬¬äºŒæ­¥ â€”â€” å®šä¹‰ `predict` å‡½æ•°

æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°æŽ¥æ”¶*ç”¨æˆ·è¾“å…¥*ä½œä¸ºå‚æ•°ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸ºå›¾åƒï¼‰ï¼Œå¹¶è¿”å›žé¢„æµ‹ç»“æžœã€‚é¢„æµ‹ç»“æžœåº”ä»¥å­—å…¸å½¢å¼è¿”å›žï¼Œå…¶ä¸­é”®æ˜¯ç±»åï¼Œå€¼æ˜¯ç½®ä¿¡æ¦‚çŽ‡ã€‚æˆ‘ä»¬å°†ä»Žè¿™ä¸ª[text æ–‡ä»¶](https://git.io/JJkYN)ä¸­åŠ è½½ç±»åã€‚

å¯¹äºŽæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼Œå‡½æ•°å°†å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import requests

# ä»ŽImageNetä¸‹è½½å¯è¯»æ€§æ ‡ç­¾ã€‚
response = requests.get("https://git.io/JJkYN")
labels = response.text.split("\n")

def classify_image(inp):
  inp = inp.reshape((-1, 224, 224, 3))
  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)
  prediction = inception_net.predict(inp).flatten()
  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}
  return confidences
```

è®©æˆ‘ä»¬æ¥è¯¦ç»†äº†è§£ä¸€ä¸‹ã€‚è¯¥å‡½æ•°æŽ¥å—ä¸€ä¸ªå‚æ•°ï¼š

- `inp`ï¼šè¾“å…¥å›¾åƒçš„ `numpy` æ•°ç»„

ç„¶åŽï¼Œå‡½æ•°æ·»åŠ ä¸€ä¸ªæ‰¹æ¬¡ç»´åº¦ï¼Œé€šè¿‡æ¨¡åž‹è¿›è¡Œå¤„ç†ï¼Œå¹¶è¿”å›žï¼š

- `confidences`ï¼šé¢„æµ‹ç»“æžœï¼Œä»¥å­—å…¸å½¢å¼è¡¨ç¤ºï¼Œå…¶ä¸­é”®æ˜¯ç±»æ ‡ç­¾ï¼Œå€¼æ˜¯ç½®ä¿¡æ¦‚çŽ‡

## ç¬¬ä¸‰æ­¥ â€”â€” åˆ›å»º Gradio ç•Œé¢

çŽ°åœ¨æˆ‘ä»¬å·²ç»è®¾ç½®å¥½äº†é¢„æµ‹å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å›´ç»•å®ƒåˆ›å»ºä¸€ä¸ª Gradio ç•Œé¢ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¾“å…¥ç»„ä»¶æ˜¯ä¸€ä¸ªæ‹–æ”¾å›¾åƒç»„ä»¶ã€‚è¦åˆ›å»ºæ­¤è¾“å…¥ç»„ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `"gradio.inputs.Image"` ç±»ï¼Œè¯¥ç±»åˆ›å»ºè¯¥ç»„ä»¶å¹¶å¤„ç†é¢„å¤„ç†ä»¥å°†å…¶è½¬æ¢ä¸º numpy æ•°ç»„ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå‚æ•°æ¥å®žä¾‹åŒ–è¯¥ç±»ï¼Œè¯¥å‚æ•°ä¼šè‡ªåŠ¨å°†è¾“å…¥å›¾åƒé¢„å¤„ç†ä¸º 224 åƒç´  x224 åƒç´ çš„å¤§å°ï¼Œè¿™æ˜¯ MobileNet æ‰€æœŸæœ›çš„å°ºå¯¸ã€‚

è¾“å‡ºç»„ä»¶å°†æ˜¯ä¸€ä¸ª `"label"`ï¼Œå®ƒä»¥ç¾Žè§‚çš„å½¢å¼æ˜¾ç¤ºé¡¶éƒ¨æ ‡ç­¾ã€‚ç”±äºŽæˆ‘ä»¬ä¸æƒ³æ˜¾ç¤ºæ‰€æœ‰çš„ 1,000 ä¸ªç±»æ ‡ç­¾ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†è‡ªå®šä¹‰å®ƒåªæ˜¾ç¤ºå‰ 3 ä¸ªæ ‡ç­¾ã€‚

æœ€åŽï¼Œæˆ‘ä»¬è¿˜å°†æ·»åŠ ä¸€ä¸ª `examples` å‚æ•°ï¼Œå®ƒå…è®¸æˆ‘ä»¬ä½¿ç”¨ä¸€äº›é¢„å®šä¹‰çš„ç¤ºä¾‹é¢„å¡«å……æˆ‘ä»¬çš„æŽ¥å£ã€‚Gradio çš„ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import gradio as gr

gr.Interface(fn=classify_image,
             inputs=gr.Image(width=224, height=224),
             outputs=gr.Label(num_top_classes=3),
             examples=["banana.jpg", "car.jpg"]).launch()
```

è¿™å°†ç”Ÿæˆä»¥ä¸‹ç•Œé¢ï¼Œæ‚¨å¯ä»¥åœ¨æµè§ˆå™¨ä¸­ç«‹å³å°è¯•ï¼ˆå°è¯•ä¸Šä¼ æ‚¨è‡ªå·±çš„ç¤ºä¾‹ï¼ï¼‰ï¼š

<iframe src="https://abidlabs-keras-image-classifier.hf.space" frameBorder="0" height="660" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

---

å®Œæˆï¼è¿™å°±æ˜¯æž„å»ºå›¾åƒåˆ†ç±»å™¨çš„ Web æ¼”ç¤ºæ‰€éœ€çš„æ‰€æœ‰ä»£ç ã€‚å¦‚æžœæ‚¨æƒ³ä¸Žä»–äººåˆ†äº«ï¼Œè¯·å°è¯•åœ¨å¯åŠ¨æŽ¥å£æ—¶è®¾ç½® `share=True`ï¼

---

<!-- Source: guides/cn/04_integrating-other-frameworks/image-classification-with-vision-transformers.md -->
# Vision Transformers å›¾åƒåˆ†ç±»

ç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/abidlabs/vision-transformer
æ ‡ç­¾ï¼šVISION, TRANSFORMERS, HUB

## ç®€ä»‹

å›¾åƒåˆ†ç±»æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„é‡è¦ä»»åŠ¡ã€‚æž„å»ºæ›´å¥½çš„åˆ†ç±»å™¨ä»¥ç¡®å®šå›¾åƒä¸­å­˜åœ¨çš„å¯¹è±¡æ˜¯å½“å‰ç ”ç©¶çš„çƒ­ç‚¹é¢†åŸŸï¼Œå› ä¸ºå®ƒåœ¨ä»Žäººè„¸è¯†åˆ«åˆ°åˆ¶é€ è´¨é‡æŽ§åˆ¶ç­‰æ–¹é¢éƒ½æœ‰åº”ç”¨ã€‚

æœ€å…ˆè¿›çš„å›¾åƒåˆ†ç±»å™¨åŸºäºŽ _transformers_ æž¶æž„ï¼Œè¯¥æž¶æž„æœ€åˆåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å¾ˆå—æ¬¢è¿Žã€‚è¿™ç§æž¶æž„é€šå¸¸è¢«ç§°ä¸º vision transformers (ViT)ã€‚è¿™äº›æ¨¡åž‹éžå¸¸é€‚åˆä¸Ž Gradio çš„*å›¾åƒ*è¾“å…¥ç»„ä»¶ä¸€èµ·ä½¿ç”¨ï¼Œå› æ­¤åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æž„å»ºä¸€ä¸ªä½¿ç”¨ Gradio è¿›è¡Œå›¾åƒåˆ†ç±»çš„ Web æ¼”ç¤ºã€‚æˆ‘ä»¬åªéœ€ç”¨**ä¸€è¡Œ Python ä»£ç **å³å¯æž„å»ºæ•´ä¸ª Web åº”ç”¨ç¨‹åºï¼Œå…¶æ•ˆæžœå¦‚ä¸‹ï¼ˆè¯•ç”¨ä¸€ä¸‹ç¤ºä¾‹ä¹‹ä¸€ï¼ï¼‰ï¼š

<iframe src="https://abidlabs-vision-transformer.hf.space" frameBorder="0" height="660" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

### å…ˆå†³æ¡ä»¶

ç¡®ä¿æ‚¨å·²ç»[å®‰è£…](/getting_started)äº† `gradio` Python åŒ…ã€‚

## æ­¥éª¤ 1 - é€‰æ‹© Vision å›¾åƒåˆ†ç±»æ¨¡åž‹

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå›¾åƒåˆ†ç±»æ¨¡åž‹ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification)ä¸Šçš„ä¸€ä¸ªæ¨¡åž‹ã€‚è¯¥ Hub åŒ…å«æ•°åƒä¸ªæ¨¡åž‹ï¼Œæ¶µç›–äº†å¤šç§ä¸åŒçš„æœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚

åœ¨å·¦ä¾§è¾¹æ ä¸­å±•å¼€ Tasks ç±»åˆ«ï¼Œå¹¶é€‰æ‹©æˆ‘ä»¬æ„Ÿå…´è¶£çš„â€œImage Classificationâ€ä½œä¸ºæˆ‘ä»¬çš„ä»»åŠ¡ã€‚ç„¶åŽï¼Œæ‚¨å°†çœ‹åˆ° Hub ä¸Šä¸ºå›¾åƒåˆ†ç±»è®¾è®¡çš„æ‰€æœ‰æ¨¡åž‹ã€‚

åœ¨æ’°å†™æ—¶ï¼Œæœ€å—æ¬¢è¿Žçš„æ¨¡åž‹æ˜¯ `google/vit-base-patch16-224`ï¼Œè¯¥æ¨¡åž‹åœ¨åˆ†è¾¨çŽ‡ä¸º 224x224 åƒç´ çš„ ImageNet å›¾åƒä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚æˆ‘ä»¬å°†åœ¨æ¼”ç¤ºä¸­ä½¿ç”¨æ­¤æ¨¡åž‹ã€‚

## æ­¥éª¤ 2 - ä½¿ç”¨ Gradio åŠ è½½ Vision Transformer æ¨¡åž‹

å½“ä½¿ç”¨ Hugging Face Hub ä¸Šçš„æ¨¡åž‹æ—¶ï¼Œæˆ‘ä»¬æ— éœ€ä¸ºæ¼”ç¤ºå®šä¹‰è¾“å…¥æˆ–è¾“å‡ºç»„ä»¶ã€‚åŒæ ·ï¼Œæˆ‘ä»¬ä¸éœ€è¦å…³å¿ƒé¢„å¤„ç†æˆ–åŽå¤„ç†çš„ç»†èŠ‚ã€‚æ‰€æœ‰è¿™äº›éƒ½å¯ä»¥ä»Žæ¨¡åž‹æ ‡ç­¾ä¸­è‡ªåŠ¨æŽ¨æ–­å‡ºæ¥ã€‚

é™¤äº†å¯¼å…¥è¯­å¥å¤–ï¼Œæˆ‘ä»¬åªéœ€è¦ä¸€è¡Œä»£ç å³å¯åŠ è½½å¹¶å¯åŠ¨æ¼”ç¤ºã€‚

æˆ‘ä»¬ä½¿ç”¨ `gr.Interface.load()` æ–¹æ³•ï¼Œå¹¶ä¼ å…¥åŒ…å« `huggingface/` çš„æ¨¡åž‹è·¯å¾„ï¼Œä»¥æŒ‡å®šå®ƒæ¥è‡ª Hugging Face Hubã€‚

```python
import gradio as gr

gr.Interface.load(
             "huggingface/google/vit-base-patch16-224",
             examples=["alligator.jpg", "laptop.jpg"]).launch()
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ª `examples` å‚æ•°ï¼Œå…è®¸æˆ‘ä»¬ä½¿ç”¨ä¸€äº›é¢„å®šä¹‰çš„ç¤ºä¾‹é¢„å¡«å……æˆ‘ä»¬çš„ç•Œé¢ã€‚

è¿™å°†ç”Ÿæˆä»¥ä¸‹æŽ¥å£ï¼Œæ‚¨å¯ä»¥ç›´æŽ¥åœ¨æµè§ˆå™¨ä¸­å°è¯•ã€‚å½“æ‚¨è¾“å…¥å›¾åƒæ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨è¿›è¡Œé¢„å¤„ç†å¹¶å‘é€åˆ° Hugging Face Hub APIï¼Œé€šè¿‡æ¨¡åž‹å¤„ç†ï¼Œå¹¶ä»¥äººç±»å¯è§£é‡Šçš„é¢„æµ‹ç»“æžœè¿”å›žã€‚å°è¯•ä¸Šä¼ æ‚¨è‡ªå·±çš„å›¾åƒï¼

<iframe src="https://abidlabs-vision-transformer.hf.space" frameBorder="0" height="660" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

---

å®Œæˆï¼åªéœ€ä¸€è¡Œä»£ç ï¼Œæ‚¨å°±å»ºç«‹äº†ä¸€ä¸ªå›¾åƒåˆ†ç±»å™¨çš„ Web æ¼”ç¤ºã€‚å¦‚æžœæ‚¨æƒ³ä¸Žä»–äººåˆ†äº«ï¼Œè¯·åœ¨ `launch()` æŽ¥å£æ—¶è®¾ç½® `share=True`ã€‚

---

<!-- Source: guides/cn/05_tabular-data-science-and-plots/creating-a-dashboard-from-bigquery-data.md -->
# ä»Ž BigQuery æ•°æ®åˆ›å»ºå®žæ—¶ä»ªè¡¨ç›˜

Tags: è¡¨æ ¼ , ä»ªè¡¨ç›˜ , ç»˜å›¾

[Google BigQuery](https://cloud.google.com/bigquery) æ˜¯ä¸€ä¸ªåŸºäºŽäº‘çš„ç”¨äºŽå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†çš„æœåŠ¡ã€‚å®ƒæ˜¯ä¸€ä¸ªæ— æœåŠ¡å™¨ä¸”é«˜åº¦å¯æ‰©å±•çš„æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿä½¿ç”¨ç±»ä¼¼ SQL çš„æŸ¥è¯¢åˆ†æžæ•°æ®ã€‚

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ `gradio` åœ¨ Python ä¸­æŸ¥è¯¢ BigQuery æ•°æ®é›†å¹¶åœ¨å®žæ—¶ä»ªè¡¨ç›˜ä¸­æ˜¾ç¤ºæ•°æ®ã€‚ä»ªè¡¨æ¿å°†å¦‚ä¸‹æ‰€ç¤ºï¼š

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/bigquery-dashboard.gif">

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä»¥ä¸‹æ­¥éª¤ï¼š

1. è®¾ç½® BigQuery å‡­æ®
2. ä½¿ç”¨ BigQuery å®¢æˆ·ç«¯
3. æž„å»ºå®žæ—¶ä»ªè¡¨ç›˜ï¼ˆä»…éœ€ _7 è¡Œ Python ä»£ç _ï¼‰

æˆ‘ä»¬å°†ä½¿ç”¨[çº½çº¦æ—¶æŠ¥çš„ COVID æ•°æ®é›†](https://www.nytimes.com/interactive/2021/us/covid-cases.html)ï¼Œè¯¥æ•°æ®é›†ä½œä¸ºä¸€ä¸ªå…¬å…±æ•°æ®é›†å¯åœ¨ BigQuery ä¸Šä½¿ç”¨ã€‚æ•°æ®é›†åä¸º `covid19_nyt.us_counties`ï¼Œå…¶ä¸­åŒ…å«æœ‰å…³ç¾Žå›½å„åŽ¿ COVID ç¡®è¯Šç—…ä¾‹å’Œæ­»äº¡äººæ•°çš„æœ€æ–°ä¿¡æ¯ã€‚

**å…ˆå†³æ¡ä»¶**ï¼šæœ¬æŒ‡å—ä½¿ç”¨ [Gradio Blocks](../quickstart/#blocks-more-flexibility-and-control)ï¼Œå› æ­¤è¯·ç¡®ä¿æ‚¨ç†Ÿæ‚‰ Blocks ç±»ã€‚

## è®¾ç½® BigQuery å‡­æ®

è¦ä½¿ç”¨ Gradio å’Œ BigQueryï¼Œæ‚¨éœ€è¦èŽ·å–æ‚¨çš„ BigQuery å‡­æ®ï¼Œå¹¶å°†å…¶ä¸Ž [BigQuery Python å®¢æˆ·ç«¯](https://pypi.org/project/google-cloud-bigquery/) ä¸€èµ·ä½¿ç”¨ã€‚å¦‚æžœæ‚¨å·²ç»æ‹¥æœ‰ BigQuery å‡­æ®ï¼ˆä½œä¸º `.json` æ–‡ä»¶ï¼‰ï¼Œåˆ™å¯ä»¥è·³è¿‡æ­¤éƒ¨åˆ†ã€‚å¦åˆ™ï¼Œæ‚¨å¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å…è´¹å®Œæˆæ­¤æ“ä½œã€‚

1. é¦–å…ˆï¼Œç™»å½•åˆ°æ‚¨çš„ Google Cloud å¸æˆ·ï¼Œå¹¶è½¬åˆ° Google Cloud æŽ§åˆ¶å° (https://console.cloud.google.com/)

2. åœ¨ Cloud æŽ§åˆ¶å°ä¸­ï¼Œå•å‡»å·¦ä¸Šè§’çš„æ±‰å ¡èœå•ï¼Œç„¶åŽä»Žèœå•ä¸­é€‰æ‹©â€œAPI ä¸ŽæœåŠ¡â€ã€‚å¦‚æžœæ‚¨æ²¡æœ‰çŽ°æœ‰é¡¹ç›®ï¼Œåˆ™éœ€è¦åˆ›å»ºä¸€ä¸ªé¡¹ç›®ã€‚

3. ç„¶åŽï¼Œå•å‡»â€œ+ å¯ç”¨çš„ API ä¸ŽæœåŠ¡â€æŒ‰é’®ï¼Œè¯¥æŒ‰é’®å…è®¸æ‚¨ä¸ºé¡¹ç›®å¯ç”¨ç‰¹å®šæœåŠ¡ã€‚æœç´¢â€œBigQuery APIâ€ï¼Œå•å‡»å®ƒï¼Œç„¶åŽå•å‡»â€œå¯ç”¨â€æŒ‰é’®ã€‚å¦‚æžœæ‚¨çœ‹åˆ°â€œç®¡ç†â€æŒ‰é’®ï¼Œåˆ™è¡¨ç¤º BigQuery å·²å¯ç”¨ï¼Œæ‚¨å·²å‡†å¤‡å°±ç»ªã€‚

4. åœ¨â€œAPI ä¸ŽæœåŠ¡â€èœå•ä¸­ï¼Œå•å‡»â€œå‡­æ®â€é€‰é¡¹å¡ï¼Œç„¶åŽå•å‡»â€œåˆ›å»ºå‡­æ®â€æŒ‰é’®ã€‚

5. åœ¨â€œåˆ›å»ºå‡­æ®â€å¯¹è¯æ¡†ä¸­ï¼Œé€‰æ‹©â€œæœåŠ¡å¸å·å¯†é’¥â€ä½œä¸ºè¦åˆ›å»ºçš„å‡­æ®ç±»åž‹ï¼Œå¹¶ä¸ºå…¶å‘½åã€‚è¿˜å¯ä»¥é€šè¿‡ä¸ºå…¶æŽˆäºˆè§’è‰²ï¼ˆä¾‹å¦‚â€œBigQuery ç”¨æˆ·â€ï¼‰ä¸ºæœåŠ¡å¸å·æŽˆäºˆæƒé™ï¼Œä»Žè€Œå…è®¸æ‚¨è¿è¡ŒæŸ¥è¯¢ã€‚

6. åœ¨é€‰æ‹©æœåŠ¡å¸å·åŽï¼Œé€‰æ‹©â€œJSONâ€å¯†é’¥ç±»åž‹ï¼Œç„¶åŽå•å‡»â€œåˆ›å»ºâ€æŒ‰é’®ã€‚è¿™å°†ä¸‹è½½åŒ…å«æ‚¨å‡­æ®çš„ JSON å¯†é’¥æ–‡ä»¶åˆ°æ‚¨çš„è®¡ç®—æœºã€‚å®ƒçš„å¤–è§‚ç±»ä¼¼äºŽä»¥ä¸‹å†…å®¹ï¼š

```json
{
	"type": "service_account",
	"project_id": "your project",
	"private_key_id": "your private key id",
	"private_key": "private key",
	"client_email": "email",
	"client_id": "client id",
	"auth_uri": "https://accounts.google.com/o/oauth2/auth",
	"token_uri": "https://accounts.google.com/o/oauth2/token",
	"auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
	"client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/email_id"
}
```

## ä½¿ç”¨ BigQuery å®¢æˆ·ç«¯

èŽ·å¾—å‡­æ®åŽï¼Œæ‚¨éœ€è¦ä½¿ç”¨ BigQuery Python å®¢æˆ·ç«¯ä½¿ç”¨æ‚¨çš„å‡­æ®è¿›è¡Œèº«ä»½éªŒè¯ã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£… BigQuery Python å®¢æˆ·ç«¯ï¼š

```bash
pip install google-cloud-bigquery[pandas]
```

æ‚¨ä¼šæ³¨æ„åˆ°æˆ‘ä»¬å·²å®‰è£…äº† pandas æ’ä»¶ï¼Œè¿™å¯¹äºŽå°† BigQuery æ•°æ®é›†å¤„ç†ä¸º pandas æ•°æ®å¸§å°†éžå¸¸æœ‰ç”¨ã€‚å®‰è£…äº†å®¢æˆ·ç«¯ä¹‹åŽï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹ä»£ç ä½¿ç”¨æ‚¨çš„å‡­æ®è¿›è¡Œèº«ä»½éªŒè¯ï¼š

```py
from google.cloud import bigquery

client = bigquery.Client.from_service_account_json("path/to/key.json")
```

å®Œæˆå‡­æ®èº«ä»½éªŒè¯åŽï¼Œæ‚¨çŽ°åœ¨å¯ä»¥ä½¿ç”¨ BigQuery Python å®¢æˆ·ç«¯ä¸Žæ‚¨çš„ BigQuery æ•°æ®é›†è¿›è¡Œäº¤äº’ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹å‡½æ•°ï¼Œè¯¥å‡½æ•°åœ¨ BigQuery ä¸­æŸ¥è¯¢ `covid19_nyt.us_counties` æ•°æ®é›†ï¼Œä»¥æ˜¾ç¤ºæˆªè‡³å½“å‰æ—¥æœŸçš„ç¡®è¯Šäººæ•°æœ€å¤šçš„å‰ 20 ä¸ªåŽ¿ï¼š

```py
import numpy as np

QUERY = (
    'SELECT * FROM `bigquery-public-data.covid19_nyt.us_counties` '
    'ORDER BY date DESC,confirmed_cases DESC '
    'LIMIT 20')

def run_query():
    query_job = client.query(QUERY)
    query_result = query_job.result()
    df = query_result.to_dataframe()
    # Select a subset of columns
    df = df[["confirmed_cases", "deaths", "county", "state_name"]]
    # Convert numeric columns to standard numpy types
    df = df.astype({"deaths": np.int64, "confirmed_cases": np.int64})
    return df
```

## æž„å»ºå®žæ—¶ä»ªè¡¨ç›˜

ä¸€æ—¦æ‚¨æœ‰äº†æŸ¥è¯¢æ•°æ®çš„å‡½æ•°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Gradio åº“çš„ `gr.DataFrame` ç»„ä»¶ä»¥è¡¨æ ¼å½¢å¼æ˜¾ç¤ºç»“æžœã€‚è¿™æ˜¯ä¸€ç§æ£€æŸ¥æ•°æ®å¹¶ç¡®ä¿æŸ¥è¯¢æ­£ç¡®çš„æœ‰ç”¨æ–¹å¼ã€‚

ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨ `gr.DataFrame` ç»„ä»¶æ˜¾ç¤ºç»“æžœçš„ç¤ºä¾‹ã€‚é€šè¿‡å°† `run_query` å‡½æ•°ä¼ é€’ç»™ `gr.DataFrame`ï¼Œæˆ‘ä»¬æŒ‡ç¤º Gradio åœ¨é¡µé¢åŠ è½½æ—¶ç«‹å³è¿è¡Œè¯¥å‡½æ•°å¹¶æ˜¾ç¤ºç»“æžœã€‚æ­¤å¤–ï¼Œæ‚¨è¿˜å¯ä»¥ä¼ é€’å…³é”®å­— `every`ï¼Œä»¥å‘ŠçŸ¥ä»ªè¡¨æ¿æ¯å°æ—¶åˆ·æ–°ä¸€æ¬¡ï¼ˆ60\*60 ç§’ï¼‰ã€‚

```py
import gradio as gr

with gr.Blocks() as demo:
    gr.DataFrame(run_query, every=gr.Timer(60*60))

demo.queue().launch()  # Run the demo using queuing
```

ä¹Ÿè®¸æ‚¨æƒ³åœ¨æˆ‘ä»¬çš„ä»ªè¡¨ç›˜ä¸­æ·»åŠ ä¸€ä¸ªå¯è§†åŒ–æ•ˆæžœã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `gr.ScatterPlot()` ç»„ä»¶å°†æ•°æ®å¯è§†åŒ–ä¸ºæ•£ç‚¹å›¾ã€‚è¿™å¯ä»¥è®©æ‚¨æŸ¥çœ‹æ•°æ®ä¸­ä¸åŒå˜é‡ï¼ˆä¾‹å¦‚ç—…ä¾‹æ•°å’Œæ­»äº¡æ•°ï¼‰ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å¯ç”¨äºŽæŽ¢ç´¢æ•°æ®å’ŒèŽ·å–è§è§£ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥å®žæ—¶å®Œæˆè¿™ä¸€æ“ä½œ
é€šè¿‡ä¼ é€’ `every` å‚æ•°ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•åœ¨æ˜¾ç¤ºæ•°æ®æ—¶ä½¿ç”¨ `gr.ScatterPlot` æ¥è¿›è¡Œå¯è§†åŒ–ã€‚

```py
import gradio as gr

with gr.Blocks() as demo:
    gr.Markdown("# ðŸ’‰ Covid Dashboard (Updated Hourly)")
    with gr.Row():
        gr.DataFrame(run_query, every=gr.Timer(60*60))
        gr.ScatterPlot(run_query, every=gr.Timer(60*60), x="confirmed_cases",
                        y="deaths", tooltip="county", width=500, height=500)

demo.queue().launch()  # Run the demo with queuing enabled
```

---

<!-- Source: guides/cn/05_tabular-data-science-and-plots/creating-a-dashboard-from-supabase-data.md -->
# ä»Ž Supabase æ•°æ®åˆ›å»ºä»ªè¡¨ç›˜

Tags: TABULAR, DASHBOARD, PLOTS

[Supabase](https://supabase.com/) æ˜¯ä¸€ä¸ªåŸºäºŽäº‘çš„å¼€æºåŽç«¯ï¼Œæä¾›äº† PostgreSQL æ•°æ®åº“ã€èº«ä»½éªŒè¯å’Œå…¶ä»–æœ‰ç”¨çš„åŠŸèƒ½ï¼Œç”¨äºŽæž„å»º Web å’Œç§»åŠ¨åº”ç”¨ç¨‹åºã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ä»Ž Supabase è¯»å–æ•°æ®ï¼Œå¹¶åœ¨ Gradio ä»ªè¡¨ç›˜ä¸Šä»¥**å®žæ—¶**æ–¹å¼ç»˜åˆ¶æ•°æ®ã€‚

**å…ˆå†³æ¡ä»¶ :** è¦å¼€å§‹ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªå…è´¹çš„ Supabase è´¦æˆ·ï¼Œæ‚¨å¯ä»¥åœ¨æ­¤å¤„æ³¨å†Œï¼š[https://app.supabase.com/](https://app.supabase.com/)

åœ¨è¿™ä¸ªç«¯åˆ°ç«¯æŒ‡å—ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ï¼š

- åœ¨ Supabase ä¸­åˆ›å»ºè¡¨
- ä½¿ç”¨ Supabase Python å®¢æˆ·ç«¯å‘ Supabase å†™å…¥æ•°æ®
- ä½¿ç”¨ Gradio åœ¨å®žæ—¶ä»ªè¡¨ç›˜ä¸­å¯è§†åŒ–æ•°æ®

å¦‚æžœæ‚¨å·²ç»åœ¨ Supabase ä¸Šæœ‰æ•°æ®æƒ³è¦åœ¨ä»ªè¡¨ç›˜ä¸­å¯è§†åŒ–ï¼Œæ‚¨å¯ä»¥è·³è¿‡å‰ä¸¤ä¸ªéƒ¨åˆ†ï¼Œç›´æŽ¥åˆ°[å¯è§†åŒ–æ•°æ®](#visualize-the-data-in-a-real-time-gradio-dashboard)ï¼

## åœ¨ Supabase ä¸­åˆ›å»ºè¡¨

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›è¦å¯è§†åŒ–çš„æ•°æ®ã€‚æ ¹æ®è¿™ä¸ª[å‡ºè‰²çš„æŒ‡å—](https://supabase.com/blog/loading-data-supabase-python)ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€äº›è™šå‡çš„å•†åŠ¡æ•°æ®ï¼Œå¹¶å°†å…¶æ”¾å…¥ Supabase ä¸­ã€‚

1\. åœ¨ Supabase ä¸­åˆ›å»ºä¸€ä¸ªæ–°é¡¹ç›®ã€‚ä¸€æ—¦æ‚¨ç™»å½•ï¼Œç‚¹å‡» "New Project" æŒ‰é’®

2\. ç»™æ‚¨çš„é¡¹ç›®å‘½åå¹¶è®¾ç½®æ•°æ®åº“å¯†ç ã€‚æ‚¨è¿˜å¯ä»¥é€‰æ‹©å®šä»·è®¡åˆ’ï¼ˆå¯¹äºŽæˆ‘ä»¬æ¥è¯´ï¼Œå…è´¹è®¡åˆ’å·²è¶³å¤Ÿï¼ï¼‰

3\. åœ¨æ•°æ®åº“å¯åŠ¨æ—¶ï¼ˆå¯èƒ½éœ€è¦å¤šè¾¾ 2 åˆ†é’Ÿï¼‰ï¼Œæ‚¨å°†çœ‹åˆ°æ‚¨çš„ API å¯†é’¥ã€‚

4\. åœ¨å·¦ä¾§çª—æ ¼ä¸­å•å‡» "Table Editor"ï¼ˆè¡¨å›¾æ ‡ï¼‰ä»¥åˆ›å»ºä¸€ä¸ªæ–°è¡¨ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåä¸º `Product` çš„å•è¡¨ï¼Œå…·æœ‰ä»¥ä¸‹æ¨¡å¼ï¼š

<center>
<table>
<tr><td>product_id</td><td>int8</td></tr>
<tr><td>inventory_count</td><td>int8</td></tr>
<tr><td>price</td><td>float8</td></tr>
<tr><td>product_name</td><td>varchar</td></tr>
</table>
</center>

5\. ç‚¹å‡»ä¿å­˜ä»¥ä¿å­˜è¡¨ç»“æž„ã€‚

æˆ‘ä»¬çš„è¡¨å·²ç»å‡†å¤‡å¥½äº†ï¼

## å°†æ•°æ®å†™å…¥ Supabase

ä¸‹ä¸€æ­¥æ˜¯å‘ Supabase æ•°æ®é›†ä¸­å†™å…¥æ•°æ®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Supabase Python åº“æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚

6\. é€šè¿‡åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£… `supabase` åº“ï¼š

```bash
pip install supabase
```

7\. èŽ·å–é¡¹ç›® URL å’Œ API å¯†é’¥ã€‚ç‚¹å‡»å·¦ä¾§çª—æ ¼ä¸Šçš„è®¾ç½®ï¼ˆé½¿è½®å›¾æ ‡ï¼‰ï¼Œç„¶åŽç‚¹å‡» 'API'ã€‚URL åˆ—åœ¨é¡¹ç›® URL æ¡†ä¸­ï¼ŒAPI å¯†é’¥åˆ—åœ¨é¡¹ç›® API å¯†é’¥ï¼ˆå¸¦æœ‰ `service_role`ã€`secret` æ ‡ç­¾ï¼‰ä¸­

8\. çŽ°åœ¨ï¼Œè¿è¡Œä»¥ä¸‹ Python è„šæœ¬å°†ä¸€äº›è™šå‡æ•°æ®å†™å…¥è¡¨ä¸­ï¼ˆæ³¨æ„æ‚¨éœ€è¦åœ¨æ­¥éª¤ 7 ä¸­æ”¾å…¥ `SUPABASE_URL` å’Œ `SUPABASE_SECRET_KEY` çš„å€¼ï¼‰ï¼š

```python
import supabase

# åˆå§‹åŒ–Supabaseå®¢æˆ·ç«¯
client = supabase.create_client('SUPABASE_URL', 'SUPABASE_SECRET_KEY')

# å®šä¹‰è¦å†™å…¥çš„æ•°æ®
import random

main_list = []
for i in range(10):
    value = {'product_id': i,
             'product_name': f"Item {i}",
             'inventory_count': random.randint(1, 100),
             'price': random.random()*100
            }
    main_list.append(value)

# å°†æ•°æ®å†™å…¥è¡¨ä¸­
data = client.table('Product').insert(main_list).execute()
```

è¿”å›ž Supabase ä»ªè¡¨æ¿å¹¶åˆ·æ–°é¡µé¢ï¼Œæ‚¨å°†çœ‹åˆ° 10 è¡Œæ•°æ®å¡«å……åˆ° `Product` è¡¨ä¸­ï¼

## åœ¨å®žæ—¶ Gradio ä»ªè¡¨ç›˜ä¸­å¯è§†åŒ–æ•°æ®

æœ€åŽï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„ `supabase` Python åº“ä»Ž Supabase æ•°æ®é›†ä¸­è¯»å–æ•°æ®ï¼Œå¹¶ä½¿ç”¨ `gradio` åˆ›å»ºä¸€ä¸ªå®žæ—¶ä»ªè¡¨ç›˜ã€‚

æ³¨æ„ï¼šæˆ‘ä»¬åœ¨æœ¬èŠ‚ä¸­é‡å¤äº†æŸäº›æ­¥éª¤ï¼ˆæ¯”å¦‚åˆ›å»º Supabase å®¢æˆ·ç«¯ï¼‰ï¼Œä»¥é˜²æ‚¨æ²¡æœ‰å®Œæˆä¹‹å‰çš„éƒ¨åˆ†ã€‚å¦‚ç¬¬ 7 æ­¥æ‰€è¿°ï¼Œæ‚¨å°†éœ€è¦æ•°æ®åº“çš„é¡¹ç›® URL å’Œ API å¯†é’¥ã€‚

9\. ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œä»Ž `Product` è¡¨åŠ è½½æ•°æ®å¹¶å°†å…¶ä½œä¸º pandas DataFrame è¿”å›žï¼š

import supabase

```python
import supabase
import pandas as pd

client = supabase.create_client('SUPABASE_URL', 'SUPABASE_SECRET_KEY')

def read_data():
    response = client.table('Product').select("*").execute()
    df = pd.DataFrame(response.data)
    return df
```

10\. ä½¿ç”¨ä¸¤ä¸ªæ¡å½¢å›¾åˆ›å»ºä¸€ä¸ªå°çš„ Gradio ä»ªè¡¨ç›˜ï¼Œæ¯åˆ†é’Ÿç»˜åˆ¶æ‰€æœ‰é¡¹ç›®çš„ä»·æ ¼å’Œåº“å­˜é‡ï¼Œå¹¶å®žæ—¶æ›´æ–°ï¼š

```python
import gradio as gr

with gr.Blocks() as dashboard:
    with gr.Row():
        gr.BarPlot(read_data, x="product_id", y="price", title="ä»·æ ¼", every=gr.Timer(60))
        gr.BarPlot(read_data, x="product_id", y="inventory_count", title="åº“å­˜", every=gr.Timer(60))

dashboard.queue().launch()
```

è¯·æ³¨æ„ï¼Œé€šè¿‡å°†å‡½æ•°ä¼ é€’ç»™ `gr.BarPlot()`ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç½‘ç»œåº”ç”¨åŠ è½½æ—¶æŸ¥è¯¢æ•°æ®åº“ï¼ˆç„¶åŽæ¯ 60 ç§’æŸ¥è¯¢ä¸€æ¬¡ï¼Œå› ä¸ºæœ‰ `every` å‚æ•°ï¼‰ã€‚æ‚¨çš„æœ€ç»ˆä»ªè¡¨ç›˜åº”å¦‚ä¸‹æ‰€ç¤ºï¼š

<gradio-app space="abidlabs/supabase"></gradio-app>

## ç»“è®º

å°±æ˜¯è¿™æ ·ï¼åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å­¦ä¹ äº†å¦‚ä½•å°†æ•°æ®å†™å…¥ Supabase æ•°æ®é›†ï¼Œç„¶åŽè¯»å–è¯¥æ•°æ®å¹¶å°†ç»“æžœç»˜åˆ¶ä¸ºæ¡å½¢å›¾ã€‚å¦‚æžœæ‚¨æ›´æ–° Supabase æ•°æ®åº“ä¸­çš„æ•°æ®ï¼Œæ‚¨ä¼šæ³¨æ„åˆ° Gradio ä»ªè¡¨ç›˜å°†åœ¨ä¸€åˆ†é’Ÿå†…æ›´æ–°ã€‚

å°è¯•åœ¨æ­¤ç¤ºä¾‹ä¸­æ·»åŠ æ›´å¤šç»˜å›¾å’Œå¯è§†åŒ–ï¼ˆæˆ–ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†ï¼‰ï¼Œä»¥æž„å»ºä¸€ä¸ªæ›´å¤æ‚çš„ä»ªè¡¨ç›˜ï¼

---

<!-- Source: guides/cn/05_tabular-data-science-and-plots/creating-a-realtime-dashboard-from-google-sheets.md -->
# ä»Ž Google Sheets åˆ›å»ºå®žæ—¶ä»ªè¡¨ç›˜

Tags: TABULAR, DASHBOARD, PLOTS
[Google Sheets](https://www.google.com/sheets/about/) æ˜¯ä¸€ç§ä»¥ç”µå­è¡¨æ ¼å½¢å¼å­˜å‚¨è¡¨æ ¼æ•°æ®çš„ç®€ä¾¿æ–¹æ³•ã€‚å€ŸåŠ© Gradio å’Œ pandasï¼Œå¯ä»¥è½»æ¾ä»Žå…¬å…±æˆ–ç§æœ‰ Google Sheets è¯»å–æ•°æ®ï¼Œç„¶åŽæ˜¾ç¤ºæ•°æ®æˆ–ç»˜åˆ¶æ•°æ®ã€‚åœ¨æœ¬åšæ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æž„å»ºä¸€ä¸ªå°åž‹ _real-time_ ä»ªè¡¨ç›˜ï¼Œè¯¥ä»ªè¡¨ç›˜åœ¨ Google Sheets ä¸­çš„æ•°æ®æ›´æ–°æ—¶è¿›è¡Œæ›´æ–°ã€‚
æž„å»ºä»ªè¡¨ç›˜æœ¬èº«åªéœ€è¦ä½¿ç”¨ Gradio çš„ 9 è¡Œ Python ä»£ç ï¼Œæˆ‘ä»¬çš„æœ€ç»ˆä»ªè¡¨ç›˜å¦‚ä¸‹æ‰€ç¤ºï¼š
<gradio-app space="gradio/line-plot"></gradio-app>

**å…ˆå†³æ¡ä»¶**ï¼šæœ¬æŒ‡å—ä½¿ç”¨[Gradio Blocks](../quickstart/#blocks-more-flexibility-and-control)ï¼Œå› æ­¤è¯·ç¡®ä¿æ‚¨ç†Ÿæ‚‰ Blocks ç±»ã€‚
å…·ä½“æ­¥éª¤ç•¥æœ‰ä¸åŒï¼Œå…·ä½“å–å†³äºŽæ‚¨æ˜¯ä½¿ç”¨å…¬å¼€è®¿é—®è¿˜æ˜¯ç§æœ‰ Google Sheetã€‚æˆ‘ä»¬å°†åˆ†åˆ«ä»‹ç»è¿™ä¸¤ç§æƒ…å†µï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹å§ï¼

## Public Google Sheets

ç”±äºŽ[`pandas` åº“](https://pandas.pydata.org/)çš„å­˜åœ¨ï¼Œä»Žå…¬å…± Google Sheet æž„å»ºä»ªè¡¨ç›˜éžå¸¸ç®€å•ï¼š

1. èŽ·å–è¦ä½¿ç”¨çš„ Google Sheets çš„ç½‘å€ã€‚ä¸ºæ­¤ï¼Œåªéœ€è¿›å…¥ Google Sheetsï¼Œå•å‡»å³ä¸Šè§’çš„â€œå…±äº«â€æŒ‰é’®ï¼Œç„¶åŽå•å‡»â€œèŽ·å–å¯å…±äº«é“¾æŽ¥â€æŒ‰é’®ã€‚è¿™å°†ç»™æ‚¨ä¸€ä¸ªç±»ä¼¼äºŽä»¥ä¸‹ç¤ºä¾‹çš„ç½‘å€ï¼š

```html
https://docs.google.com/spreadsheets/d/1UoKzzRzOCt-FXLLqDKLbryEKEgllGAQUEJ5qtmmQwpU/edit#gid=0
```

2. çŽ°åœ¨ï¼Œä¿®æ”¹æ­¤ç½‘å€å¹¶ä½¿ç”¨å®ƒä»Ž Google Sheets è¯»å–æ•°æ®åˆ° Pandas DataFrame ä¸­ã€‚ (åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œç”¨æ‚¨çš„å…¬å¼€ Google Sheet çš„ç½‘å€æ›¿æ¢ `URL` å˜é‡)ï¼š

```python
import pandas as pd
URL = "https://docs.google.com/spreadsheets/d/1UoKzzRzOCt-FXLLqDKLbryEKEgllGAQUEJ5qtmmQwpU/edit#gid=0"csv_url = URL.replace('/edit#gid=', '/export?format=csv&gid=')
def get_data():
    return pd.read_csv(csv_url)
```

3. æ•°æ®æŸ¥è¯¢æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè¿™æ„å‘³ç€å¯ä»¥ä½¿ç”¨ `gr.DataFrame` ç»„ä»¶å®žæ—¶æ˜¾ç¤ºæˆ–ä½¿ç”¨ `gr.LinePlot` ç»„ä»¶å®žæ—¶ç»˜åˆ¶æ•°æ®ï¼ˆå½“ç„¶ï¼Œæ ¹æ®æ•°æ®çš„ä¸åŒï¼Œå¯èƒ½éœ€è¦ä¸åŒçš„ç»˜å›¾æ–¹æ³•ï¼‰ã€‚åªéœ€å°†å‡½æ•°ä¼ é€’ç»™ç›¸åº”çš„ç»„ä»¶ï¼Œå¹¶æ ¹æ®ç»„ä»¶åˆ·æ–°çš„é¢‘çŽ‡ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼‰è®¾ç½® `every` å‚æ•°ã€‚ä»¥ä¸‹æ˜¯ Gradio ä»£ç ï¼š

```python
import gradio as gr

with gr.Blocks() as demo:
    gr.Markdown("# ðŸ“ˆ Real-Time Line Plot")
    with gr.Row():
        with gr.Column():
            gr.DataFrame(get_data, every=gr.Timer(5))
        with gr.Column():
            gr.LinePlot(get_data, every=gr.Timer(5), x="Date", y="Sales", y_title="Sales ($ millions)", overlay_point=True, width=500, height=500)

demo.queue().launch()  # Run the demo with queuing enabled
```

åˆ°æ­¤ä¸ºæ­¢ï¼æ‚¨çŽ°åœ¨æ‹¥æœ‰ä¸€ä¸ªä»ªè¡¨ç›˜ï¼Œæ¯ 5 ç§’åˆ·æ–°ä¸€æ¬¡ï¼Œä»Ž Google Sheets ä¸­èŽ·å–æ•°æ®ã€‚

## ç§æœ‰ Google Sheets

å¯¹äºŽç§æœ‰ Google Sheetsï¼Œæµç¨‹éœ€è¦æ›´å¤šçš„å·¥ä½œé‡ï¼Œä½†å¹¶ä¸å¤šï¼å…³é”®åŒºåˆ«åœ¨äºŽï¼ŒçŽ°åœ¨æ‚¨å¿…é¡»ç»è¿‡èº«ä»½éªŒè¯ï¼Œä»¥æŽˆæƒè®¿é—®ç§æœ‰ Google Sheetsã€‚

### èº«ä»½éªŒè¯

è¦è¿›è¡Œèº«ä»½éªŒè¯ï¼Œéœ€ä»Ž Google Cloud èŽ·å–å‡­æ®ã€‚ä»¥ä¸‹æ˜¯[å¦‚ä½•è®¾ç½® Google Cloud å‡­æ®](https://developers.google.com/workspace/guides/create-credentials)ï¼š

1. é¦–å…ˆï¼Œç™»å½•æ‚¨çš„ Google Cloud å¸æˆ·å¹¶è½¬åˆ° Google Cloud æŽ§åˆ¶å°ï¼ˆhttps://console.cloud.google.com/ï¼‰
2. åœ¨ Cloud æŽ§åˆ¶å°ä¸­ï¼Œå•å‡»å·¦ä¸Šè§’çš„æ±‰å ¡èœå•ï¼Œç„¶åŽä»Žèœå•ä¸­é€‰æ‹©â€œAPI å’ŒæœåŠ¡â€ã€‚å¦‚æžœæ‚¨æ²¡æœ‰çŽ°æœ‰é¡¹ç›®ï¼Œåˆ™éœ€è¦åˆ›å»ºä¸€ä¸ªã€‚
3. ç„¶åŽï¼Œç‚¹å‡»â€œ+ å¯ç”¨çš„ API å’ŒæœåŠ¡â€æŒ‰é’®ï¼Œå…è®¸æ‚¨ä¸ºé¡¹ç›®å¯ç”¨ç‰¹å®šçš„æœåŠ¡ã€‚æœç´¢â€œGoogle Sheets APIâ€ï¼Œç‚¹å‡»å®ƒï¼Œç„¶åŽå•å‡»â€œå¯ç”¨â€æŒ‰é’®ã€‚å¦‚æžœçœ‹åˆ°â€œç®¡ç†â€æŒ‰é’®ï¼Œåˆ™è¡¨ç¤º Google Sheets å·²å¯ç”¨ï¼Œå¹¶ä¸”æ‚¨å·²å‡†å¤‡å°±ç»ªã€‚
4. åœ¨ API å’ŒæœåŠ¡èœå•ä¸­ï¼Œç‚¹å‡»â€œå‡­æ®â€é€‰é¡¹å¡ï¼Œç„¶åŽç‚¹å‡»â€œåˆ›å»ºå‡­æ®â€æŒ‰é’®ã€‚
5. åœ¨â€œåˆ›å»ºå‡­æ®â€å¯¹è¯æ¡†ä¸­ï¼Œé€‰æ‹©â€œæœåŠ¡å¸å·å¯†é’¥â€ä½œä¸ºè¦åˆ›å»ºçš„å‡­æ®ç±»åž‹ï¼Œå¹¶ä¸ºå…¶å‘½åã€‚**è®°ä¸‹æœåŠ¡å¸å·çš„ç”µå­é‚®ä»¶åœ°å€**
6. åœ¨é€‰æ‹©æœåŠ¡å¸å·ä¹‹åŽï¼Œé€‰æ‹©â€œJSONâ€å¯†é’¥ç±»åž‹ï¼Œç„¶åŽç‚¹å‡»â€œåˆ›å»ºâ€æŒ‰é’®ã€‚è¿™å°†ä¸‹è½½åŒ…å«æ‚¨å‡­æ®çš„ JSON å¯†é’¥æ–‡ä»¶åˆ°æ‚¨çš„è®¡ç®—æœºã€‚æ–‡ä»¶ç±»ä¼¼äºŽä»¥ä¸‹ç¤ºä¾‹ï¼š

```json
{
	"type": "service_account",
	"project_id": "your project",
	"private_key_id": "your private key id",
	"private_key": "private key",
	"client_email": "email",
	"client_id": "client id",
	"auth_uri": "https://accounts.google.com/o/oauth2/auth",
	"token_uri": "https://accounts.google.com/o/oauth2/token",
	"auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
	"client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/email_id"
}
```

### æŸ¥è¯¢

åœ¨èŽ·å¾—å‡­æ®çš„ `.json` æ–‡ä»¶åŽï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æŸ¥è¯¢æ‚¨çš„ Google Sheetï¼š

1. å•å‡» Google Sheet å³ä¸Šè§’çš„â€œå…±äº«â€æŒ‰é’®ã€‚ä½¿ç”¨èº«ä»½éªŒè¯å­éƒ¨åˆ†ç¬¬ 5 æ­¥çš„æœåŠ¡çš„ç”µå­é‚®ä»¶åœ°å€å…±äº« Google Sheetsï¼ˆæ­¤æ­¥éª¤å¾ˆé‡è¦ï¼ï¼‰ã€‚ç„¶åŽå•å‡»â€œèŽ·å–å¯å…±äº«é“¾æŽ¥â€æŒ‰é’®ã€‚è¿™å°†ç»™æ‚¨ä¸€ä¸ªç±»ä¼¼äºŽä»¥ä¸‹ç¤ºä¾‹çš„ç½‘å€ï¼š

```html
https://docs.google.com/spreadsheets/d/1UoKzzRzOCt-FXLLqDKLbryEKEgllGAQUEJ5qtmmQwpU/edit#gid=0
```

2. å®‰è£… [`gspread` åº“](https://docs.gspread.org/en/v5.7.0/)ï¼Œé€šè¿‡åœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤ä½¿ Python ä¸­ä½¿ç”¨ [Google Sheets API](https://developers.google.com/sheets/api/guides/concepts) æ›´åŠ ç®€å•ï¼š`pip install gspread`
3. ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥ä»Ž Google Sheet ä¸­åŠ è½½æ•°æ®ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼ˆç”¨æ‚¨çš„ç§æœ‰ Google Sheet çš„ URL æ›¿æ¢ `URL` å˜é‡ï¼‰ï¼š

```python
import gspreadimport pandas as pd
# ä¸Ž Google è¿›è¡Œèº«ä»½éªŒè¯å¹¶èŽ·å–è¡¨æ ¼URL = 'https://docs.google.com/spreadsheets/d/1_91Vps76SKOdDQ8cFxZQdgjTJiz23375sAT7vPvaj4k/edit#gid=0'
gc = gspread.service_account("path/to/key.json")sh = gc.open_by_url(URL)worksheet = sh.sheet1
def get_data():
    values = worksheet.get_all_values()
    df = pd.DataFrame(values[1:], columns=values[0])
    return df
```

4\. æ•°æ®æŸ¥è¯¢æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè¿™æ„å‘³ç€å¯ä»¥ä½¿ç”¨ `gr.DataFrame` ç»„ä»¶å®žæ—¶æ˜¾ç¤ºæ•°æ®ï¼Œæˆ–ä½¿ç”¨ `gr.LinePlot` ç»„ä»¶å®žæ—¶ç»˜åˆ¶æ•°æ®ï¼ˆå½“ç„¶ï¼Œæ ¹æ®æ•°æ®çš„ä¸åŒï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨ä¸åŒçš„å›¾è¡¨ï¼‰ã€‚è¦å®žçŽ°è¿™ä¸€ç‚¹ï¼Œåªéœ€å°†å‡½æ•°ä¼ é€’ç»™ç›¸åº”çš„ç»„ä»¶ï¼Œå¹¶æ ¹æ®éœ€è¦è®¾ç½® `every` å‚æ•°æ¥ç¡®å®šç»„ä»¶åˆ·æ–°çš„é¢‘çŽ‡ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼‰ã€‚ä»¥ä¸‹æ˜¯ Gradio ä»£ç ï¼š

```python
import gradio as gr

with gr.Blocks() as demo:
    gr.Markdown("# ðŸ“ˆ å®žæ—¶æŠ˜çº¿å›¾")
    with gr.Row():
        with gr.Column():
            gr.DataFrame(get_data, every=gr.Timer(5))
        with gr.Column():
            gr.LinePlot(get_data, every=gr.Timer(5), x="æ—¥æœŸ", y="é”€å”®é¢", y_title="é”€å”®é¢ï¼ˆç™¾ä¸‡ç¾Žå…ƒï¼‰", overlay_point=True, width=500, height=500)

demo.queue().launch()  # å¯åŠ¨å¸¦æœ‰æŽ’é˜ŸåŠŸèƒ½çš„æ¼”ç¤º
```

çŽ°åœ¨ä½ æœ‰ä¸€ä¸ªæ¯ 5 ç§’åˆ·æ–°ä¸€æ¬¡çš„ä»ªè¡¨ç›˜ï¼Œå¯ä»¥ä»Žä½ çš„ Google è¡¨æ ¼ä¸­èŽ·å–æ•°æ®ã€‚

## ç»“è®º

å°±æ˜¯è¿™æ ·ï¼åªéœ€å‡ è¡Œä»£ç ï¼Œä½ å°±å¯ä»¥ä½¿ç”¨ `gradio` å’Œå…¶ä»–åº“ä»Žå…¬å…±æˆ–ç§æœ‰çš„ Google è¡¨æ ¼ä¸­è¯»å–æ•°æ®ï¼Œç„¶åŽåœ¨å®žæ—¶ä»ªè¡¨ç›˜ä¸­æ˜¾ç¤ºå’Œç»˜åˆ¶æ•°æ®ã€‚

---

<!-- Source: guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md -->
# å¦‚ä½•ä½¿ç”¨åœ°å›¾ç»„ä»¶ç»˜åˆ¶å›¾è¡¨

Related spaces:
Tags: PLOTS, MAPS

## ç®€ä»‹

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ Gradio çš„ `Plot` ç»„ä»¶åœ¨åœ°å›¾ä¸Šç»˜åˆ¶åœ°ç†æ•°æ®ã€‚Gradio çš„ `Plot` ç»„ä»¶å¯ä»¥ä¸Ž Matplotlibã€Bokeh å’Œ Plotly ä¸€èµ·ä½¿ç”¨ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Plotly è¿›è¡Œæ“ä½œã€‚Plotly å¯ä»¥è®©å¼€å‘äººå‘˜è½»æ¾åˆ›å»ºå„ç§åœ°å›¾æ¥å±•ç¤ºä»–ä»¬çš„åœ°ç†æ•°æ®ã€‚ç‚¹å‡»[è¿™é‡Œ](https://plotly.com/python/maps/)æŸ¥çœ‹ä¸€äº›ç¤ºä¾‹ã€‚

## æ¦‚è¿°

æˆ‘ä»¬å°†ä½¿ç”¨çº½çº¦å¸‚çš„ Airbnb æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ‰˜ç®¡åœ¨ kaggle ä¸Šï¼Œç‚¹å‡»[è¿™é‡Œ](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)ã€‚æˆ‘å·²ç»å°†å…¶ä¸Šä¼ åˆ° Hugging Face Hub ä½œä¸ºä¸€ä¸ªæ•°æ®é›†ï¼Œæ–¹ä¾¿ä½¿ç”¨å’Œä¸‹è½½ï¼Œç‚¹å‡»[è¿™é‡Œ](https://huggingface.co/datasets/gradio/NYC-Airbnb-Open-Data)ã€‚ä½¿ç”¨è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬å°†åœ¨åœ°å›¾ä¸Šç»˜åˆ¶ Airbnb çš„ä½ç½®ï¼Œå¹¶å…è®¸åŸºäºŽä»·æ ¼å’Œä½ç½®è¿›è¡Œç­›é€‰ã€‚ä¸‹é¢æ˜¯æˆ‘ä»¬å°†è¦æž„å»ºçš„æ¼”ç¤ºã€‚ âš¡ï¸

$demo_map_airbnb

## æ­¥éª¤ 1-åŠ è½½ CSV æ•°æ® ðŸ’¾

è®©æˆ‘ä»¬é¦–å…ˆä»Ž Hugging Face Hub åŠ è½½çº½çº¦å¸‚çš„ Airbnb æ•°æ®ã€‚

```python
from datasets import load_dataset

dataset = load_dataset("gradio/NYC-Airbnb-Open-Data", split="train")
df = dataset.to_pandas()

def filter_map(min_price, max_price, boroughs):
    new_df = df[(df['neighbourhood_group'].isin(boroughs)) &
            (df['price'] > min_price) & (df['price'] < max_price)]
    names = new_df["name"].tolist()
    prices = new_df["price"].tolist()
    text_list = [(names[i], prices[i]) for i in range(0, len(names))]
```

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å…ˆå°† CSV æ•°æ®åŠ è½½åˆ°ä¸€ä¸ª pandas dataframe ä¸­ã€‚è®©æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¿™å°†ä½œä¸º gradio åº”ç”¨ç¨‹åºçš„é¢„æµ‹å‡½æ•°ã€‚è¯¥å‡½æ•°å°†æŽ¥å—æœ€ä½Žä»·æ ¼ã€æœ€é«˜ä»·æ ¼èŒƒå›´å’Œç­›é€‰ç»“æžœåœ°åŒºçš„åˆ—è¡¨ä½œä¸ºå‚æ•°ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¼ å…¥çš„å€¼ (`min_price`ã€`max_price` å’Œåœ°åŒºåˆ—è¡¨) æ¥ç­›é€‰æ•°æ®æ¡†å¹¶åˆ›å»º `new_df`ã€‚æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ›å»ºåŒ…å«æ¯ä¸ª Airbnb çš„åç§°å’Œä»·æ ¼çš„ `text_list`ï¼Œä»¥ä¾¿åœ¨åœ°å›¾ä¸Šä½¿ç”¨ä½œä¸ºæ ‡ç­¾ã€‚

## æ­¥éª¤ 2-åœ°å›¾å›¾è¡¨ ðŸŒ

Plotly ä½¿å¾—å¤„ç†åœ°å›¾å˜å¾—å¾ˆå®¹æ˜“ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä¸‹é¢çš„ä»£ç ï¼Œäº†è§£å¦‚ä½•åˆ›å»ºåœ°å›¾å›¾è¡¨ã€‚

```python
import plotly.graph_objects as go

fig = go.Figure(go.Scattermapbox(
            customdata=text_list,
            lat=new_df['latitude'].tolist(),
            lon=new_df['longitude'].tolist(),
            mode='markers',
            marker=go.scattermapbox.Marker(
                size=6
            ),
            hoverinfo="text",
            hovertemplate='<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}'
        ))

fig.update_layout(
    mapbox_style="open-street-map",
    hovermode='closest',
    mapbox=dict(
        bearing=0,
        center=go.layout.mapbox.Center(
            lat=40.67,
            lon=-73.90
        ),
        pitch=0,
        zoom=9
    ),
)
```

ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¼ å…¥ç»çº¬åº¦åˆ—è¡¨æ¥åˆ›å»ºä¸€ä¸ªæ•£ç‚¹å›¾ã€‚æˆ‘ä»¬è¿˜ä¼ å…¥äº†åç§°å’Œä»·æ ¼çš„è‡ªå®šä¹‰æ•°æ®ï¼Œä»¥ä¾¿åœ¨é¼ æ ‡æ‚¬åœåœ¨æ¯ä¸ªæ ‡è®°ä¸Šæ—¶æ˜¾ç¤ºé¢å¤–çš„ä¿¡æ¯ã€‚æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ `update_layout` æ¥æŒ‡å®šå…¶ä»–åœ°å›¾è®¾ç½®ï¼Œä¾‹å¦‚ç¼©æ”¾å’Œå±…ä¸­ã€‚

æœ‰å…³ä½¿ç”¨ Mapbox å’Œ Plotly åˆ›å»ºæ•£ç‚¹å›¾çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·ç‚¹å‡»[è¿™é‡Œ](https://plotly.com/python/scattermapbox/)ã€‚

## æ­¥éª¤ 3-Gradio åº”ç”¨ç¨‹åº âš¡ï¸

æˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ª `gr.Number` ç»„ä»¶å’Œä¸€ä¸ª `gr.CheckboxGroup` ç»„ä»¶ï¼Œå…è®¸ç”¨æˆ·æŒ‡å®šä»·æ ¼èŒƒå›´å’Œåœ°åŒºä½ç½®ã€‚ç„¶åŽï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `gr.Plot` ç»„ä»¶ä½œä¸ºæˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ Plotly + Mapbox åœ°å›¾çš„è¾“å‡ºã€‚

```python
with gr.Blocks() as demo:
    with gr.Column():
        with gr.Row():
            min_price = gr.Number(value=250, label="Minimum Price")
            max_price = gr.Number(value=1000, label="Maximum Price")
        boroughs = gr.CheckboxGroup(choices=["Queens", "Brooklyn", "Manhattan", "Bronx", "Staten Island"], value=["Queens", "Brooklyn"], label="Select Boroughs:")
        btn = gr.Button(value="Update Filter")
        map = gr.Plot()
    demo.load(filter_map, [min_price, max_price, boroughs], map)
    btn.click(filter_map, [min_price, max_price, boroughs], map)
```

æˆ‘ä»¬ä½¿ç”¨ `gr.Column` å’Œ `gr.Row` å¸ƒå±€è¿™äº›ç»„ä»¶ï¼Œå¹¶ä¸ºæ¼”ç¤ºåŠ è½½æ—¶å’Œç‚¹å‡» " æ›´æ–°ç­›é€‰ " æŒ‰é’®æ—¶æ·»åŠ äº†äº‹ä»¶è§¦å‘å™¨ï¼Œä»¥è§¦å‘åœ°å›¾æ›´æ–°æ–°çš„ç­›é€‰æ¡ä»¶ã€‚

ä»¥ä¸‹æ˜¯å®Œæ•´æ¼”ç¤ºä»£ç ï¼š

$code_map_airbnb

## æ­¥éª¤ 4-éƒ¨ç½² Deployment ðŸ¤—

å¦‚æžœä½ è¿è¡Œä¸Šé¢çš„ä»£ç ï¼Œä½ çš„åº”ç”¨ç¨‹åºå°†åœ¨æœ¬åœ°è¿è¡Œã€‚
å¦‚æžœè¦èŽ·å–ä¸´æ—¶å…±äº«é“¾æŽ¥ï¼Œå¯ä»¥å°† `share=True` å‚æ•°ä¼ é€’ç»™ `launch`ã€‚

ä½†å¦‚æžœä½ æƒ³è¦ä¸€ä¸ªæ°¸ä¹…çš„éƒ¨ç½²è§£å†³æ–¹æ¡ˆå‘¢ï¼Ÿ
è®©æˆ‘ä»¬å°†æˆ‘ä»¬çš„ Gradio åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ°å…è´¹çš„ HuggingFace Spaces å¹³å°ã€‚

å¦‚æžœä½ ä»¥å‰æ²¡æœ‰ä½¿ç”¨è¿‡ Spacesï¼Œè¯·æŒ‰ç…§ä¹‹å‰çš„æŒ‡å—[è¿™é‡Œ](/using_hugging_face_integrations)ã€‚

## ç»“è®º ðŸŽ‰

ä½ å·²ç»å®Œæˆäº†ï¼è¿™æ˜¯æž„å»ºåœ°å›¾æ¼”ç¤ºæ‰€éœ€çš„æ‰€æœ‰ä»£ç ã€‚

é“¾æŽ¥åˆ°æ¼”ç¤ºï¼š[åœ°å›¾æ¼”ç¤º](https://huggingface.co/spaces/gradio/map_airbnb)å’Œ[å®Œæ•´ä»£ç ](https://huggingface.co/spaces/gradio/map_airbnb/blob/main/run.py)ï¼ˆåœ¨ Hugging Face Spacesï¼‰

---

<!-- Source: guides/cn/05_tabular-data-science-and-plots/using-gradio-for-tabular-workflows.md -->
## ä½¿ç”¨ Gradio è¿›è¡Œè¡¨æ ¼æ•°æ®ç§‘å­¦å·¥ä½œæµ

Related spaces: https://huggingface.co/spaces/scikit-learn/gradio-skops-integrationï¼Œhttps://huggingface.co/spaces/scikit-learn/tabular-playgroundï¼Œhttps://huggingface.co/spaces/merve/gradio-analysis-dashboard

## ä»‹ç»

è¡¨æ ¼æ•°æ®ç§‘å­¦æ˜¯æœºå™¨å­¦ä¹ ä¸­åº”ç”¨æœ€å¹¿æ³›çš„é¢†åŸŸï¼Œæ¶‰åŠçš„é—®é¢˜ä»Žå®¢æˆ·åˆ†å‰²åˆ°æµå¤±é¢„æµ‹ä¸ç­‰ã€‚åœ¨è¡¨æ ¼æ•°æ®ç§‘å­¦å·¥ä½œæµçš„å„ä¸ªé˜¶æ®µä¸­ï¼Œå°†å·¥ä½œå†…å®¹ä¼ è¾¾ç»™åˆ©ç›Šç›¸å…³è€…æˆ–å®¢æˆ·å¯èƒ½å¾ˆéº»çƒ¦ï¼Œè¿™ä¼šé˜»ç¢æ•°æ®ç§‘å­¦å®¶ä¸“æ³¨äºŽé‡è¦äº‹é¡¹ï¼Œå¦‚æ•°æ®åˆ†æžå’Œæ¨¡åž‹æž„å»ºã€‚æ•°æ®ç§‘å­¦å®¶å¯èƒ½ä¼šèŠ±è´¹æ•°å°æ—¶æž„å»ºä¸€ä¸ªæŽ¥å— DataFrame å¹¶è¿”å›žå›¾è¡¨ã€é¢„æµ‹æˆ–æ•°æ®é›†ä¸­çš„èšç±»å›¾çš„ä»ªè¡¨æ¿ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨ `gradio` æ”¹è¿›æ‚¨çš„æ•°æ®ç§‘å­¦å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬è¿˜å°†è®¨è®ºå¦‚ä½•ä½¿ç”¨ `gradio` å’Œ[skops](https://skops.readthedocs.io/en/stable/)ä¸€è¡Œä»£ç å³å¯æž„å»ºç•Œé¢ï¼

### å…ˆå†³æ¡ä»¶

ç¡®ä¿æ‚¨å·²ç»[å®‰è£…](/getting_started)äº† `gradio` Python è½¯ä»¶åŒ…ã€‚

## è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„ç•Œé¢ï¼

æˆ‘ä»¬å°†çœ‹ä¸€ä¸‹å¦‚ä½•åˆ›å»ºä¸€ä¸ªç®€å•çš„ç•Œé¢ï¼Œè¯¥ç•Œé¢æ ¹æ®äº§å“ä¿¡æ¯é¢„æµ‹æ•…éšœã€‚

```python
import gradio as gr
import pandas as pd
import joblib
import datasets


inputs = [gr.Dataframe(row_count = (2, "dynamic"), col_count=(4,"dynamic"), label="Input Data", interactive=1)]

outputs = [gr.Dataframe(row_count = (2, "dynamic"), col_count=(1, "fixed"), label="Predictions", headers=["Failures"])]

model = joblib.load("model.pkl")

# we will give our dataframe as example
df = datasets.load_dataset("merve/supersoaker-failures")
df = df["train"].to_pandas()

def infer(input_dataframe):
  return pd.DataFrame(model.predict(input_dataframe))

gr.Interface(fn = infer, inputs = inputs, outputs = outputs, examples = [[df.head(2)]]).launch()
```

è®©æˆ‘ä»¬æ¥è§£æžä¸Šè¿°ä»£ç ã€‚

- `fn`ï¼šæŽ¨ç†å‡½æ•°ï¼ŒæŽ¥å—è¾“å…¥æ•°æ®å¸§å¹¶è¿”å›žé¢„æµ‹ç»“æžœã€‚
- `inputs`ï¼šæˆ‘ä»¬ä½¿ç”¨ `Dataframe` ç»„ä»¶ä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬å°†è¾“å…¥å®šä¹‰ä¸ºå…·æœ‰ 2 è¡Œ 4 åˆ—çš„æ•°æ®å¸§ï¼Œæœ€åˆçš„æ•°æ®å¸§å°†å‘ˆçŽ°å‡ºä¸Šè¿°å½¢çŠ¶çš„ç©ºæ•°æ®å¸§ã€‚å½“å°† `row_count` è®¾ç½®ä¸º `dynamic` æ—¶ï¼Œä¸å¿…ä¾èµ–äºŽæ­£åœ¨è¾“å…¥çš„æ•°æ®é›†æ¥é¢„å®šä¹‰ç»„ä»¶ã€‚
- `outputs`ï¼šç”¨äºŽå­˜å‚¨è¾“å‡ºçš„æ•°æ®å¸§ç»„ä»¶ã€‚è¯¥ç•Œé¢å¯ä»¥æŽ¥å—å•ä¸ªæˆ–å¤šä¸ªæ ·æœ¬è¿›è¡ŒæŽ¨æ–­ï¼Œå¹¶åœ¨ä¸€åˆ—ä¸­ä¸ºæ¯ä¸ªæ ·æœ¬è¿”å›ž 0 æˆ– 1ï¼Œå› æ­¤æˆ‘ä»¬å°† `row_count` è®¾ç½®ä¸º 2ï¼Œ`col_count` è®¾ç½®ä¸º 1ã€‚`headers` æ˜¯ç”±æ•°æ®å¸§çš„åˆ—åç»„æˆçš„åˆ—è¡¨ã€‚
- `examples`ï¼šæ‚¨å¯ä»¥é€šè¿‡æ‹–æ”¾ CSV æ–‡ä»¶æˆ–é€šè¿‡ç¤ºä¾‹ä¼ é€’ pandas DataFrameï¼Œç•Œé¢ä¼šè‡ªåŠ¨èŽ·å–å…¶æ ‡é¢˜ã€‚

çŽ°åœ¨æˆ‘ä»¬å°†ä¸ºç®€åŒ–ç‰ˆæ•°æ®å¯è§†åŒ–ä»ªè¡¨æ¿åˆ›å»ºä¸€ä¸ªç¤ºä¾‹ã€‚æ‚¨å¯ä»¥åœ¨ç›¸å…³ç©ºé—´ä¸­æ‰¾åˆ°æ›´å…¨é¢çš„ç‰ˆæœ¬ã€‚

<gradio-app space="gradio/tabular-playground"></gradio-app>

```python
import gradio as gr
import pandas as pd
import datasets
import seaborn as sns
import matplotlib.pyplot as plt

df = datasets.load_dataset("merve/supersoaker-failures")
df = df["train"].to_pandas()
df.dropna(axis=0, inplace=True)

def plot(df):
  plt.scatter(df.measurement_13, df.measurement_15, c = df.loading,alpha=0.5)
  plt.savefig("scatter.png")
  df['failure'].value_counts().plot(kind='bar')
  plt.savefig("bar.png")
  sns.heatmap(df.select_dtypes(include="number").corr())
  plt.savefig("corr.png")
  plots = ["corr.png","scatter.png", "bar.png"]
  return plots

inputs = [gr.Dataframe(label="Supersoaker Production Data")]
outputs = [gr.Gallery(label="Profiling Dashboard", columns=(1,3))]

gr.Interface(plot, inputs=inputs, outputs=outputs, examples=[df.head(100)], title="Supersoaker Failures Analysis Dashboard").launch()
```

<gradio-app space="gradio/gradio-analysis-dashboard-minimal"></gradio-app>

æˆ‘ä»¬å°†ä½¿ç”¨ä¸Žè®­ç»ƒæ¨¡åž‹ç›¸åŒçš„æ•°æ®é›†ï¼Œä½†è¿™æ¬¡æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå¯è§†åŒ–ä»ªè¡¨æ¿ä»¥å±•ç¤ºå®ƒã€‚

- `fn`ï¼šæ ¹æ®æ•°æ®åˆ›å»ºå›¾è¡¨çš„å‡½æ•°ã€‚
- `inputs`ï¼šæˆ‘ä»¬ä½¿ç”¨äº†ä¸Žä¸Šè¿°ç›¸åŒçš„ `Dataframe` ç»„ä»¶ã€‚
- `outputs`ï¼šæˆ‘ä»¬ä½¿ç”¨ `Gallery` ç»„ä»¶æ¥å­˜æ”¾æˆ‘ä»¬çš„å¯è§†åŒ–ç»“æžœã€‚
- `examples`ï¼šæˆ‘ä»¬å°†æ•°æ®é›†æœ¬èº«ä½œä¸ºç¤ºä¾‹ã€‚

## ä½¿ç”¨ skops ä¸€è¡Œä»£ç è½»æ¾åŠ è½½è¡¨æ ¼æ•°æ®ç•Œé¢

`skops` æ˜¯ä¸€ä¸ªæž„å»ºåœ¨ `huggingface_hub` å’Œ `sklearn` ä¹‹ä¸Šçš„åº“ã€‚é€šè¿‡æœ€æ–°çš„ `gradio` é›†æˆï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸€è¡Œä»£ç æž„å»ºè¡¨æ ¼æ•°æ®ç•Œé¢ï¼

```python
import gradio as gr

# æ ‡é¢˜å’Œæè¿°æ˜¯å¯é€‰çš„
title = "Supersoakeräº§å“ç¼ºé™·é¢„æµ‹"
description = "è¯¥æ¨¡åž‹é¢„æµ‹Supersoakerç”Ÿäº§çº¿æ•…éšœã€‚åœ¨ä¸‹é¢çš„æ•°æ®å¸§ç»„ä»¶ä¸­ï¼Œæ‚¨å¯ä»¥æ‹–æ”¾æ•°æ®é›†çš„ä»»æ„åˆ‡ç‰‡æˆ–è‡ªè¡Œç¼–è¾‘å€¼ã€‚"

gr.load("huggingface/scikit-learn/tabular-playground", title=title, description=description).launch()
```

<gradio-app space="gradio/gradio-skops-integration"></gradio-app>

ä½¿ç”¨ `skops` å°† `sklearn` æ¨¡åž‹æŽ¨é€åˆ° Hugging Face Hub æ—¶ï¼Œä¼šåŒ…å«ä¸€ä¸ªåŒ…å«ç¤ºä¾‹è¾“å…¥å’Œåˆ—åçš„ `config.json` æ–‡ä»¶ï¼Œè§£å†³çš„ä»»åŠ¡ç±»åž‹æ˜¯ `tabular-classification` æˆ– `tabular-regression`ã€‚æ ¹æ®ä»»åŠ¡ç±»åž‹ï¼Œ`gradio` æž„å»ºç•Œé¢å¹¶ä½¿ç”¨åˆ—åå’Œç¤ºä¾‹è¾“å…¥æ¥æž„å»ºå®ƒã€‚æ‚¨å¯ä»¥[å‚è€ƒ skops åœ¨ Hub ä¸Šæ‰˜ç®¡æ¨¡åž‹çš„æ–‡æ¡£](https://skops.readthedocs.io/en/latest/auto_examples/plot_hf_hub.html#sphx-glr-auto-examples-plot-hf-hub-py)æ¥äº†è§£å¦‚ä½•ä½¿ç”¨ `skops` å°†æ¨¡åž‹æŽ¨é€åˆ° Hubã€‚

---

<!-- Source: guides/cn/06_client-libraries/fastapi-app-with-the-gradio-client.md -->
# ä½¿ç”¨Gradio Pythonå®¢æˆ·ç«¯æž„å»ºFastAPIåº”ç”¨

Tags: CLIENT, API, WEB APP

åœ¨æœ¬åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ `gradio_client` [Pythonåº“](getting-started-with-the-python-client/) æ¥ä»¥ç¼–ç¨‹æ–¹å¼åˆ›å»ºGradioåº”ç”¨çš„è¯·æ±‚ï¼Œé€šè¿‡åˆ›å»ºä¸€ä¸ªç¤ºä¾‹FastAPI Webåº”ç”¨ã€‚æˆ‘ä»¬å°†æž„å»ºçš„ Web åº”ç”¨åä¸ºâ€œAcappellifyâ€ï¼Œå®ƒå…è®¸ç”¨æˆ·ä¸Šä¼ è§†é¢‘æ–‡ä»¶ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›žä¸€ä¸ªæ²¡æœ‰ä¼´å¥éŸ³ä¹çš„è§†é¢‘ç‰ˆæœ¬ã€‚å®ƒè¿˜ä¼šæ˜¾ç¤ºç”Ÿæˆçš„è§†é¢‘åº“ã€‚

**å…ˆå†³æ¡ä»¶**

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨æ­£åœ¨è¿è¡ŒPython 3.9æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œå¹¶å·²å®‰è£…ä»¥ä¸‹åº“ï¼š

- `gradio_client`
- `fastapi`
- `uvicorn`

æ‚¨å¯ä»¥ä½¿ç”¨`pip`å®‰è£…è¿™äº›åº“ï¼š

```bash
$ pip install gradio_client fastapi uvicorn
```

æ‚¨è¿˜éœ€è¦å®‰è£…ffmpegã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥æ£€æŸ¥æ‚¨æ˜¯å¦å·²å®‰è£…ffmpegï¼š

```bash
$ ffmpeg version
```

å¦åˆ™ï¼Œé€šè¿‡æŒ‰ç…§è¿™äº›è¯´æ˜Žå®‰è£…ffmpeg [é“¾æŽ¥](https://www.hostinger.com/tutorials/how-to-install-ffmpeg)ã€‚

## æ­¥éª¤1ï¼šç¼–å†™è§†é¢‘å¤„ç†å‡½æ•°

è®©æˆ‘ä»¬ä»Žä¼¼ä¹Žæœ€å¤æ‚çš„éƒ¨åˆ†å¼€å§‹--ä½¿ç”¨æœºå™¨å­¦ä¹ ä»Žè§†é¢‘ä¸­åŽ»é™¤éŸ³ä¹ã€‚

å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªçŽ°æœ‰çš„Spaceå¯ä»¥ç®€åŒ–è¿™ä¸ªè¿‡ç¨‹ï¼š[https://huggingface.co/spaces/abidlabs/music-separation](https://huggingface.co/spaces/abidlabs/music-separation)ã€‚è¯¥ç©ºé—´æŽ¥å—ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆä¸¤ä¸ªç‹¬ç«‹çš„éŸ³é¢‘æ–‡ä»¶ï¼šä¸€ä¸ªå¸¦æœ‰ä¼´å¥éŸ³ä¹ï¼Œä¸€ä¸ªå¸¦æœ‰åŽŸå§‹å‰ªè¾‘ä¸­çš„å…¶ä»–æ‰€æœ‰å£°éŸ³ã€‚éžå¸¸é€‚åˆæˆ‘ä»¬çš„å®¢æˆ·ç«¯ä½¿ç”¨ï¼

æ‰“å¼€ä¸€ä¸ªæ–°çš„Pythonæ–‡ä»¶ï¼Œæ¯”å¦‚`main.py`ï¼Œå¹¶é€šè¿‡ä»Ž`gradio_client`å¯¼å…¥ `Client` ç±»ï¼Œå¹¶å°†å…¶è¿žæŽ¥åˆ°è¯¥Spaceï¼š

```py
from gradio_client import Client

client = Client("abidlabs/music-separation")

def acapellify(audio_path):
    result = client.predict(audio_path, api_name="/predict")
    return result[0]
```

æ‰€éœ€çš„ä»£ç ä»…å¦‚ä¸Šæ‰€ç¤º--è¯·æ³¨æ„ï¼ŒAPIç«¯ç‚¹è¿”å›žä¸€ä¸ªåŒ…å«ä¸¤ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼ˆä¸€ä¸ªæ²¡æœ‰éŸ³ä¹ï¼Œä¸€ä¸ªåªæœ‰éŸ³ä¹ï¼‰çš„åˆ—è¡¨ï¼Œå› æ­¤æˆ‘ä»¬åªè¿”å›žåˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ ã€‚

---

**æ³¨æ„**ï¼šç”±äºŽè¿™æ˜¯ä¸€ä¸ªå…¬å…±Spaceï¼Œå¯èƒ½ä¼šæœ‰å…¶ä»–ç”¨æˆ·åŒæ—¶ä½¿ç”¨è¯¥Spaceï¼Œè¿™å¯èƒ½å¯¼è‡´é€Ÿåº¦è¾ƒæ…¢ã€‚æ‚¨å¯ä»¥ä½¿ç”¨è‡ªå·±çš„[Hugging Face token](https://huggingface.co/settings/tokens)å¤åˆ¶æ­¤Spaceï¼Œåˆ›å»ºä¸€ä¸ªåªæœ‰æ‚¨è‡ªå·±è®¿é—®æƒé™çš„ç§æœ‰Spaceï¼Œå¹¶ç»•è¿‡æŽ’é˜Ÿã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œåªéœ€ç”¨ä¸‹é¢çš„ä»£ç æ›¿æ¢ä¸Šé¢çš„å‰ä¸¤è¡Œï¼š

```py
from gradio_client import Client

client = Client.duplicate("abidlabs/music-separation", hf_token=YOUR_HF_TOKEN)
```

å…¶ä»–çš„ä»£ç ä¿æŒä¸å˜ï¼

---

çŽ°åœ¨ï¼Œå½“ç„¶ï¼Œæˆ‘ä»¬æ­£åœ¨å¤„ç†è§†é¢‘æ–‡ä»¶ï¼Œæ‰€ä»¥æˆ‘ä»¬é¦–å…ˆéœ€è¦ä»Žè§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`ffmpeg`åº“ï¼Œå®ƒåœ¨å¤„ç†éŸ³é¢‘å’Œè§†é¢‘æ–‡ä»¶æ—¶åšäº†å¾ˆå¤šè‰°å·¨çš„å·¥ä½œã€‚ä½¿ç”¨`ffmpeg`çš„æœ€å¸¸è§æ–¹æ³•æ˜¯é€šè¿‡å‘½ä»¤è¡Œï¼Œåœ¨Pythonçš„`subprocess`æ¨¡å—ä¸­è°ƒç”¨å®ƒï¼š

æˆ‘ä»¬çš„è§†é¢‘å¤„ç†å·¥ä½œæµåŒ…å«ä¸‰ä¸ªæ­¥éª¤ï¼š

1. é¦–å…ˆï¼Œæˆ‘ä»¬ä»Žè§†é¢‘æ–‡ä»¶è·¯å¾„å¼€å§‹ï¼Œå¹¶ä½¿ç”¨`ffmpeg`æå–éŸ³é¢‘ã€‚
2. ç„¶åŽï¼Œæˆ‘ä»¬é€šè¿‡ä¸Šé¢çš„`acapellify()`å‡½æ•°ä¼ å…¥éŸ³é¢‘æ–‡ä»¶ã€‚
3. æœ€åŽï¼Œæˆ‘ä»¬å°†æ–°éŸ³é¢‘ä¸ŽåŽŸå§‹è§†é¢‘åˆå¹¶ï¼Œç”Ÿæˆæœ€ç»ˆçš„Acapellifyè§†é¢‘ã€‚

ä»¥ä¸‹æ˜¯Pythonä¸­çš„å®Œæ•´ä»£ç ï¼Œæ‚¨å¯ä»¥å°†å…¶æ·»åŠ åˆ°`main.py`æ–‡ä»¶ä¸­ï¼š

```python
import subprocess

def process_video(video_path):
    old_audio = os.path.basename(video_path).split(".")[0] + ".m4a"
    subprocess.run(['ffmpeg', '-y', '-i', video_path, '-vn', '-acodec', 'copy', old_audio])

    new_audio = acapellify(old_audio)

    new_video = f"acap_{video_path}"
    subprocess.call(['ffmpeg', '-y', '-i', video_path, '-i', new_audio, '-map', '0:v', '-map', '1:a', '-c:v', 'copy', '-c:a', 'aac', '-strict', 'experimental', f"static/{new_video}"])
    return new_video
```

å¦‚æžœæ‚¨æƒ³äº†è§£æ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·é˜…è¯»[ffmpegæ–‡æ¡£](https://ffmpeg.org/ffmpeg.html)ï¼Œå› ä¸ºå®ƒä»¬è¶…å‡ºäº†æœ¬æ•™ç¨‹çš„èŒƒå›´ã€‚

## æ­¥éª¤2: åˆ›å»ºä¸€ä¸ªFastAPIåº”ç”¨ï¼ˆåŽç«¯è·¯ç”±ï¼‰

æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªç®€å•çš„FastAPIåº”ç”¨ç¨‹åºã€‚å¦‚æžœæ‚¨ä»¥å‰æ²¡æœ‰ä½¿ç”¨è¿‡FastAPIï¼Œè¯·æŸ¥çœ‹[ä¼˜ç§€çš„FastAPIæ–‡æ¡£](https://fastapi.tiangolo.com/)ã€‚å¦åˆ™ï¼Œä¸‹é¢çš„åŸºæœ¬æ¨¡æ¿å°†çœ‹èµ·æ¥å¾ˆç†Ÿæ‚‰ï¼Œæˆ‘ä»¬å°†å…¶æ·»åŠ åˆ°`main.py`ä¸­ï¼š

```python
import os
from fastapi import FastAPI, File, UploadFile, Request
from fastapi.responses import HTMLResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates

app = FastAPI()
os.makedirs("static", exist_ok=True)
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

videos = []

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    return templates.TemplateResponse(
        "home.html", {"request": request, "videos": videos})

@app.post("/uploadvideo/")
async def upload_video(video: UploadFile = File(...)):
    new_video = process_video(video.filename)
    videos.append(new_video)
    return RedirectResponse(url='/', status_code=303)
```

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼ŒFastAPIåº”ç”¨ç¨‹åºæœ‰ä¸¤ä¸ªè·¯ç”±ï¼š`/` å’Œ `/uploadvideo/`ã€‚

`/` è·¯ç”±è¿”å›žä¸€ä¸ªæ˜¾ç¤ºæ‰€æœ‰ä¸Šä¼ è§†é¢‘çš„ç”»å»Šçš„HTMLæ¨¡æ¿ã€‚

`/uploadvideo/` è·¯ç”±æŽ¥å—ä¸€ä¸ªå¸¦æœ‰`UploadFile`å¯¹è±¡çš„ `POST` è¯·æ±‚ï¼Œè¡¨ç¤ºä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶ã€‚è§†é¢‘æ–‡ä»¶é€šè¿‡`process_video()`æ–¹æ³•è¿›è¡Œ "acapellify"ï¼Œå¹¶å°†è¾“å‡ºè§†é¢‘å­˜å‚¨åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œè¯¥åˆ—è¡¨åœ¨å†…å­˜ä¸­å­˜å‚¨äº†æ‰€æœ‰ä¸Šä¼ çš„è§†é¢‘ã€‚

è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯ä¸€ä¸ªéžå¸¸åŸºæœ¬çš„ç¤ºä¾‹ï¼Œå¦‚æžœè¿™æ˜¯ä¸€ä¸ªå‘å¸ƒåº”ç”¨ç¨‹åºï¼Œåˆ™éœ€è¦æ·»åŠ æ›´å¤šé€»è¾‘æ¥å¤„ç†æ–‡ä»¶å­˜å‚¨ã€ç”¨æˆ·èº«ä»½éªŒè¯å’Œå®‰å…¨æ€§è€ƒè™‘ç­‰ã€‚

## æ­¥éª¤3ï¼šåˆ›å»ºä¸€ä¸ªFastAPIåº”ç”¨ï¼ˆå‰ç«¯æ¨¡æ¿ï¼‰

æœ€åŽï¼Œæˆ‘ä»¬åˆ›å»ºWebåº”ç”¨çš„å‰ç«¯ã€‚é¦–å…ˆï¼Œåœ¨ä¸Ž`main.py`ç›¸åŒçš„ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªåä¸º`templates`çš„æ–‡ä»¶å¤¹ã€‚ç„¶åŽï¼Œåœ¨`templates`æ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ªåä¸º`home.html`çš„æ¨¡æ¿ã€‚ä¸‹é¢æ˜¯æœ€ç»ˆçš„æ–‡ä»¶ç»“æž„ï¼š

```csv
â”œâ”€â”€ main.py
â”œâ”€â”€ templates
â”‚   â””â”€â”€ home.html
```

å°†ä»¥ä¸‹å†…å®¹å†™å…¥`home.html`æ–‡ä»¶ä¸­ï¼š

```html
&lt;!DOCTYPE html> &lt;html> &lt;head> &lt;title> è§†é¢‘åº“ &lt;/title> &lt;style>
body { font-family: sans-serif; margin: 0; padding: 0; background-color:
#f5f5f5; } h1 { text-align: center; margin-top: 30px; margin-bottom: 20px; }
.gallery { display: flex; flex-wrap: wrap; justify-content: center; gap: 20px;
padding: 20px; } .video { border: 2px solid #ccc; box-shadow: 0px 0px 10px
rgba(0, 0, 0, 0.2); border-radius: 5px; overflow: hidden; width: 300px;
margin-bottom: 20px; } .video video { width: 100%; height: 200px; } .video p {
text-align: center; margin: 10px 0; } form { margin-top: 20px; text-align:
center; } input[type="file"] { display: none; } .upload-btn { display:
inline-block; background-color: #3498db; color: #fff; padding: 10px 20px;
font-size: 16px; border: none; border-radius: 5px; cursor: pointer; }
.upload-btn:hover { background-color: #2980b9; } .file-name { margin-left: 10px;
} &lt;/style> &lt;/head> &lt;body> &lt;h1> è§†é¢‘åº“ &lt;/h1> {% if videos %}
&lt;div class="gallery"> {% for video in videos %} &lt;div class="video">
&lt;video controls> &lt;source src="{{ url_for('static', path=video) }}"
type="video/mp4"> æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè§†é¢‘æ ‡ç­¾ã€‚ &lt;/video> &lt;p>{{ video
}}&lt;/p> &lt;/div> {% endfor %} &lt;/div> {% else %} &lt;p>
å°šæœªä¸Šä¼ ä»»ä½•è§†é¢‘ã€‚&lt;/p> {% endif %} &lt;form action="/uploadvideo/"
method="post" enctype="multipart/form-data"> &lt;label for="video-upload"
class="upload-btn"> é€‰æ‹©è§†é¢‘æ–‡ä»¶ &lt;/label> &lt;input type="file" name="video"
id="video-upload"> &lt;span class="file-name">&lt;/span> &lt;button
type="submit" class="upload-btn"> ä¸Šä¼  &lt;/button> &lt;/form> &lt;script> //
åœ¨è¡¨å•ä¸­æ˜¾ç¤ºæ‰€é€‰æ–‡ä»¶å const fileUpload =
document.getElementById("video-upload"); const fileName =
document.querySelector(".file-name"); fileUpload.addEventListener("change", (e)
=> { fileName.textContent = e.target.files[0].name; }); &lt;/script> &lt;/body>
&lt;/html>
```

## ç¬¬4æ­¥ï¼šè¿è¡Œ FastAPI åº”ç”¨

æœ€åŽï¼Œæˆ‘ä»¬å‡†å¤‡å¥½è¿è¡Œç”± Gradio Python å®¢æˆ·ç«¯æä¾›æ”¯æŒçš„ FastAPI åº”ç”¨ç¨‹åºã€‚

æ‰“å¼€ç»ˆç«¯å¹¶å¯¼èˆªåˆ°åŒ…å« `main.py` æ–‡ä»¶çš„ç›®å½•ï¼Œç„¶åŽåœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
$ uvicorn main:app
```

æ‚¨åº”è¯¥ä¼šçœ‹åˆ°å¦‚ä¸‹è¾“å‡ºï¼š

```csv
Loaded as API: https://abidlabs-music-separation.hf.space âœ”
INFO:     Started server process [1360]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

å°±æ˜¯è¿™æ ·ï¼å¼€å§‹ä¸Šä¼ è§†é¢‘ï¼Œæ‚¨å°†åœ¨å“åº”ä¸­å¾—åˆ°ä¸€äº›â€œacapellifiedâ€è§†é¢‘ï¼ˆå¤„ç†æ—¶é—´æ ¹æ®æ‚¨çš„è§†é¢‘é•¿åº¦å¯èƒ½éœ€è¦å‡ ç§’é’Ÿåˆ°å‡ åˆ†é’Ÿï¼‰ã€‚ä»¥ä¸‹æ˜¯ä¸Šä¼ ä¸¤ä¸ªè§†é¢‘åŽ UI çš„å¤–è§‚ï¼š

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/acapellify.png)

å¦‚æžœæ‚¨æƒ³äº†è§£å¦‚ä½•åœ¨é¡¹ç›®ä¸­ä½¿ç”¨ Gradio Python å®¢æˆ·ç«¯çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·[é˜…è¯»ä¸“é—¨çš„æŒ‡å—](/getting-started-with-the-python-client/)ã€‚

---

<!-- Source: guides/cn/06_client-libraries/gradio-and-llm-agents.md -->
# Gradio & LLM Agents ðŸ¤

éžå¸¸å¼ºå¤§çš„å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰ï¼Œå¦‚æžœæˆ‘ä»¬èƒ½èµ‹äºˆå®ƒä»¬å®Œæˆä¸“é—¨ä»»åŠ¡çš„æŠ€èƒ½ï¼Œå®ƒä»¬å°†å˜å¾—æ›´åŠ å¼ºå¤§ã€‚

[gradio_tools](https://github.com/freddyaboulton/gradio-tools)åº“å¯ä»¥å°†ä»»ä½•[Gradio](https://github.com/gradio-app/gradio)åº”ç”¨ç¨‹åºè½¬åŒ–ä¸º[å·¥å…·](https://python.langchain.com/en/latest/modules/agents/tools.html)ï¼Œä¾›[ä»£ç†](https://docs.langchain.com/docs/components/agents/agent)ä½¿ç”¨ä»¥å®Œæˆä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªLLMå¯ä»¥ä½¿ç”¨Gradioå·¥å…·è½¬å½•åœ¨ç½‘ä¸Šæ‰¾åˆ°çš„è¯­éŸ³è®°å½•ï¼Œç„¶åŽä¸ºæ‚¨summarizeå®ƒã€‚æˆ–è€…å®ƒå¯ä»¥ä½¿ç”¨ä¸åŒçš„Gradioå·¥å…·å¯¹æ‚¨çš„Google Driveä¸Šçš„æ–‡æ¡£åº”ç”¨OCRï¼Œç„¶åŽå›žç­”ç›¸å…³é—®é¢˜ã€‚

æœ¬æŒ‡å—å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨`gradio_tools`è®©æ‚¨çš„LLMä»£ç†è®¿é—®å…¨çƒæ‰˜ç®¡çš„æœ€å…ˆè¿›çš„Gradioåº”ç”¨ç¨‹åºã€‚å°½ç®¡`gradio_tools`ä¸Žä¸æ­¢ä¸€ä¸ªä»£ç†æ¡†æž¶å…¼å®¹ï¼Œä½†æœ¬æŒ‡å—å°†é‡ç‚¹ä»‹ç»[Langchainä»£ç†](https://docs.langchain.com/docs/components/agents/)ã€‚

## ä¸€äº›èƒŒæ™¯ä¿¡æ¯

### ä»£ç†æ˜¯ä»€ä¹ˆï¼Ÿ

[LangChainä»£ç†](https://docs.langchain.com/docs/components/agents/agent)æ˜¯ä¸€ä¸ªå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰ï¼Œå®ƒæ ¹æ®ä½¿ç”¨å…¶ä¼—å¤šå·¥å…·ä¹‹ä¸€çš„è¾“å…¥æ¥ç”Ÿæˆè¾“å‡ºã€‚

### Gradioæ˜¯ä»€ä¹ˆï¼Ÿ

[Gradio](https://github.com/gradio-app/gradio)æ˜¯ç”¨äºŽæž„å»ºæœºå™¨å­¦ä¹ Webåº”ç”¨ç¨‹åºå¹¶ä¸Žå…¨çƒå…±äº«çš„äº‹å®žä¸Šçš„æ ‡å‡†æ¡†æž¶-å®Œå…¨ç”±Pythoné©±åŠ¨ï¼ðŸ

## gradio_tools - ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç¤ºä¾‹

è¦å¼€å§‹ä½¿ç”¨`gradio_tools`ï¼Œæ‚¨åªéœ€è¦å¯¼å…¥å’Œåˆå§‹åŒ–å·¥å…·ï¼Œç„¶åŽå°†å…¶ä¼ é€’ç»™langchainä»£ç†ï¼

åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯¼å…¥`StableDiffusionPromptGeneratorTool`ä»¥åˆ›å»ºä¸€ä¸ªè‰¯å¥½çš„ç¨³å®šæ‰©æ•£æç¤ºï¼Œ
`StableDiffusionTool`ä»¥ä½¿ç”¨æˆ‘ä»¬æ”¹è¿›çš„æç¤ºåˆ›å»ºä¸€å¼ å›¾ç‰‡ï¼Œ`ImageCaptioningTool`ä»¥ä¸ºç”Ÿæˆçš„å›¾ç‰‡åŠ ä¸Šæ ‡é¢˜ï¼Œä»¥åŠ
`TextToVideoTool`ä»¥æ ¹æ®æç¤ºåˆ›å»ºä¸€ä¸ªè§†é¢‘ã€‚

ç„¶åŽï¼Œæˆ‘ä»¬å‘Šè¯‰æˆ‘ä»¬çš„ä»£ç†åˆ›å»ºä¸€å¼ ç‹—æ­£åœ¨æ»‘æ¿çš„å›¾ç‰‡ï¼Œä½†åœ¨ä½¿ç”¨å›¾åƒç”Ÿæˆå™¨ä¹‹å‰è¯·å…ˆæ”¹è¿›æˆ‘ä»¬çš„æç¤ºã€‚æˆ‘ä»¬è¿˜è¦æ±‚
å®ƒä¸ºç”Ÿæˆçš„å›¾ç‰‡æ·»åŠ æ ‡é¢˜å¹¶åˆ›å»ºä¸€ä¸ªè§†é¢‘ã€‚ä»£ç†å¯ä»¥æ ¹æ®éœ€è¦å†³å®šä½¿ç”¨å“ªä¸ªå·¥å…·ï¼Œè€Œä¸éœ€è¦æˆ‘ä»¬æ˜Žç¡®å‘ŠçŸ¥ã€‚

```python
import os

if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEY å¿…é¡»è®¾ç½® ")

from langchain.agents import initialize_agent
from langchain.llms import OpenAI
from gradio_tools import (StableDiffusionTool, ImageCaptioningTool, StableDiffusionPromptGeneratorTool,
                          TextToVideoTool)

from langchain.memory import ConversationBufferMemory

llm = OpenAI(temperature=0)
memory = ConversationBufferMemory(memory_key="chat_history")
tools = [StableDiffusionTool().langchain, ImageCaptioningTool().langchain,
         StableDiffusionPromptGeneratorTool().langchain, TextToVideoTool().langchain]

agent = initialize_agent(tools, llm, memory=memory, agent="conversational-react-description", verbose=True)
output = agent.run(input=("Please create a photo of a dog riding a skateboard "
                          "but improve my prompt prior to using an image generator."
                          "Please caption the generated image and create a video for it using the improved prompt."))
```

æ‚¨ä¼šæ³¨æ„åˆ°æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ä¸€äº›ä¸Ž`gradio_tools`ä¸€èµ·æä¾›çš„é¢„æž„å»ºå·¥å…·ã€‚è¯·å‚é˜…æ­¤[æ–‡æ¡£](https://github.com/freddyaboulton/gradio-tools#gradio-tools-gradio--llm-agents)ä»¥èŽ·å–å®Œæ•´çš„`gradio_tools`å·¥å…·åˆ—è¡¨ã€‚
å¦‚æžœæ‚¨æƒ³ä½¿ç”¨å½“å‰ä¸åœ¨`gradio_tools`ä¸­çš„å·¥å…·ï¼Œå¾ˆå®¹æ˜“æ·»åŠ æ‚¨è‡ªå·±çš„å·¥å…·ã€‚ä¸‹ä¸€èŠ‚å°†ä»‹ç»å¦‚ä½•æ·»åŠ è‡ªå·±çš„å·¥å…·ã€‚

## gradio_tools - åˆ›å»ºè‡ªå·±çš„å·¥å…·

æ ¸å¿ƒæŠ½è±¡æ˜¯`GradioTool`ï¼Œå®ƒå…è®¸æ‚¨ä¸ºLLMå®šä¹‰ä¸€ä¸ªæ–°çš„å·¥å…·ï¼Œåªè¦æ‚¨å®žçŽ°æ ‡å‡†æŽ¥å£ï¼š

```python
class GradioTool(BaseTool):

    def __init__(self, name: str, description: str, src: str) -> None:

    @abstractmethod
    def create_job(self, query: str) -> Job:
        pass

    @abstractmethod
    def postprocess(self, output: Tuple[Any] | Any) -> str:
        pass
```

éœ€è¦æ»¡è¶³çš„è¦æ±‚æ˜¯ï¼š

1. å·¥å…·çš„åç§°
2. å·¥å…·çš„æè¿°ã€‚è¿™éžå¸¸å…³é”®ï¼ä»£ç†æ ¹æ®å…¶æè¿°å†³å®šä½¿ç”¨å“ªä¸ªå·¥å…·ã€‚è¯·ç¡®åˆ‡æè¿°è¾“å…¥å’Œè¾“å‡ºåº”è¯¥æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œæœ€å¥½åŒ…æ‹¬ç¤ºä¾‹ã€‚
3. Gradioåº”ç”¨ç¨‹åºçš„urlæˆ–space idï¼Œä¾‹å¦‚`freddyaboulton/calculator`ã€‚åŸºäºŽè¯¥å€¼ï¼Œ`gradio_tool`å°†é€šè¿‡APIåˆ›å»ºä¸€ä¸ª[gradioå®¢æˆ·ç«¯](https://github.com/gradio-app/gradio/blob/main/client/python/README.md)å®žä¾‹æ¥æŸ¥è¯¢ä¸Šæ¸¸åº”ç”¨ç¨‹åºã€‚å¦‚æžœæ‚¨ä¸ç†Ÿæ‚‰gradioå®¢æˆ·ç«¯åº“ï¼Œè¯·ç¡®ä¿ç‚¹å‡»é“¾æŽ¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚
4. create_job - ç»™å®šä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¯¥æ–¹æ³•åº”è¯¥è§£æžè¯¥å­—ç¬¦ä¸²å¹¶ä»Žå®¢æˆ·ç«¯è¿”å›žä¸€ä¸ªjobã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™åªéœ€å°†å­—ç¬¦ä¸²ä¼ é€’ç»™å®¢æˆ·ç«¯çš„`submit`å‡½æ•°å³å¯ã€‚æœ‰å…³åˆ›å»ºjobçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è¿™é‡Œ](https://github.com/gradio-app/gradio/blob/main/client/python/README.md#making-a-prediction)
5. postprocess - ç»™å®šä½œä¸šçš„ç»“æžœï¼Œå°†å…¶è½¬æ¢ä¸ºLLMå¯ä»¥å‘ç”¨æˆ·æ˜¾ç¤ºçš„å­—ç¬¦ä¸²ã€‚
6. _Optionalå¯é€‰_ - æŸäº›åº“ï¼Œä¾‹å¦‚[MiniChain](https://github.com/srush/MiniChain/tree/main)ï¼Œå¯èƒ½éœ€è¦ä¸€äº›å…³äºŽå·¥å…·ä½¿ç”¨çš„åº•å±‚gradioè¾“å…¥å’Œè¾“å‡ºç±»åž‹çš„ä¿¡æ¯ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™å°†è¿”å›žgr.Textbox()ï¼Œä½†å¦‚æžœæ‚¨æƒ³æä¾›æ›´å‡†ç¡®çš„ä¿¡æ¯ï¼Œè¯·å®žçŽ°å·¥å…·çš„`_block_input(self, gr)`å’Œ`_block_output(self, gr)`æ–¹æ³•ã€‚`gr`å˜é‡æ˜¯gradioæ¨¡å—ï¼ˆé€šè¿‡`import gradio as gr`èŽ·å¾—çš„ç»“æžœï¼‰ã€‚`GradiTool`çˆ¶ç±»å°†è‡ªåŠ¨å¼•å…¥`gr`å¹¶å°†å…¶ä¼ é€’ç»™`_block_input`å’Œ`_block_output`æ–¹æ³•ã€‚

å°±æ˜¯è¿™æ ·ï¼

ä¸€æ—¦æ‚¨åˆ›å»ºäº†è‡ªå·±çš„å·¥å…·ï¼Œè¯·åœ¨`gradio_tools`å­˜å‚¨åº“ä¸Šå‘èµ·æ‹‰å–è¯·æ±‚ï¼æˆ‘ä»¬æ¬¢è¿Žæ‰€æœ‰è´¡çŒ®ã€‚

## ç¤ºä¾‹å·¥å…· - ç¨³å®šæ‰©æ•£

ä»¥ä¸‹æ˜¯ä½œä¸ºç¤ºä¾‹çš„ç¨³å®šæ‰©æ•£å·¥å…·ä»£ç ï¼š

from gradio_tool import GradioTool
import os

class StableDiffusionTool(GradioTool):
"""Tool for calling stable diffusion from llm"""

    def __init__(
        self,
        name="StableDiffusion",
        description=(
            "An image generator. Use this to generate images based on "
            "text input. Input should be a description of what the image should "
            "look like. The output will be a path to an image file."
        ),
        src="gradio-client-demos/stable-diffusion",
        hf_token=None,
    ) -> None:
        super().__init__(name, description, src, hf_token)

    def create_job(self, query: str) -> Job:
        return self.client.submit(query, "", 9, fn_index=1)

    def postprocess(self, output: str) -> str:
        return [os.path.join(output, i) for i in os.listdir(output) if not i.endswith("json")][0]

    def _block_input(self, gr) -> "gr.components.Component":
        return gr.Textbox()

    def _block_output(self, gr) -> "gr.components.Component":
        return gr.Image()

```
å…³äºŽæ­¤å®žçŽ°çš„ä¸€äº›æ³¨æ„äº‹é¡¹ï¼š
1. æ‰€æœ‰çš„ `GradioTool` å®žä¾‹éƒ½æœ‰ä¸€ä¸ªåä¸º `client` çš„å±žæ€§ï¼Œå®ƒæŒ‡å‘åº•å±‚çš„ [gradio å®¢æˆ·ç«¯](https://github.com/gradio-app/gradio/tree/main/client/python#gradio_client-use-a-gradio-app-as-an-api----in-3-lines-of-python)ï¼Œè¿™å°±æ˜¯æ‚¨åœ¨ `create_job` æ–¹æ³•ä¸­åº”è¯¥ä½¿ç”¨çš„å†…å®¹ã€‚

2. `create_job` æ–¹æ³•åªæ˜¯å°†æŸ¥è¯¢å­—ç¬¦ä¸²ä¼ é€’ç»™å®¢æˆ·ç«¯çš„ `submit` å‡½æ•°ï¼Œå¹¶ç¡¬ç¼–ç äº†ä¸€äº›å…¶ä»–å‚æ•°ï¼Œå³è´Ÿé¢æç¤ºå­—ç¬¦ä¸²å’ŒæŒ‡å—ç¼©æ”¾ã€‚æˆ‘ä»¬å¯ä»¥åœ¨åŽç»­ç‰ˆæœ¬ä¸­ä¿®æ”¹æˆ‘ä»¬çš„å·¥å…·ï¼Œä»¥ä¾¿ä»Žè¾“å…¥å­—ç¬¦ä¸²ä¸­æŽ¥å—è¿™äº›å€¼ã€‚

3. `postprocess` æ–¹æ³•åªæ˜¯è¿”å›žç”±ç¨³å®šæ‰©æ•£ç©ºé—´åˆ›å»ºçš„å›¾åº“ä¸­çš„ç¬¬ä¸€ä¸ªå›¾åƒã€‚æˆ‘ä»¬ä½¿ç”¨ `os` æ¨¡å—èŽ·å–å›¾åƒçš„å®Œæ•´è·¯å¾„ã€‚

## Conclusion

çŽ°åœ¨ï¼Œæ‚¨å·²ç»çŸ¥é“å¦‚ä½•é€šè¿‡æ•°åƒä¸ªè¿è¡Œåœ¨é‡Žå¤–çš„ gradio ç©ºé—´æ¥æ‰©å±•æ‚¨çš„ LLM çš„èƒ½åŠ›äº†ï¼
åŒæ ·ï¼Œæˆ‘ä»¬æ¬¢è¿Žå¯¹ [gradio_tools](https://github.com/freddyaboulton/gradio-tools) åº“çš„ä»»ä½•è´¡çŒ®ã€‚æˆ‘ä»¬å¾ˆå…´å¥‹çœ‹åˆ°å¤§å®¶æž„å»ºçš„å·¥å…·ï¼
```

---

<!-- Source: guides/cn/07_other-tutorials/create-your-own-friends-with-a-gan.md -->
# ä½¿ç”¨ GAN åˆ›å»ºæ‚¨è‡ªå·±çš„æœ‹å‹

spaces/NimaBoscarino/cryptopunks, https://huggingface.co/spaces/nateraw/cryptopunks-generator
Tags: GAN, IMAGE, HUB

ç”± <a href="https://huggingface.co/NimaBoscarino">Nima Boscarino</a> å’Œ <a href="https://huggingface.co/nateraw">Nate Raw</a> è´¡çŒ®

## ç®€ä»‹

æœ€è¿‘ï¼ŒåŠ å¯†è´§å¸ã€NFTs å’Œ Web3 è¿åŠ¨ä¼¼ä¹Žéƒ½éžå¸¸æµè¡Œï¼æ•°å­—èµ„äº§ä»¥æƒŠäººçš„é‡‘é¢åœ¨å¸‚åœºä¸Šä¸Šå¸‚ï¼Œå‡ ä¹Žæ¯ä¸ªåäººéƒ½æŽ¨å‡ºäº†è‡ªå·±çš„ NFT æ”¶è—ã€‚è™½ç„¶æ‚¨çš„åŠ å¯†èµ„äº§å¯èƒ½æ˜¯åº”ç¨Žçš„ï¼Œä¾‹å¦‚åœ¨åŠ æ‹¿å¤§ï¼ˆhttps://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/compliance/digital-currency/cryptocurrency-guide.htmlï¼‰ï¼Œä½†ä»Šå¤©æˆ‘ä»¬å°†æŽ¢ç´¢ä¸€äº›æœ‰è¶£ä¸”æ— ç¨Žçš„æ–¹æ³•æ¥ç”Ÿæˆè‡ªå·±çš„ä¸€ç³»åˆ—è¿‡ç¨‹ç”Ÿæˆçš„ CryptoPunksï¼ˆhttps://www.larvalabs.com/cryptopunksï¼‰ã€‚

ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼Œé€šå¸¸ç§°ä¸º GANsï¼Œæ˜¯ä¸€ç±»ç‰¹å®šçš„æ·±åº¦å­¦ä¹ æ¨¡åž‹ï¼Œæ—¨åœ¨é€šè¿‡å­¦ä¹ è¾“å…¥æ•°æ®é›†æ¥åˆ›å»ºï¼ˆç”Ÿæˆï¼ï¼‰ä¸ŽåŽŸå§‹è®­ç»ƒé›†ä¸­çš„å…ƒç´ å…·æœ‰ä»¤äººä¿¡æœçš„ç›¸ä¼¼æ€§çš„æ–°ææ–™ã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œç½‘ç«™[thispersondoesnotexist.com](https://thispersondoesnotexist.com/)é€šè¿‡åä¸º StyleGAN2 çš„æ¨¡åž‹ç”Ÿæˆäº†æ ©æ ©å¦‚ç”Ÿä½†æ˜¯åˆæˆçš„äººç‰©å›¾åƒè€Œè¿…é€Ÿèµ°çº¢ã€‚GANs åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸèŽ·å¾—äº†äººä»¬çš„å…³æ³¨ï¼ŒçŽ°åœ¨è¢«ç”¨äºŽç”Ÿæˆå„ç§å›¾åƒã€æ–‡æœ¬ç”šè‡³éŸ³ä¹ï¼

ä»Šå¤©æˆ‘ä»¬å°†ç®€è¦ä»‹ç» GAN çš„é«˜çº§ç›´è§‰ï¼Œç„¶åŽæˆ‘ä»¬å°†å›´ç»•ä¸€ä¸ªé¢„è®­ç»ƒçš„ GAN æž„å»ºä¸€ä¸ªå°åž‹æ¼”ç¤ºï¼Œçœ‹çœ‹è¿™ä¸€åˆ‡éƒ½æ˜¯æ€Žä¹ˆå›žäº‹ã€‚ä¸‹é¢æ˜¯æˆ‘ä»¬å°†è¦ç»„åˆçš„ä¸œè¥¿çš„ä¸€çž¥ï¼š

<iframe src="https://nimaboscarino-cryptopunks.hf.space" frameBorder="0" height="855" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

### å…ˆå†³æ¡ä»¶

ç¡®ä¿å·²ç»[å®‰è£…](/getting_started)äº† `gradio` Python åŒ…ã€‚è¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡åž‹ï¼Œè¯·è¿˜å®‰è£… `torch` å’Œ `torchvision`ã€‚

## GANsï¼šç®€ä»‹

æœ€åˆåœ¨[Goodfellow ç­‰äºº 2014 å¹´çš„è®ºæ–‡](https://arxiv.org/abs/1406.2661)ä¸­æå‡ºï¼ŒGANs ç”±äº’ç›¸ç«žäº‰çš„ç¥žç»ç½‘ç»œç»„æˆï¼Œæ—¨åœ¨ç›¸äº’æ™ºèƒ½åœ°æ¬ºéª—å¯¹æ–¹ã€‚ä¸€ç§ç½‘ç»œï¼Œç§°ä¸ºâ€œç”Ÿæˆå™¨â€ï¼Œè´Ÿè´£ç”Ÿæˆå›¾åƒã€‚å¦ä¸€ä¸ªç½‘ç»œï¼Œç§°ä¸ºâ€œé‰´åˆ«å™¨â€ï¼Œä»Žç”Ÿæˆå™¨ä¸€æ¬¡æŽ¥æ”¶ä¸€å¼ å›¾åƒï¼Œä»¥åŠæ¥è‡ªè®­ç»ƒæ•°æ®é›†çš„ **real çœŸå®ž**å›¾åƒã€‚ç„¶åŽï¼Œé‰´åˆ«å™¨å¿…é¡»çŒœæµ‹ï¼šå“ªå¼ å›¾åƒæ˜¯å‡çš„ï¼Ÿ

ç”Ÿæˆå™¨ä¸æ–­è®­ç»ƒä»¥åˆ›å»ºå¯¹é‰´åˆ«å™¨æ›´éš¾ä»¥è¯†åˆ«çš„å›¾åƒï¼Œè€Œé‰´åˆ«å™¨æ¯æ¬¡æ­£ç¡®æ£€æµ‹åˆ°ä¼ªé€ å›¾åƒæ—¶ï¼Œéƒ½ä¼šä¸ºç”Ÿæˆå™¨è®¾ç½®æ›´é«˜çš„é—¨æ§›ã€‚éšç€ç½‘ç»œä¹‹é—´çš„è¿™ç§ç«žäº‰ï¼ˆ**adversarial å¯¹æŠ—æ€§ï¼**ï¼‰ï¼Œç”Ÿæˆçš„å›¾åƒæ”¹å–„åˆ°äº†å¯¹äººçœ¼æ¥è¯´æ— æ³•åŒºåˆ†çš„åœ°æ­¥ï¼

å¦‚æžœæ‚¨æƒ³æ›´æ·±å…¥åœ°äº†è§£ GANsï¼Œå¯ä»¥å‚è€ƒ[Analytics Vidhya ä¸Šçš„è¿™ç¯‡ä¼˜ç§€æ–‡ç« ](https://www.analyticsvidhya.com/blog/2021/06/a-detailed-explanation-of-gan-with-implementation-using-tensorflow-and-keras/)æˆ–è¿™ä¸ª[PyTorch æ•™ç¨‹](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)ã€‚ä¸è¿‡ï¼ŒçŽ°åœ¨æˆ‘ä»¬å°†æ·±å…¥çœ‹ä¸€ä¸‹æ¼”ç¤ºï¼

## æ­¥éª¤ 1 - åˆ›å»ºç”Ÿæˆå™¨æ¨¡åž‹

è¦ä½¿ç”¨ GAN ç”Ÿæˆæ–°å›¾åƒï¼Œåªéœ€è¦ç”Ÿæˆå™¨æ¨¡åž‹ã€‚ç”Ÿæˆå™¨å¯ä»¥ä½¿ç”¨è®¸å¤šä¸åŒçš„æž¶æž„ï¼Œä½†æ˜¯å¯¹äºŽè¿™ä¸ªæ¼”ç¤ºï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„ GAN ç”Ÿæˆå™¨æ¨¡åž‹ï¼Œå…¶æž¶æž„å¦‚ä¸‹ï¼š

```python
from torch import nn

class Generator(nn.Module):
    # æœ‰å…³ncï¼Œnzå’Œngfçš„è§£é‡Šï¼Œè¯·å‚è§ä¸‹é¢çš„é“¾æŽ¥
    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs
    def __init__(self, nc=4, nz=100, ngf=64):
        super(Generator, self).__init__()
        self.network = nn.Sequential(
            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),
            nn.Tanh(),
        )

    def forward(self, input):
        output = self.network(input)
        return output
```

æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨æ¥è‡ª[æ­¤ repo çš„ @teddykoker](https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py#L90)çš„ç”Ÿæˆå™¨æ¨¡åž‹ï¼Œæ‚¨è¿˜å¯ä»¥åœ¨é‚£é‡Œçœ‹åˆ°åŽŸå§‹çš„é‰´åˆ«å™¨æ¨¡åž‹ç»“æž„ã€‚

åœ¨å®žä¾‹åŒ–æ¨¡åž‹ä¹‹åŽï¼Œæˆ‘ä»¬å°†åŠ è½½æ¥è‡ª Hugging Face Hub çš„æƒé‡ï¼Œå­˜å‚¨åœ¨[nateraw/cryptopunks-gan](https://huggingface.co/nateraw/cryptopunks-gan)ä¸­ï¼š

```python
from huggingface_hub import hf_hub_download
import torch

model = Generator()
weights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')
model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # å¦‚æžœæœ‰å¯ç”¨çš„GPUï¼Œè¯·ä½¿ç”¨'cuda'
```

## æ­¥éª¤ 2 - å®šä¹‰â€œpredictâ€å‡½æ•°

`predict` å‡½æ•°æ˜¯ä½¿ Gradio å·¥ä½œçš„å…³é”®ï¼æˆ‘ä»¬é€šè¿‡ Gradio ç•Œé¢é€‰æ‹©çš„ä»»ä½•è¾“å…¥éƒ½å°†é€šè¿‡æˆ‘ä»¬çš„ `predict` å‡½æ•°ä¼ é€’ï¼Œè¯¥å‡½æ•°åº”å¯¹è¾“å…¥è¿›è¡Œæ“ä½œå¹¶ç”Ÿæˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ Gradio è¾“å‡ºç»„ä»¶æ˜¾ç¤ºçš„è¾“å‡ºã€‚å¯¹äºŽ GANsï¼Œå¸¸è§çš„åšæ³•æ˜¯å°†éšæœºå™ªå£°ä¼ å…¥æˆ‘ä»¬çš„æ¨¡åž‹ä½œä¸ºè¾“å…¥ï¼Œå› æ­¤æˆ‘ä»¬å°†ç”Ÿæˆä¸€å¼ éšæœºæ•°çš„å¼ é‡å¹¶å°†å…¶ä¼ é€’ç»™æ¨¡åž‹ã€‚ç„¶åŽï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `torchvision` çš„ `save_image` å‡½æ•°å°†æ¨¡åž‹çš„è¾“å‡ºä¿å­˜ä¸º `png` æ–‡ä»¶ï¼Œå¹¶è¿”å›žæ–‡ä»¶åï¼š

```python
from torchvision.utils import save_image

def predict(seed):
    num_punks = 4
    torch.manual_seed(seed)
    z = torch.randn(num_punks, 100, 1, 1)
    punks = model(z)
    save_image(punks, "punks.png", normalize=True)
    return 'punks.png'
```

æˆ‘ä»¬ç»™ `predict` å‡½æ•°ä¸€ä¸ª `seed` å‚æ•°ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ä¸€ä¸ªç§å­å›ºå®šéšæœºå¼ é‡ç”Ÿæˆã€‚ç„¶åŽï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¼ å…¥ç›¸åŒçš„ç§å­å†æ¬¡æŸ¥çœ‹ç”Ÿæˆçš„ punksã€‚

_æ³¨æ„ï¼_ æˆ‘ä»¬çš„æ¨¡åž‹éœ€è¦ä¸€ä¸ª 100x1x1 çš„è¾“å…¥å¼ é‡è¿›è¡Œå•æ¬¡æŽ¨ç†ï¼Œæˆ–è€… (BatchSize)x100x1x1 æ¥ç”Ÿæˆä¸€æ‰¹å›¾åƒã€‚åœ¨è¿™ä¸ªæ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬æ¯æ¬¡ç”Ÿæˆ 4 ä¸ª punkã€‚

## ç¬¬ä¸‰æ­¥â€”åˆ›å»ºä¸€ä¸ª Gradio æŽ¥å£

æ­¤æ—¶ï¼Œæ‚¨ç”šè‡³å¯ä»¥è¿è¡Œæ‚¨æ‹¥æœ‰çš„ä»£ç  `predict(<SOME_NUMBER>)`ï¼Œå¹¶åœ¨æ‚¨çš„æ–‡ä»¶ç³»ç»Ÿä¸­æ‰¾åˆ°æ–°ç”Ÿæˆçš„ punk åœ¨ `./punks.png`ã€‚ç„¶è€Œï¼Œä¸ºäº†åˆ¶ä½œä¸€ä¸ªçœŸæ­£çš„äº¤äº’æ¼”ç¤ºï¼Œæˆ‘ä»¬å°†ç”¨ Gradio æž„å»ºä¸€ä¸ªç®€å•çš„ç•Œé¢ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ï¼š

- è®¾ç½®ä¸€ä¸ªæ»‘å—è¾“å…¥ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥é€‰æ‹©â€œseedâ€å€¼
- ä½¿ç”¨å›¾åƒç»„ä»¶ä½œä¸ºè¾“å‡ºï¼Œå±•ç¤ºç”Ÿæˆçš„ punk
- ä½¿ç”¨æˆ‘ä»¬çš„ `predict()` å‡½æ•°æ¥æŽ¥å—ç§å­å¹¶ç”Ÿæˆå›¾åƒ

é€šè¿‡ä½¿ç”¨ `gr.Interface()`ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå‡½æ•°è°ƒç”¨æ¥å®šä¹‰æ‰€æœ‰è¿™äº› :

```python
import gradio as gr

gr.Interface(
    predict,
    inputs=[
        gr.Slider(0, 1000, label='Seed', default=42),
    ],
    outputs="image",
).launch()
```

å¯åŠ¨ç•Œé¢åŽï¼Œæ‚¨åº”è¯¥ä¼šçœ‹åˆ°åƒè¿™æ ·çš„ä¸œè¥¿ :

<iframe src="https://nimaboscarino-cryptopunks-1.hf.space" frameBorder="0" height="365" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

## ç¬¬å››æ­¥â€”æ›´å¤š punkï¼

æ¯æ¬¡ç”Ÿæˆ 4 ä¸ª punk æ˜¯ä¸€ä¸ªå¥½çš„å¼€å§‹ï¼Œä½†æ˜¯ä¹Ÿè®¸æˆ‘ä»¬æƒ³æŽ§åˆ¶æ¯æ¬¡æƒ³ç”Ÿæˆå¤šå°‘ã€‚é€šè¿‡ç®€å•åœ°å‘æˆ‘ä»¬ä¼ é€’ç»™ `gr.Interface` çš„ `inputs` åˆ—è¡¨æ·»åŠ å¦ä¸€é¡¹å³å¯å‘æˆ‘ä»¬çš„ Gradio ç•Œé¢æ·»åŠ æ›´å¤šè¾“å…¥ :

```python
gr.Interface(
    predict,
    inputs=[
        gr.Slider(0, 1000, label='Seed', default=42),
        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # æ·»åŠ å¦ä¸€ä¸ªæ»‘å—!
    ],
    outputs="image",
).launch()
```

æ–°çš„è¾“å…¥å°†ä¼ é€’ç»™æˆ‘ä»¬çš„ `predict()` å‡½æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»å¯¹è¯¥å‡½æ•°è¿›è¡Œä¸€äº›æ›´æ”¹ï¼Œä»¥æŽ¥å—ä¸€ä¸ªæ–°çš„å‚æ•° :

```python
def predict(seed, num_punks):
    torch.manual_seed(seed)
    z = torch.randn(num_punks, 100, 1, 1)
    punks = model(z)
    save_image(punks, "punks.png", normalize=True)
    return 'punks.png'
```

å½“æ‚¨é‡æ–°å¯åŠ¨ç•Œé¢æ—¶ï¼Œæ‚¨åº”è¯¥ä¼šçœ‹åˆ°ä¸€ä¸ªç¬¬äºŒä¸ªæ»‘å—ï¼Œå®ƒå¯ä»¥è®©æ‚¨æŽ§åˆ¶ punk çš„æ•°é‡ï¼

## ç¬¬äº”æ­¥-å®Œå–„å®ƒ

æ‚¨çš„ Gradio åº”ç”¨å·²ç»å‡†å¤‡å¥½è¿è¡Œäº†ï¼Œä½†æ˜¯æ‚¨å¯ä»¥æ·»åŠ ä¸€äº›é¢å¤–çš„åŠŸèƒ½æ¥ä½¿å…¶çœŸæ­£å‡†å¤‡å¥½å‘å…‰ âœ¨

æˆ‘ä»¬å¯ä»¥æ·»åŠ ä¸€äº›ç”¨æˆ·å¯ä»¥è½»æ¾å°è¯•çš„ç¤ºä¾‹ï¼Œé€šè¿‡å°†å…¶æ·»åŠ åˆ° `gr.Interface` ä¸­å®žçŽ° :

```python
gr.Interface(
    # ...
    # å°†æ‰€æœ‰å†…å®¹ä¿æŒä¸å˜ï¼Œç„¶åŽæ·»åŠ 
    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],
).launch(cache_examples=True) # cache_examplesæ˜¯å¯é€‰çš„
```

`examples` å‚æ•°æŽ¥å—ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼Œå…¶ä¸­å­åˆ—è¡¨ä¸­çš„æ¯ä¸ªé¡¹ç›®çš„é¡ºåºä¸Žæˆ‘ä»¬åˆ—å‡ºçš„ `inputs` çš„é¡ºåºç›¸åŒã€‚æ‰€ä»¥åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œ`[seed, num_punks]`ã€‚è¯•ä¸€è¯•å§ï¼

æ‚¨è¿˜å¯ä»¥å°è¯•åœ¨ `gr.Interface` ä¸­æ·»åŠ  `title`ã€`description` å’Œ `article`ã€‚æ¯ä¸ªå‚æ•°éƒ½æŽ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ‰€ä»¥è¯•è¯•çœ‹å‘ç”Ÿäº†ä»€ä¹ˆðŸ‘€ `article` ä¹ŸæŽ¥å— HTMLï¼Œå¦‚[å‰é¢çš„æŒ‡å—](./key_features/#descriptive-content)æ‰€è¿°ï¼

å½“æ‚¨å®Œæˆæ‰€æœ‰æ“ä½œåŽï¼Œæ‚¨å¯èƒ½ä¼šå¾—åˆ°ç±»ä¼¼äºŽè¿™æ ·çš„ç»“æžœ :

<iframe src="https://nimaboscarino-cryptopunks.hf.space" frameBorder="0" height="855" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

ä¾›å‚è€ƒï¼Œè¿™æ˜¯æˆ‘ä»¬çš„å®Œæ•´ä»£ç  :

```python
import torch
from torch import nn
from huggingface_hub import hf_hub_download
from torchvision.utils import save_image
import gradio as gr

class Generator(nn.Module):
    # å…³äºŽncã€nzå’Œngfçš„è§£é‡Šï¼Œè¯·å‚è§ä¸‹é¢çš„é“¾æŽ¥
    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs
    def __init__(self, nc=4, nz=100, ngf=64):
        super(Generator, self).__init__()
        self.network = nn.Sequential(
            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),
            nn.Tanh(),
        )

    def forward(self, input):
        output = self.network(input)
        return output

model = Generator()
weights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')
model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # å¦‚æžœæ‚¨æœ‰å¯ç”¨çš„GPUï¼Œä½¿ç”¨'cuda'

def predict(seed, num_punks):
    torch.manual_seed(seed)
    z = torch.randn(num_punks, 100, 1, 1)
    punks = model(z)
    save_image(punks, "punks.png", normalize=True)
    return 'punks.png'

gr.Interface(
    predict,
    inputs=[
        gr.Slider(0, 1000, label='Seed', default=42),
        gr.Slider(4, 64, label='Number of Punks', step=1, default=10),
    ],
    outputs="image",
    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],
).launch(cache_examples=True)
```

---

æ­å–œï¼ä½ å·²ç»æˆåŠŸæž„å»ºäº†è‡ªå·±çš„åŸºäºŽ GAN çš„ CryptoPunks ç”Ÿæˆå™¨ï¼Œé…å¤‡äº†ä¸€ä¸ªæ—¶å°šçš„ Gradio ç•Œé¢ï¼Œä½¿ä»»ä½•äººéƒ½èƒ½è½»æ¾ä½¿ç”¨ã€‚çŽ°åœ¨ä½ å¯ä»¥åœ¨ Hub ä¸Š[å¯»æ‰¾æ›´å¤šçš„ GANs](https://huggingface.co/models?other=gan)ï¼ˆæˆ–è€…è‡ªå·±è®­ç»ƒï¼‰å¹¶ç»§ç»­åˆ¶ä½œæ›´å¤šä»¤äººèµžå¹çš„æ¼”ç¤ºé¡¹ç›®ã€‚ðŸ¤—

---

<!-- Source: guides/cn/07_other-tutorials/creating-a-chatbot.md -->
# å¦‚ä½•åˆ›å»ºä¸€ä¸ªèŠå¤©æœºå™¨äºº

Tags: NLP, TEXT, CHAT
Related spaces: https://huggingface.co/spaces/gradio/chatbot_streaming, https://huggingface.co/spaces/project-baize/Baize-7B,

## ç®€ä»‹

èŠå¤©æœºå™¨äººåœ¨è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ç ”ç©¶å’Œå·¥ä¸šç•Œè¢«å¹¿æ³›ä½¿ç”¨ã€‚ç”±äºŽèŠå¤©æœºå™¨äººæ˜¯ç›´æŽ¥ç”±å®¢æˆ·å’Œæœ€ç»ˆç”¨æˆ·ä½¿ç”¨çš„ï¼Œå› æ­¤éªŒè¯èŠå¤©æœºå™¨äººåœ¨é¢å¯¹å„ç§è¾“å…¥æç¤ºæ—¶çš„è¡Œä¸ºæ˜¯å¦ç¬¦åˆé¢„æœŸè‡³å…³é‡è¦ã€‚

é€šè¿‡ä½¿ç”¨ `gradio`ï¼Œæ‚¨å¯ä»¥è½»æ¾æž„å»ºèŠå¤©æœºå™¨äººæ¨¡åž‹çš„æ¼”ç¤ºï¼Œå¹¶ä¸Žç”¨æˆ·å…±äº«ï¼Œæˆ–ä½¿ç”¨ç›´è§‚çš„èŠå¤©æœºå™¨äººå›¾å½¢ç•Œé¢è‡ªå·±å°è¯•ã€‚

æœ¬æ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Gradio åˆ¶ä½œå‡ ç§ä¸åŒç±»åž‹çš„èŠå¤©æœºå™¨äººç”¨æˆ·ç•Œé¢ï¼šé¦–å…ˆæ˜¯ä¸€ä¸ªç®€å•çš„æ–‡æœ¬æ˜¾ç¤ºç•Œé¢ï¼Œå…¶æ¬¡æ˜¯ä¸€ä¸ªç”¨äºŽæµå¼æ–‡æœ¬å“åº”çš„ç•Œé¢ï¼Œæœ€åŽä¸€ä¸ªæ˜¯å¯ä»¥å¤„ç†åª’ä½“æ–‡ä»¶çš„èŠå¤©æœºå™¨äººã€‚æˆ‘ä»¬åˆ›å»ºçš„èŠå¤©æœºå™¨äººç•Œé¢å°†å¦‚ä¸‹æ‰€ç¤ºï¼š

$ æ¼”ç¤º _ èŠå¤©æœºå™¨äºº _ æµå¼

**å…ˆå†³æ¡ä»¶**ï¼šæˆ‘ä»¬å°†ä½¿ç”¨ `gradio.Blocks` ç±»æ¥æž„å»ºæˆ‘ä»¬çš„èŠå¤©æœºå™¨äººæ¼”ç¤ºã€‚
å¦‚æžœæ‚¨å¯¹æ­¤è¿˜ä¸ç†Ÿæ‚‰ï¼Œå¯ä»¥[å…ˆé˜…è¯» Blocks æŒ‡å—](https://gradio.app/blocks-and-event-listeners)ã€‚åŒæ—¶ï¼Œè¯·ç¡®ä¿æ‚¨ä½¿ç”¨çš„æ˜¯**æœ€æ–°ç‰ˆæœ¬**çš„ Gradioï¼š`pip install --upgrade gradio`ã€‚

## ç®€å•èŠå¤©æœºå™¨äººæ¼”ç¤º

è®©æˆ‘ä»¬ä»Žé‡æ–°åˆ›å»ºä¸Šé¢çš„ç®€å•æ¼”ç¤ºå¼€å§‹ã€‚æ­£å¦‚æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°çš„ï¼Œæˆ‘ä»¬çš„æœºå™¨äººåªæ˜¯éšæœºå¯¹ä»»ä½•è¾“å…¥å›žå¤ " ä½ å¥½å—ï¼Ÿ"ã€" æˆ‘çˆ±ä½  " æˆ– " æˆ‘éžå¸¸é¥¿ "ã€‚è¿™æ˜¯ä½¿ç”¨ Gradio åˆ›å»ºæ­¤æ¼”ç¤ºçš„ä»£ç ï¼š

$ ä»£ç  \_ ç®€å•èŠå¤©æœºå™¨äºº

è¿™é‡Œæœ‰ä¸‰ä¸ª Gradio ç»„ä»¶ï¼š

- ä¸€ä¸ª `Chatbot`ï¼Œå…¶å€¼å°†æ•´ä¸ªå¯¹è¯çš„åŽ†å²è®°å½•ä½œä¸ºç”¨æˆ·å’Œæœºå™¨äººä¹‹é—´çš„å“åº”å¯¹åˆ—è¡¨å­˜å‚¨ã€‚
- ä¸€ä¸ªæ–‡æœ¬æ¡†ï¼Œç”¨æˆ·å¯ä»¥åœ¨å…¶ä¸­é”®å…¥ä»–ä»¬çš„æ¶ˆæ¯ï¼Œç„¶åŽæŒ‰ä¸‹ Enter/ æäº¤ä»¥è§¦å‘èŠå¤©æœºå™¨äººçš„å“åº”
- ä¸€ä¸ª `ClearButton` æŒ‰é’®ï¼Œç”¨äºŽæ¸…é™¤æ–‡æœ¬æ¡†å’Œæ•´ä¸ªèŠå¤©æœºå™¨äººçš„åŽ†å²è®°å½•

æˆ‘ä»¬æœ‰ä¸€ä¸ªåä¸º `respond()` çš„å‡½æ•°ï¼Œå®ƒæŽ¥æ”¶èŠå¤©æœºå™¨äººçš„æ•´ä¸ªåŽ†å²è®°å½•ï¼Œé™„åŠ ä¸€ä¸ªéšæœºæ¶ˆæ¯ï¼Œç­‰å¾… 1 ç§’ï¼Œç„¶åŽè¿”å›žæ›´æ–°åŽçš„èŠå¤©åŽ†å²è®°å½•ã€‚`respond()` å‡½æ•°åœ¨è¿”å›žæ—¶è¿˜æ¸…é™¤äº†æ–‡æœ¬æ¡†ã€‚

å½“ç„¶ï¼Œå®žé™…ä¸Šï¼Œæ‚¨ä¼šç”¨è‡ªå·±æ›´å¤æ‚çš„å‡½æ•°æ›¿æ¢ `respond()`ï¼Œè¯¥å‡½æ•°å¯èƒ½è°ƒç”¨é¢„è®­ç»ƒæ¨¡åž‹æˆ– API æ¥ç”Ÿæˆå“åº”ã€‚

$ æ¼”ç¤º \_ ç®€å•èŠå¤©æœºå™¨äºº

## ä¸ºèŠå¤©æœºå™¨äººæ·»åŠ æµå¼å“åº”

æˆ‘ä»¬å¯ä»¥é€šè¿‡å‡ ç§æ–¹å¼æ¥æ”¹è¿›ä¸Šè¿°èŠå¤©æœºå™¨äººçš„ç”¨æˆ·ä½“éªŒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥æµå¼ä¼ è¾“å“åº”ï¼Œä»¥ä¾¿ç”¨æˆ·ä¸å¿…ç­‰å¾…å¤ªé•¿æ—¶é—´æ‰èƒ½ç”Ÿæˆæ¶ˆæ¯ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¯ä»¥è®©ç”¨æˆ·çš„æ¶ˆæ¯åœ¨èŠå¤©åŽ†å²è®°å½•ä¸­ç«‹å³å‡ºçŽ°ï¼ŒåŒæ—¶èŠå¤©æœºå™¨äººçš„å“åº”æ­£åœ¨ç”Ÿæˆã€‚ä»¥ä¸‹æ˜¯å®žçŽ°è¿™ä¸€ç‚¹çš„ä»£ç ï¼š

$code_chatbot_streaming

å½“ç”¨æˆ·æäº¤ä»–ä»¬çš„æ¶ˆæ¯æ—¶ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°æˆ‘ä»¬çŽ°åœ¨ä½¿ç”¨ `.then()` ä¸Žä¸‰ä¸ªäº‹ä»¶äº‹ä»¶ _é“¾_ èµ·æ¥ï¼š

1. ç¬¬ä¸€ä¸ªæ–¹æ³• `user()` ç”¨ç”¨æˆ·æ¶ˆæ¯æ›´æ–°èŠå¤©æœºå™¨äººå¹¶æ¸…é™¤è¾“å…¥å­—æ®µã€‚æ­¤æ–¹æ³•è¿˜ä½¿è¾“å…¥å­—æ®µå¤„äºŽéžäº¤äº’çŠ¶æ€ï¼Œä»¥é˜²èŠå¤©æœºå™¨äººæ­£åœ¨å“åº”æ—¶ç”¨æˆ·å‘é€å¦ä¸€æ¡æ¶ˆæ¯ã€‚ç”±äºŽæˆ‘ä»¬å¸Œæœ›æ­¤æ“ä½œç«‹å³å‘ç”Ÿï¼Œå› æ­¤æˆ‘ä»¬è®¾ç½® `queue=False`ï¼Œä»¥è·³è¿‡ä»»ä½•å¯èƒ½çš„é˜Ÿåˆ—ã€‚èŠå¤©æœºå™¨äººçš„åŽ†å²è®°å½•é™„åŠ äº†`(user_message, None)`ï¼Œå…¶ä¸­çš„ `None` è¡¨ç¤ºæœºå™¨äººæœªä½œå‡ºå“åº”ã€‚

2. ç¬¬äºŒä¸ªæ–¹æ³• `bot()` ä½¿ç”¨æœºå™¨äººçš„å“åº”æ›´æ–°èŠå¤©æœºå™¨äººçš„åŽ†å²è®°å½•ã€‚æˆ‘ä»¬ä¸æ˜¯åˆ›å»ºæ–°æ¶ˆæ¯ï¼Œè€Œæ˜¯å°†å…ˆå‰åˆ›å»ºçš„ `None` æ¶ˆæ¯æ›¿æ¢ä¸ºæœºå™¨äººçš„å“åº”ã€‚æœ€åŽï¼Œæˆ‘ä»¬é€ä¸ªå­—ç¬¦æž„é€ æ¶ˆæ¯å¹¶ `yield` æ­£åœ¨æž„å»ºçš„ä¸­é—´è¾“å‡ºã€‚Gradio ä¼šè‡ªåŠ¨å°†å¸¦æœ‰ `yield` å…³é”®å­—çš„ä»»ä½•å‡½æ•° [è½¬æ¢ä¸ºæµå¼è¾“å‡ºæŽ¥å£](/key-features/#iterative-outputs)ã€‚

3. ç¬¬ä¸‰ä¸ªæ–¹æ³•ä½¿è¾“å…¥å­—æ®µå†æ¬¡å¯ä»¥äº¤äº’ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥å‘æœºå™¨äººå‘é€å¦ä¸€æ¡æ¶ˆæ¯ã€‚

å½“ç„¶ï¼Œå®žé™…ä¸Šï¼Œæ‚¨ä¼šç”¨è‡ªå·±æ›´å¤æ‚çš„å‡½æ•°æ›¿æ¢ `bot()`ï¼Œè¯¥å‡½æ•°å¯èƒ½è°ƒç”¨é¢„è®­ç»ƒæ¨¡åž‹æˆ– API æ¥ç”Ÿæˆå“åº”ã€‚

æœ€åŽï¼Œæˆ‘ä»¬é€šè¿‡è¿è¡Œ `demo.queue()` å¯ç”¨æŽ’é˜Ÿï¼Œè¿™å¯¹äºŽæµå¼ä¸­é—´è¾“å‡ºæ˜¯å¿…éœ€çš„ã€‚æ‚¨å¯ä»¥é€šè¿‡æ»šåŠ¨åˆ°æœ¬é¡µé¢é¡¶éƒ¨çš„æ¼”ç¤ºæ¥å°è¯•æ”¹è¿›åŽçš„èŠå¤©æœºå™¨äººã€‚

## æ·»åŠ  Markdownã€å›¾ç‰‡ã€éŸ³é¢‘æˆ–è§†é¢‘

`gr.Chatbot` ç»„ä»¶æ”¯æŒåŒ…å«åŠ ç²—ã€æ–œä½“å’Œä»£ç ç­‰ä¸€éƒ¨åˆ† Markdown åŠŸèƒ½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œä»¥ç²—ä½“å›žå¤ç”¨æˆ·çš„æ¶ˆæ¯ï¼Œç±»ä¼¼äºŽ **That's cool!**ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
def bot(history):
    response = "**That's cool!**"
    history[-1][1] = response
    return history
```

æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥å¤„ç†å›¾ç‰‡ã€éŸ³é¢‘å’Œè§†é¢‘ç­‰åª’ä½“æ–‡ä»¶ã€‚è¦ä¼ é€’åª’ä½“æ–‡ä»¶ï¼Œæˆ‘ä»¬å¿…é¡»å°†æ–‡ä»¶ä½œä¸ºä¸¤ä¸ªå­—ç¬¦ä¸²çš„å…ƒç»„ä¼ é€’ï¼Œå¦‚`(filepath, alt_text)` æ‰€ç¤ºã€‚`alt_text` æ˜¯å¯é€‰çš„ï¼Œå› æ­¤æ‚¨è¿˜å¯ä»¥åªä¼ å…¥åªæœ‰ä¸€ä¸ªå…ƒç´ çš„å…ƒç»„`(filepath,)`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
def add_file(history, file):
    history = history + [((file.name,), None)]
    return history
```

å°†æ‰€æœ‰è¿™äº›æ”¾åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ª*å¤šæ¨¡æ€*èŠå¤©æœºå™¨äººï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæ–‡æœ¬æ¡†ä¾›ç”¨æˆ·æäº¤æ–‡æœ¬ï¼Œä»¥åŠä¸€ä¸ªæ–‡ä»¶ä¸Šä¼ æŒ‰é’®ä¾›æäº¤å›¾åƒ / éŸ³é¢‘ / è§†é¢‘æ–‡ä»¶ã€‚ä½™ä¸‹çš„ä»£ç çœ‹èµ·æ¥ä¸Žä¹‹å‰çš„ä»£ç å‡ ä¹Žç›¸åŒï¼š

$code_chatbot_multimodal
$demo_chatbot_multimodal

ä½ å®Œæˆäº†ï¼è¿™å°±æ˜¯æž„å»ºèŠå¤©æœºå™¨äººæ¨¡åž‹ç•Œé¢æ‰€éœ€çš„æ‰€æœ‰ä»£ç ã€‚æœ€åŽï¼Œæˆ‘ä»¬å°†ç»“æŸæˆ‘ä»¬çš„æŒ‡å—ï¼Œå¹¶æä¾›ä¸€äº›åœ¨ Spaces ä¸Šè¿è¡Œçš„èŠå¤©æœºå™¨äººçš„é“¾æŽ¥ï¼Œä»¥è®©ä½ äº†è§£å…¶ä»–å¯èƒ½æ€§ï¼š

- [project-baize/Baize-7B](https://huggingface.co/spaces/project-baize/Baize-7B)ï¼šä¸€ä¸ªå¸¦æœ‰åœæ­¢ç”Ÿæˆå’Œé‡æ–°ç”Ÿæˆå“åº”åŠŸèƒ½çš„æ ·å¼åŒ–èŠå¤©æœºå™¨äººã€‚
- [MAGAer13/mPLUG-Owl](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl)ï¼šä¸€ä¸ªå¤šæ¨¡æ€èŠå¤©æœºå™¨äººï¼Œå…è®¸æ‚¨å¯¹å“åº”è¿›è¡ŒæŠ•ç¥¨ã€‚

---

<!-- Source: guides/cn/07_other-tutorials/creating-a-new-component.md -->
# å¦‚ä½•åˆ›å»ºä¸€ä¸ªæ–°ç»„ä»¶

## ç®€ä»‹

æœ¬æŒ‡å—æ—¨åœ¨è¯´æ˜Žå¦‚ä½•æ·»åŠ ä¸€ä¸ªæ–°ç»„ä»¶ï¼Œä½ å¯ä»¥åœ¨ Gradio åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨è¯¥ç»„ä»¶ã€‚è¯¥æŒ‡å—å°†é€šè¿‡ä»£ç ç‰‡æ®µé€æ­¥å±•ç¤ºå¦‚ä½•æ·»åŠ [ColorPicker](https://gradio.app/docs/colorpicker)ç»„ä»¶ã€‚

## å…ˆå†³æ¡ä»¶

ç¡®ä¿æ‚¨å·²ç»æŒ‰ç…§[CONTRIBUTING.md](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md)æŒ‡å—è®¾ç½®äº†æœ¬åœ°å¼€å‘çŽ¯å¢ƒï¼ˆåŒ…æ‹¬å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ç«¯ï¼‰ã€‚

ä»¥ä¸‹æ˜¯åœ¨ Gradio ä¸Šåˆ›å»ºæ–°ç»„ä»¶çš„æ­¥éª¤ï¼š

1. [åˆ›å»ºä¸€ä¸ªæ–°çš„ Python ç±»å¹¶å¯¼å…¥å®ƒ](#1-create-a-new-python-class-and-import-it)
2. [åˆ›å»ºä¸€ä¸ªæ–°çš„ Svelte ç»„ä»¶](#2-create-a-new-svelte-component)
3. [åˆ›å»ºä¸€ä¸ªæ–°çš„æ¼”ç¤º](#3-create-a-new-demo)

## 1. åˆ›å»ºä¸€ä¸ªæ–°çš„ Python ç±»å¹¶å¯¼å…¥å®ƒ

é¦–å…ˆè¦åšçš„æ˜¯åœ¨[components.py](https://github.com/gradio-app/gradio/blob/main/gradio/components.py)æ–‡ä»¶ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„ç±»ã€‚è¿™ä¸ª Python ç±»åº”è¯¥ç»§æ‰¿è‡ªä¸€ç³»åˆ—çš„åŸºæœ¬ç»„ä»¶ï¼Œå¹¶ä¸”åº”è¯¥æ ¹æ®è¦æ·»åŠ çš„ç»„ä»¶çš„ç±»åž‹ï¼ˆä¾‹å¦‚è¾“å…¥ã€è¾“å‡ºæˆ–é™æ€ç»„ä»¶ï¼‰å°†å…¶æ”¾ç½®åœ¨æ–‡ä»¶ä¸­çš„æ­£ç¡®éƒ¨åˆ†ã€‚
ä¸€èˆ¬æ¥è¯´ï¼Œå»ºè®®å‚è€ƒçŽ°æœ‰çš„ç»„ä»¶ï¼ˆä¾‹å¦‚[TextBox](https://github.com/gradio-app/gradio/blob/main/gradio/components.py#L290)ï¼‰ï¼Œå°†å…¶ä»£ç å¤åˆ¶ä¸ºéª¨æž¶ï¼Œç„¶åŽæ ¹æ®å®žé™…æƒ…å†µè¿›è¡Œä¿®æ”¹ã€‚

è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æ·»åŠ åˆ°[components.py](https://github.com/gradio-app/gradio/blob/main/gradio/components.py)æ–‡ä»¶ä¸­çš„ ColorPicker ç»„ä»¶çš„ç±»ï¼š

```python
@document()
class ColorPicker(Changeable, Submittable, IOComponent):
    """
    åˆ›å»ºä¸€ä¸ªé¢œè‰²é€‰æ‹©å™¨ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©é¢œè‰²ä½œä¸ºå­—ç¬¦ä¸²è¾“å…¥ã€‚
    é¢„å¤„ç†ï¼šå°†é€‰æ‹©çš„é¢œè‰²å€¼ä½œä¸º{str}ä¼ é€’ç»™å‡½æ•°ã€‚
    åŽå¤„ç†ï¼šæœŸæœ›ä»Žå‡½æ•°ä¸­è¿”å›žä¸€ä¸ª{str}ï¼Œå¹¶å°†é¢œè‰²é€‰æ‹©å™¨çš„å€¼è®¾ç½®ä¸ºå®ƒã€‚
    ç¤ºä¾‹æ ¼å¼ï¼šè¡¨ç¤ºé¢œè‰²çš„åå…­è¿›åˆ¶{str}ï¼Œä¾‹å¦‚çº¢è‰²çš„"#ff0000"ã€‚
    æ¼”ç¤ºï¼šcolor_pickerï¼Œcolor_generator
    """

    def __init__(
        self,
        value: str = None,
        *,
        label: Optional[str] = None,
        show_label: bool = True,
        interactive: Optional[bool] = None,
        visible: bool = True,
        elem_id: Optional[str] = None,
        **kwargs,
    ):
        """
        Parameters:
        """
        Parameters:
            value: default text to provide in color picker.
            label: component name in interface.
            show_label: if True, will display label.
            interactive: if True, will be rendered as an editable color picker; if False, editing will be disabled. If not provided, this is inferred based on whether the component is used as an input or output.
            visible: If False, component will be hidden.
            elem_id: An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.
        """
        self.value = self.postprocess(value)
        self.cleared_value = "#000000"
        self.test_input = value
        IOComponent.__init__(
            self,
            label=label,
            show_label=show_label,
            interactive=interactive,
            visible=visible,
            elem_id=elem_id,
            **kwargs,
        )

    def get_config(self):
        return {
            "value": self.value,
            **IOComponent.get_config(self),
        }

    @staticmethod
    def update(
        value: Optional[Any] = None,
        label: Optional[str] = None,
        show_label: Optional[bool] = None,
        visible: Optional[bool] = None,
        interactive: Optional[bool] = None,
    ):
        return {
            "value": value,
            "label": label,
            "show_label": show_label,
            "visible": visible,
            "interactive": interactive,
            "__type__": "update",
        }

    # è¾“å…¥åŠŸèƒ½
    def preprocess(self, x: str | None) -> Any:
        """
        Any preprocessing needed to be performed on function input.
        Parameters:
        x (str): text
        Returns:
        (str): text
        """
        if x is None:
            return None
        else:
            return str(x)

    def preprocess_example(self, x: str | None) -> Any:
        """
        åœ¨ä¼ é€’ç»™ä¸»å‡½æ•°ä¹‹å‰ï¼Œå¯¹ç¤ºä¾‹è¿›è¡Œä»»ä½•é¢„å¤„ç†ã€‚
        """
        if x is None:
            return None
        else:
            return str(x)

    # è¾“å‡ºåŠŸèƒ½
    def postprocess(self, y: str | None):
        """
        Any postprocessing needed to be performed on function output.
        Parameters:
        y (str | None): text
        Returns:
        (str | None): text
        """
        if y is None:
            return None
        else:
            return str(y)

    def deserialize(self, x):
        """
        å°†ä»Žè°ƒç”¨æŽ¥å£çš„åºåˆ—åŒ–è¾“å‡ºï¼ˆä¾‹å¦‚base64è¡¨ç¤ºï¼‰è½¬æ¢ä¸ºè¾“å‡ºçš„äººç±»å¯è¯»ç‰ˆæœ¬ï¼ˆå›¾åƒçš„è·¯å¾„ç­‰ï¼‰
        """
        return x
```

ä¸€æ—¦å®šä¹‰å®Œï¼Œå°±éœ€è¦åœ¨[\_\_init\_\_](https://github.com/gradio-app/gradio/blob/main/gradio/__init__.py)æ¨¡å—ç±»ä¸­å¯¼å…¥æ–°ç±»ï¼Œä»¥ä½¿å…¶å¯è§ã€‚

```python

from gradio.components import (
    ...
    ColorPicker,
    ...
)

```

### 1.1 ä¸º Python ç±»ç¼–å†™å•å…ƒæµ‹è¯•

åœ¨å¼€å‘æ–°ç»„ä»¶æ—¶ï¼Œè¿˜åº”ä¸ºå…¶ç¼–å†™ä¸€å¥—å•å…ƒæµ‹è¯•ã€‚è¿™äº›æµ‹è¯•åº”è¯¥æ”¾åœ¨[gradio/test/test_components.py](https://github.com/gradio-app/gradio/blob/main/test/test_components.py)æ–‡ä»¶ä¸­ã€‚åŒæ ·ï¼Œå¦‚ä¸Šæ‰€è¿°ï¼Œå‚è€ƒå…¶ä»–ç»„ä»¶çš„æµ‹è¯•ï¼ˆä¾‹å¦‚[Textbox](https://github.com/gradio-app/gradio/blob/main/test/test_components.py)ï¼‰å¹¶æ·»åŠ å°½å¯èƒ½å¤šçš„å•å…ƒæµ‹è¯•ï¼Œä»¥æµ‹è¯•æ–°ç»„ä»¶çš„æ‰€æœ‰ä¸åŒæ–¹é¢å’ŒåŠŸèƒ½ã€‚ä¾‹å¦‚ï¼Œä¸º ColorPicker ç»„ä»¶æ·»åŠ äº†ä»¥ä¸‹æµ‹è¯•ï¼š

```python
class TestColorPicker(unittest.TestCase):
    def test_component_functions(self):
        """
        Preprocess, postprocess, serialize, save_flagged, restore_flagged, tokenize, get_config
        """
        color_picker_input = gr.ColorPicker()
        self.assertEqual(color_picker_input.preprocess("#000000"), "#000000")
        self.assertEqual(color_picker_input.preprocess_example("#000000"), "#000000")
        self.assertEqual(color_picker_input.postprocess(None), None)
        self.assertEqual(color_picker_input.postprocess("#FFFFFF"), "#FFFFFF")
        self.assertEqual(color_picker_input.serialize("#000000", True), "#000000")

        color_picker_input.interpretation_replacement = "unknown"

        self.assertEqual(
            color_picker_input.get_config(),
            {
                "value": None,
                "show_label": True,
                "label": None,
                "style": {},
                "elem_id": None,
                "visible": True,
                "interactive": None,
                "name": "colorpicker",
            },
        )

    def test_in_interface_as_input(self):
        """
        æŽ¥å£ã€å¤„ç†ã€è§£é‡Š
        """
        iface = gr.Interface(lambda x: x, "colorpicker", "colorpicker")
        self.assertEqual(iface.process(["#000000"]), ["#000000"])

    def test_in_interface_as_output(self):
        """
        æŽ¥å£ã€å¤„ç†

        """
        iface = gr.Interface(lambda x: x, "colorpicker", gr.ColorPicker())
        self.assertEqual(iface.process(["#000000"]), ["#000000"])

    def test_static(self):
        """
        åŽå¤„ç†
        """
        component = gr.ColorPicker("#000000")
        self.assertEqual(component.get_config().get("value"), "#000000")
```

## 2. åˆ›å»ºä¸€ä¸ªæ–°çš„ Svelte ç»„ä»¶

è®©æˆ‘ä»¬æ¥çœ‹çœ‹åˆ›å»ºæ–°ç»„ä»¶çš„å‰ç«¯å¹¶å°†å…¶ä¸Žå…¶ Python ä»£ç æ˜ å°„èµ·æ¥çš„æ­¥éª¤ï¼š

- åœ¨ [js æ–‡ä»¶å¤¹](https://github.com/gradio-app/gradio/tree/main/js/) ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„ UI-side Svelte ç»„ä»¶ï¼Œå¹¶ç¡®å®šè¦æ”¾ç½®åœ¨ä»€ä¹ˆåœ°æ–¹ã€‚é€‰é¡¹åŒ…æ‹¬ï¼šåˆ›å»ºæ–°ç»„ä»¶çš„åŒ…ï¼ˆå¦‚æžœä¸ŽçŽ°æœ‰ç»„ä»¶å®Œå…¨ä¸åŒï¼‰ï¼Œæˆ–å°†æ–°ç»„ä»¶æ·»åŠ åˆ°çŽ°æœ‰åŒ…ä¸­ï¼Œä¾‹å¦‚ [form åŒ…](https://github.com/gradio-app/gradio/tree/main/js/form)ã€‚ä¾‹å¦‚ï¼ŒColorPicker ç»„ä»¶è¢«åŒ…å«åœ¨ form åŒ…ä¸­ï¼Œå› ä¸ºå®ƒä¸Žå·²å­˜åœ¨çš„ç»„ä»¶ç›¸ä¼¼ã€‚
- åœ¨æ‚¨å°† Svelte ç»„ä»¶æ”¾ç½®çš„åŒ…çš„ src æ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ªå¸¦æœ‰é€‚å½“åç§°çš„æ–‡ä»¶ï¼Œæ³¨æ„ï¼šåç§°å¿…é¡»ä»¥å¤§å†™å­—æ¯å¼€å¤´ã€‚è¿™æ˜¯â€œæ ¸å¿ƒâ€ç»„ä»¶ï¼Œæ˜¯æ²¡æœ‰ Gradio ç‰¹å®šåŠŸèƒ½äº†è§£çš„é€šç”¨ç»„ä»¶ã€‚æœ€åˆï¼Œå°†ä»»ä½•æ–‡æœ¬ /HTML æ·»åŠ åˆ°æ­¤æ–‡ä»¶ï¼Œä»¥ä¾¿ç»„ä»¶å‘ˆçŽ°ä»»ä½•å†…å®¹ã€‚ColorPicker çš„ Svelte åº”ç”¨ç¨‹åºä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```typescript
<script lang="ts">
	import { createEventDispatcher } from "svelte";
	import { get_styles } from "@gradio/utils";
	import { BlockTitle } from "@gradio/atoms";
	import type { Styles } from "@gradio/utils";

	export let value: string = "#000000";
	export let style: Styles = {};
	export let label: string;
	export let disabled = false;
	export let show_label: boolean = true;

	$: value;
	$: handle_change(value);

	const dispatch = createEventDispatcher<{
		change: string;
		submit: undefined;
	}>();

	function handle_change(val: string) {
		dispatch("change", val);
	}

	$: ({ styles } = get_styles(style, ["rounded", "border"]));
</script>

<!-- svelte-ignore a11y-label-has-associated-control -->
<label class="block">
	<BlockTitle {show_label}>{label}</BlockTitle>
	<input
		type="color"
		class="gr-box-unrounded {classes}"
		bind:value
		{disabled}
	/>
</label>
```

- é€šè¿‡æ‰§è¡Œ `export { default as FileName } from "./FileName.svelte"`ï¼Œåœ¨æ‚¨å°† Svelte ç»„ä»¶æ”¾ç½®çš„åŒ…çš„ index.ts æ–‡ä»¶ä¸­å¯¼å‡ºæ­¤æ–‡ä»¶ã€‚ä¾‹å¦‚ï¼Œåœ¨ [index.ts](https://github.com/gradio-app/gradio/blob/main/js/form/src/index.ts) æ–‡ä»¶ä¸­å¯¼å‡ºäº† ColorPicker æ–‡ä»¶ï¼Œå¹¶é€šè¿‡ `export { default as ColorPicker } from "./ColorPicker.svelte";` æ‰§è¡Œå¯¼å‡ºã€‚
- åˆ›å»º [js/app/src/components](https://github.com/gradio-app/gradio/tree/main/js/app/src/components) ä¸­çš„ Gradio ç‰¹å®šç»„ä»¶ã€‚è¿™æ˜¯ä¸€ä¸ª Gradio åŒ…è£…å™¨ï¼Œå¤„ç†åº“çš„ç‰¹å®šé€»è¾‘ï¼Œå°†å¿…è¦çš„æ•°æ®ä¼ é€’ç»™æ ¸å¿ƒç»„ä»¶ï¼Œå¹¶é™„åŠ ä»»ä½•å¿…è¦çš„äº‹ä»¶ç›‘å¬å™¨ã€‚å¤åˆ¶å¦ä¸€ä¸ªç»„ä»¶çš„æ–‡ä»¶å¤¹ï¼Œé‡æ–°å‘½åå¹¶ç¼–è¾‘å…¶ä¸­çš„ä»£ç ï¼Œä¿æŒç»“æž„ä¸å˜ã€‚

åœ¨è¿™é‡Œï¼Œæ‚¨å°†æ‹¥æœ‰ä¸‰ä¸ªæ–‡ä»¶ï¼Œç¬¬ä¸€ä¸ªæ–‡ä»¶ç”¨äºŽ Svelte åº”ç”¨ç¨‹åºï¼Œå…·ä½“å¦‚ä¸‹æ‰€ç¤ºï¼š

```typescript
<svelte:options accessors={true} />

<script lang="ts">
	import { ColorPicker } from "@gradio/form";
	import { Block } from "@gradio/atoms";
	import StatusTracker from "../StatusTracker/StatusTracker.svelte";
	import type { LoadingStatus } from "../StatusTracker/types";
	import type { Styles } from "@gradio/utils";

	export let label: string = "ColorPicker";
	export let elem_id: string = "";
	export let visible: boolean = true;
	export let value: string;
	export let form_position: "first" | "last" | "mid" | "single" = "single";
	export let show_label: boolean;

	export let style: Styles = {};

	export let loading_status: LoadingStatus;

	export let interactive: boolean;
</script>

<Block
	{visible}
	{form_position}
	{elem_id}
	disable={typeof style.container === "boolean" && !style.container}
>
	<StatusTracker {...loading_status} />

	<ColorPicker
		{style}
		bind:value
		{label}
		{show_label}
		on:change
		on:submit
		disabled={!interactive}
	/>
</Block>
```

ç¬¬äºŒä¸ªæ–‡ä»¶åŒ…å«äº†å‰ç«¯çš„æµ‹è¯•ï¼Œä¾‹å¦‚ ColorPicker ç»„ä»¶çš„æµ‹è¯•ï¼š

```typescript
import { test, describe, assert, afterEach } from "vitest";
import { cleanup, render } from "@self/tootils";

import ColorPicker from "./ColorPicker.svelte";
import type { LoadingStatus } from "../StatusTracker/types";

const loading_status = {
	eta: 0,
	queue_position: 1,
	status: "complete" as LoadingStatus["status"],
	scroll_to_output: false,
	visible: true,
	fn_index: 0
};

describe("ColorPicker", () => {
	afterEach(() => cleanup());

	test("renders provided value", () => {
		const { getByDisplayValue } = render(ColorPicker, {
			loading_status,
			show_label: true,
			interactive: true,
			value: "#000000",
			label: "ColorPicker"
		});

		const item: HTMLInputElement = getByDisplayValue("#000000");
		assert.equal(item.value, "#000000");
	});

	test("changing the color should update the value", async () => {
		const { component, getByDisplayValue } = render(ColorPicker, {
			loading_status,
			show_label: true,
			interactive: true,
			value: "#000000",
			label: "ColorPicker"
		});

		const item: HTMLInputElement = getByDisplayValue("#000000");

		assert.equal(item.value, "#000000");

		await component.$set({
			value: "#FFFFFF"
		});

		assert.equal(component.value, "#FFFFFF");
	});
});
```

The third one is the index.ts file:

```typescript
export { default as Component } from "./ColorPicker.svelte";
export const modes = ["static", "dynamic"];
```

- `directory.ts` æ–‡ä»¶ä¸­æ·»åŠ ç»„ä»¶çš„æ˜ å°„ã€‚å¤åˆ¶å¹¶ç²˜è´´ä»»ä½•ç»„ä»¶çš„æ˜ å°„è¡Œï¼Œå¹¶ç¼–è¾‘å…¶æ–‡æœ¬ã€‚é”®åå¿…é¡»æ˜¯ Python åº“ä¸­å®žé™…ç»„ä»¶åç§°çš„å°å†™ç‰ˆæœ¬ã€‚ä¾‹å¦‚ï¼Œå¯¹äºŽ ColorPicker ç»„ä»¶ï¼Œæ˜ å°„å¦‚ä¸‹æ‰€ç¤ºï¼š

```typescript
export const component_map = {
...
colorpicker: () => import("./ColorPicker"),
...
}
```

### 2.1 ä¸º Svelte ç»„ä»¶ç¼–å†™å•å…ƒæµ‹è¯•

åœ¨å¼€å‘æ–°ç»„ä»¶æ—¶ï¼Œæ‚¨è¿˜åº”è¯¥ä¸ºå…¶ç¼–å†™ä¸€å¥—å•å…ƒæµ‹è¯•ã€‚æµ‹è¯•åº”è¯¥æ”¾ç½®åœ¨æ–°ç»„ä»¶çš„æ–‡ä»¶å¤¹ä¸­ï¼Œæ–‡ä»¶åä¸º MyAwesomeComponent.test.tsã€‚åŒæ ·ï¼Œåƒä¸Šé¢é‚£æ ·å‚è€ƒå…¶ä»–ç»„ä»¶çš„æµ‹è¯•ï¼ˆä¾‹å¦‚[Textbox.test.ts](https://github.com/gradio-app/gradio/blob/main/js/app/src/components/Textbox/Textbox.test.ts)ï¼‰ï¼Œå¹¶æ·»åŠ å°½å¯èƒ½å¤šçš„å•å…ƒæµ‹è¯•ï¼Œä»¥æµ‹è¯•æ–°ç»„ä»¶çš„ä¸åŒæ–¹é¢å’ŒåŠŸèƒ½ã€‚

### 3. åˆ›å»ºæ–°çš„æ¼”ç¤º

æœ€åŽä¸€æ­¥æ˜¯åœ¨[gradio/demo æ–‡ä»¶å¤¹](https://github.com/gradio-app/gradio/tree/main/demo)ä¸­åˆ›å»ºä¸€ä¸ªä½¿ç”¨æ–°æ·»åŠ çš„ç»„ä»¶çš„æ¼”ç¤ºã€‚åŒæ ·ï¼Œå»ºè®®å‚è€ƒçŽ°æœ‰æ¼”ç¤ºã€‚åœ¨ä¸€ä¸ªåä¸º run.py çš„æ–‡ä»¶ä¸­ç¼–å†™æ¼”ç¤ºçš„ä»£ç ï¼Œæ·»åŠ å¿…è¦çš„è¦æ±‚å’Œæ˜¾ç¤ºåº”ç”¨ç¨‹åºç•Œé¢çš„å›¾åƒã€‚æœ€åŽæ·»åŠ ä¸€ä¸ªæ˜¾ç¤ºå…¶ç”¨æ³•çš„ gifã€‚
æ‚¨å¯ä»¥æŸ¥çœ‹ä¸º ColorPicker åˆ›å»ºçš„[demo](https://github.com/gradio-app/gradio/tree/main/demo/color_picker)ï¼Œå…¶ä¸­ä»¥æ–°ç»„ä»¶é€‰æ‹©çš„å›¾æ ‡å’Œé¢œè‰²ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»¥é€‰æ‹©çš„é¢œè‰²ç€è‰²çš„ç›¸åŒå›¾æ ‡ä½œä¸ºè¾“å‡ºã€‚

è¦æµ‹è¯•åº”ç”¨ç¨‹åºï¼š

- åœ¨ç»ˆç«¯ä¸Šè¿è¡Œ `python path/demo/run.py`ï¼Œå®ƒä¼šåœ¨åœ°å€ [http://localhost:7860](http://localhost:7860) å¯åŠ¨åŽç«¯ï¼›
- åœ¨å¦ä¸€ä¸ªç»ˆç«¯ä¸Šï¼Œè¿è¡Œ `pnpm dev` ä»¥åœ¨ [http://localhost:9876](http://localhost:9876) ä¸Šå¯åŠ¨å…·æœ‰çƒ­é‡æ–°åŠ è½½åŠŸèƒ½çš„å‰ç«¯ã€‚

## ç»“è®º

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å°†æ–°ç»„ä»¶æ·»åŠ åˆ° Gradio æ˜¯å¤šä¹ˆç®€å•ï¼Œé€æ­¥ä»‹ç»äº†å¦‚ä½•æ·»åŠ  ColorPicker ç»„ä»¶ã€‚è¦äº†è§£æ›´å¤šç»†èŠ‚ï¼Œå¯ä»¥å‚è€ƒ PRï¼š[#1695](https://github.com/gradio-app/gradio/pull/1695).

---

<!-- Source: guides/cn/07_other-tutorials/developing-faster-with-reload-mode.md -->
# é€šè¿‡è‡ªåŠ¨é‡è½½å®žçŽ°æ›´å¿«çš„å¼€å‘

**å…ˆå†³æ¡ä»¶**ï¼šæœ¬æŒ‡å—è¦æ±‚æ‚¨äº†è§£å—çš„çŸ¥è¯†ã€‚è¯·ç¡®ä¿[å…ˆé˜…è¯»å—æŒ‡å—](https://gradio.app/blocks-and-event-listeners)ã€‚

æœ¬æŒ‡å—ä»‹ç»äº†è‡ªåŠ¨é‡æ–°åŠ è½½ã€åœ¨ Python IDE ä¸­é‡æ–°åŠ è½½ä»¥åŠåœ¨ Jupyter Notebooks ä¸­ä½¿ç”¨ gradio çš„æ–¹æ³•ã€‚

## ä¸ºä»€ä¹ˆè¦ä½¿ç”¨è‡ªåŠ¨é‡è½½ï¼Ÿ

å½“æ‚¨æž„å»º Gradio æ¼”ç¤ºæ—¶ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨ Blocks æž„å»ºæ—¶ï¼Œæ‚¨å¯èƒ½ä¼šå‘çŽ°åå¤è¿è¡Œä»£ç ä»¥æµ‹è¯•æ›´æ”¹å¾ˆéº»çƒ¦ã€‚

ä¸ºäº†æ›´å¿«é€Ÿã€æ›´ä¾¿æ·åœ°ç¼–å†™ä»£ç ï¼Œæˆ‘ä»¬å·²ç»ç®€åŒ–äº†åœ¨ **Python IDE**ï¼ˆå¦‚ VS Codeã€Sublime Textã€PyCharm ç­‰ï¼‰ä¸­å¼€å‘æˆ–ä»Žç»ˆç«¯è¿è¡Œ Python ä»£ç æ—¶â€œé‡æ–°åŠ è½½â€Gradio åº”ç”¨çš„æ–¹å¼ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªç±»ä¼¼çš„â€œé­”æ³•å‘½ä»¤â€ï¼Œä½¿æ‚¨å¯ä»¥æ›´å¿«é€Ÿåœ°é‡æ–°è¿è¡Œå•å…ƒæ ¼ï¼Œå¦‚æžœæ‚¨ä½¿ç”¨ Jupyter Notebooksï¼ˆæˆ–ç±»ä¼¼çš„çŽ¯å¢ƒï¼Œå¦‚ Colabï¼‰çš„è¯ã€‚

è¿™ä¸ªç®€çŸ­çš„æŒ‡å—å°†æ¶µç›–è¿™ä¸¤ç§æ–¹æ³•ï¼Œæ‰€ä»¥æ— è®ºæ‚¨å¦‚ä½•ç¼–å†™ Python ä»£ç ï¼Œæ‚¨éƒ½å°†çŸ¥é“å¦‚ä½•æ›´å¿«åœ°æž„å»º Gradio åº”ç”¨ç¨‹åºã€‚

## Python IDE é‡è½½ ðŸ”¥

å¦‚æžœæ‚¨ä½¿ç”¨ Python IDE æž„å»º Gradio Blocksï¼Œé‚£ä¹ˆä»£ç æ–‡ä»¶ï¼ˆå‡è®¾å‘½åä¸º `run.py`ï¼‰å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import gradio as gr

with gr.Blocks() as demo:
    gr.Markdown("# æ¥è‡ªGradioçš„é—®å€™ï¼")
    inp = gr.Textbox(placeholder="æ‚¨å«ä»€ä¹ˆåå­—ï¼Ÿ")
    out = gr.Textbox()

    inp.change(fn=lambda x: f"æ¬¢è¿Žï¼Œ{x}ï¼",
               inputs=inp,
               outputs=out)

if __name__ == "__main__":
    demo.launch()
```

é—®é¢˜åœ¨äºŽï¼Œæ¯å½“æ‚¨æƒ³è¦æ›´æ”¹å¸ƒå±€ã€äº‹ä»¶æˆ–ç»„ä»¶æ—¶ï¼Œéƒ½å¿…é¡»é€šè¿‡ç¼–å†™ `python run.py` æ¥å…³é—­å’Œé‡æ–°è¿è¡Œåº”ç”¨ç¨‹åºã€‚

è€Œä¸æ˜¯è¿™æ ·åšï¼Œæ‚¨å¯ä»¥é€šè¿‡æ›´æ”¹ 1 ä¸ªå•è¯æ¥ä»¥**é‡æ–°åŠ è½½æ¨¡å¼**è¿è¡Œä»£ç ï¼šå°† `python` æ›´æ”¹ä¸º `gradio`ï¼š

åœ¨ç»ˆç«¯ä¸­è¿è¡Œ `gradio run.py`ã€‚å°±æ˜¯è¿™æ ·ï¼

çŽ°åœ¨ï¼Œæ‚¨å°†çœ‹åˆ°ç±»ä¼¼äºŽè¿™æ ·çš„å†…å®¹ï¼š

```bash
Launching in *reload mode* on: http://127.0.0.1:7860 (Press CTRL+C to quit)

Watching...

WARNING:  The --reload flag should not be used in production on Windows.
```

è¿™é‡Œæœ€é‡è¦çš„ä¸€è¡Œæ˜¯ `æ­£åœ¨è§‚å¯Ÿ ...`ã€‚è¿™é‡Œå‘ç”Ÿçš„æƒ…å†µæ˜¯ Gradio å°†è§‚å¯Ÿ `run.py` æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ï¼Œå¦‚æžœæ–‡ä»¶å‘ç”Ÿæ›´æ”¹ï¼Œå®ƒå°†è‡ªåŠ¨ä¸ºæ‚¨é‡æ–°è¿è¡Œæ–‡ä»¶ã€‚å› æ­¤ï¼Œæ‚¨åªéœ€ä¸“æ³¨äºŽç¼–å†™ä»£ç ï¼ŒGradio æ¼”ç¤ºå°†è‡ªåŠ¨åˆ·æ–° ðŸ¥³

âš ï¸ è­¦å‘Šï¼š`gradio` å‘½ä»¤ä¸ä¼šæ£€æµ‹ä¼ é€’ç»™ `launch()` æ–¹æ³•çš„å‚æ•°ï¼Œå› ä¸ºåœ¨é‡æ–°åŠ è½½æ¨¡å¼ä¸‹ä»Žæœªè°ƒç”¨ `launch()` æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œè®¾ç½® `launch()` ä¸­çš„ `auth` æˆ– `show_error` ä¸ä¼šåœ¨åº”ç”¨ç¨‹åºä¸­åæ˜ å‡ºæ¥ã€‚

å½“æ‚¨ä½¿ç”¨é‡æ–°åŠ è½½æ¨¡å¼æ—¶ï¼Œè¯·è®°ä½ä¸€ä»¶é‡è¦çš„äº‹æƒ…ï¼šGradio ä¸“é—¨æŸ¥æ‰¾åä¸º `demo` çš„ Gradio Blocks/Interface æ¼”ç¤ºã€‚å¦‚æžœæ‚¨å°†æ¼”ç¤ºå‘½åä¸ºå…¶ä»–åç§°ï¼Œæ‚¨éœ€è¦åœ¨ä»£ç ä¸­çš„ç¬¬äºŒä¸ªå‚æ•°ä¸­ä¼ å…¥æ¼”ç¤ºçš„ FastAPI åº”ç”¨ç¨‹åºçš„åç§°ã€‚å¯¹äºŽ Gradio æ¼”ç¤ºï¼Œå¯ä»¥ä½¿ç”¨ `.app` å±žæ€§è®¿é—® FastAPI åº”ç”¨ç¨‹åºã€‚å› æ­¤ï¼Œå¦‚æžœæ‚¨çš„ `run.py` æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import gradio as gr

with gr.Blocks() as my_demo:
    gr.Markdown("# æ¥è‡ªGradioçš„é—®å€™ï¼")
    inp = gr.Textbox(placeholder="æ‚¨å«ä»€ä¹ˆåå­—ï¼Ÿ")
    out = gr.Textbox()

    inp.change(fn=lambda x: f"æ¬¢è¿Žï¼Œ{x}ï¼",
               inputs=inp,
               outputs=out)

if __name__ == "__main__":
    my_demo.launch()
```

é‚£ä¹ˆæ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨å®ƒï¼š`gradio run.py my_demo.app`ã€‚

Gradioé»˜è®¤ä½¿ç”¨UTF-8ç¼–ç æ ¼å¼ã€‚å¯¹äºŽ**é‡æ–°åŠ è½½æ¨¡å¼**ï¼Œå¦‚æžœä½ çš„è„šæœ¬ä½¿ç”¨çš„æ˜¯é™¤UTF-8ä»¥å¤–çš„ç¼–ç ï¼ˆå¦‚GBKï¼‰ï¼š

1. åœ¨Pythonè„šæœ¬çš„ç¼–ç å£°æ˜Žå¤„æŒ‡å®šä½ æƒ³è¦çš„ç¼–ç æ ¼å¼ï¼Œå¦‚ï¼š`# -*- coding: gbk -*-`
2. ç¡®ä¿ä½ çš„ä»£ç ç¼–è¾‘å™¨è¯†åˆ«åˆ°è¯¥æ ¼å¼ã€‚ 
3. æ‰§è¡Œï¼š`gradio run.py --encoding gbk`

ðŸ”¥ å¦‚æžœæ‚¨çš„åº”ç”¨ç¨‹åºæŽ¥å—å‘½ä»¤è¡Œå‚æ•°ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä¼ é€’å®ƒä»¬ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š

```python
import gradio as gr
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--name", type=str, default="User")
args, unknown = parser.parse_known_args()

with gr.Blocks() as demo:
    gr.Markdown(f"# æ¬¢è¿Ž {args.name}ï¼")
    inp = gr.Textbox()
    out = gr.Textbox()

    inp.change(fn=lambda x: x, inputs=inp, outputs=out)

if __name__ == "__main__":
    demo.launch()
```

æ‚¨å¯ä»¥åƒè¿™æ ·è¿è¡Œå®ƒï¼š`gradio run.py --name Gretel`

ä½œä¸ºä¸€ä¸ªå°æç¤ºï¼Œåªè¦æ›´æ”¹äº† `run.py` æºä»£ç æˆ– Gradio æºä»£ç ï¼Œè‡ªåŠ¨é‡æ–°åŠ è½½å°±ä¼šå‘ç”Ÿã€‚è¿™æ„å‘³ç€å¦‚æžœæ‚¨å†³å®š[ä¸º Gradio åšè´¡çŒ®](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md)ï¼Œè¿™å°†éžå¸¸æœ‰ç”¨ âœ…

## Jupyter Notebook é­”æ³•å‘½ä»¤ðŸ”®

å¦‚æžœæ‚¨ä½¿ç”¨ Jupyter Notebooksï¼ˆæˆ– Colab Notebooks ç­‰ï¼‰è¿›è¡Œå¼€å‘ï¼Œæˆ‘ä»¬ä¹Ÿä¸ºæ‚¨æä¾›äº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼

æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ª **magic command é­”æ³•å‘½ä»¤**ï¼Œå¯ä»¥ä¸ºæ‚¨åˆ›å»ºå’Œè¿è¡Œä¸€ä¸ª Blocks æ¼”ç¤ºã€‚è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œåœ¨ç¬”è®°æœ¬é¡¶éƒ¨åŠ è½½ gradio æ‰©å±•ï¼š

`%load_ext gradio`

ç„¶åŽï¼Œåœ¨æ‚¨æ­£åœ¨å¼€å‘ Gradio æ¼”ç¤ºçš„å•å…ƒæ ¼ä¸­ï¼Œåªéœ€åœ¨é¡¶éƒ¨å†™å…¥é­”æ³•å‘½ä»¤**`%%blocks`**ï¼Œç„¶åŽåƒå¹³å¸¸ä¸€æ ·ç¼–å†™å¸ƒå±€å’Œç»„ä»¶ï¼š

```py
%%blocks

import gradio as gr

gr.Markdown("# æ¥è‡ªGradioçš„é—®å€™ï¼")
inp = gr.Textbox(placeholder="æ‚¨å«ä»€ä¹ˆåå­—ï¼Ÿ")
out = gr.Textbox()

inp.change(fn=lambda x: f"æ¬¢è¿Žï¼Œ{x}ï¼",
           inputs=inp,
           outputs=out)
```

è¯·æ³¨æ„ï¼š

- æ‚¨ä¸éœ€è¦æ”¾ç½®æ ·æ¿ä»£ç  `with gr.Blocks() as demo:` å’Œ `demo.launch()` â€” Gradio ä¼šè‡ªåŠ¨ä¸ºæ‚¨å®Œæˆï¼

- æ¯æ¬¡é‡æ–°è¿è¡Œå•å…ƒæ ¼æ—¶ï¼ŒGradio éƒ½å°†åœ¨ç›¸åŒçš„ç«¯å£ä¸Šé‡æ–°å¯åŠ¨æ‚¨çš„åº”ç”¨ç¨‹åºï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„åº•å±‚ç½‘ç»œæœåŠ¡å™¨ã€‚è¿™æ„å‘³ç€æ‚¨å°†æ¯”æ­£å¸¸é‡æ–°è¿è¡Œå•å…ƒæ ¼æ›´å¿«åœ°çœ‹åˆ°å˜åŒ–ã€‚

ä¸‹é¢æ˜¯åœ¨ Jupyter Notebook ä¸­çš„ç¤ºä¾‹ï¼š

![](https://i.ibb.co/nrszFws/Blocks.gif)

ðŸª„è¿™åœ¨ colab ç¬”è®°æœ¬ä¸­ä¹Ÿé€‚ç”¨ï¼[è¿™æ˜¯ä¸€ä¸ª colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1jUlX1w7JqckRHVE-nbDyMPyZ7fYD8488?authuser=1#scrollTo=zxHYjbCTTz_5)ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­çœ‹åˆ° Blocks é­”æ³•æ•ˆæžœã€‚å°è¯•è¿›è¡Œä¸€äº›æ›´æ”¹å¹¶é‡æ–°è¿è¡Œå¸¦æœ‰ Gradio ä»£ç çš„å•å…ƒæ ¼ï¼

Notebook Magic çŽ°åœ¨æ˜¯ä½œè€…æž„å»º Gradio æ¼”ç¤ºçš„é¦–é€‰æ–¹å¼ã€‚æ— è®ºæ‚¨å¦‚ä½•ç¼–å†™ Python ä»£ç ï¼Œæˆ‘ä»¬éƒ½å¸Œæœ›è¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½ä¸ºæ‚¨æä¾›æ›´å¥½çš„ Gradio å¼€å‘ä½“éªŒã€‚

---

## ä¸‹ä¸€æ­¥

æ—¢ç„¶æ‚¨å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨ Gradio å¿«é€Ÿå¼€å‘ï¼Œè¯·å¼€å§‹æž„å»ºè‡ªå·±çš„åº”ç”¨ç¨‹åºå§ï¼

å¦‚æžœä½ æ­£åœ¨å¯»æ‰¾çµæ„Ÿï¼Œè¯·å°è¯•æµè§ˆå…¶ä»–äººç”¨ Gradio æž„å»ºçš„æ¼”ç¤ºï¼Œ[æµè§ˆ Hugging Face Spaces](http://hf.space/) ðŸ¤—

---

<!-- Source: guides/cn/07_other-tutorials/how-to-use-3D-model-component.md -->
# å¦‚ä½•ä½¿ç”¨ 3D æ¨¡åž‹ç»„ä»¶

ç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/dawood/Model3D, https://huggingface.co/spaces/radames/PIFu-Clothed-Human-Digitization, https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-obj
æ ‡ç­¾ï¼šVISION, IMAGE

## ä»‹ç»

æœºå™¨å­¦ä¹ ä¸­çš„ 3D æ¨¡åž‹è¶Šæ¥è¶Šå—æ¬¢è¿Žï¼Œå¹¶ä¸”æ˜¯ä¸€äº›æœ€æœ‰è¶£çš„æ¼”ç¤ºå®žéªŒã€‚ä½¿ç”¨ `gradio`ï¼Œæ‚¨å¯ä»¥è½»æ¾æž„å»ºæ‚¨çš„ 3D å›¾åƒæ¨¡åž‹çš„æ¼”ç¤ºï¼Œå¹¶ä¸Žä»»ä½•äººåˆ†äº«ã€‚Gradio 3D æ¨¡åž‹ç»„ä»¶æŽ¥å— 3 ç§æ–‡ä»¶ç±»åž‹ï¼ŒåŒ…æ‹¬ï¼š_.obj_ï¼Œ_.glb_ å’Œ _.gltf_ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨å‡ è¡Œä»£ç æž„å»ºæ‚¨çš„ 3D å›¾åƒæ¨¡åž‹çš„æ¼”ç¤ºï¼›åƒä¸‹é¢è¿™ä¸ªç¤ºä¾‹ä¸€æ ·ã€‚ç‚¹å‡»ã€æ‹–æ‹½å’Œç¼©æ”¾æ¥çŽ©è½¬ 3D å¯¹è±¡ï¼š

<gradio-app space="dawood/Model3D"> </gradio-app>

### å…ˆå†³æ¡ä»¶

ç¡®ä¿å·²ç»[å®‰è£…](https://gradio.app/quickstart)äº† `gradio` Python åŒ…ã€‚

## æŸ¥çœ‹ä»£ç 

è®©æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•åˆ›å»ºä¸Šé¢çš„æœ€ç®€ç•Œé¢ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé¢„æµ‹å‡½æ•°å°†åªè¿”å›žåŽŸå§‹çš„ 3D æ¨¡åž‹ç½‘æ ¼ï¼Œä½†æ‚¨å¯ä»¥æ›´æ”¹æ­¤å‡½æ•°ä»¥åœ¨æ‚¨çš„æœºå™¨å­¦ä¹ æ¨¡åž‹ä¸Šè¿è¡ŒæŽ¨ç†ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çœ‹æ›´å¤æ‚çš„ç¤ºä¾‹ã€‚

```python
import gradio as gr

def load_mesh(mesh_file_name):
    return mesh_file_name

demo = gr.Interface(
    fn=load_mesh,
    inputs=gr.Model3D(),
    outputs=gr.Model3D(clear_color=[0.0, 0.0, 0.0, 0.0],  label="3D Model"),
    examples=[
        ["files/Bunny.obj"],
        ["files/Duck.glb"],
        ["files/Fox.gltf"],
        ["files/face.obj"],
    ],
    cache_examples=True,
)

demo.launch()
```

è®©æˆ‘ä»¬æ¥è§£æžä¸Šé¢çš„ä»£ç ï¼š

`load_mesh`ï¼šè¿™æ˜¯æˆ‘ä»¬çš„â€œé¢„æµ‹â€å‡½æ•°ï¼Œä¸ºç®€å•èµ·è§ï¼Œè¯¥å‡½æ•°å°†æŽ¥æ”¶ 3D æ¨¡åž‹ç½‘æ ¼å¹¶è¿”å›žå®ƒã€‚

åˆ›å»ºç•Œé¢ï¼š

- `fn`ï¼šå½“ç”¨æˆ·ç‚¹å‡»æäº¤æ—¶ä½¿ç”¨çš„é¢„æµ‹å‡½æ•°ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå®ƒæ˜¯ `load_mesh` å‡½æ•°ã€‚
- `inputs`ï¼šåˆ›å»ºä¸€ä¸ª model3D è¾“å…¥ç»„ä»¶ã€‚è¾“å…¥æ˜¯ä¸€ä¸ªä¸Šä¼ çš„æ–‡ä»¶ï¼Œä½œä¸º{str}æ–‡ä»¶è·¯å¾„ã€‚
- `outputs`ï¼šåˆ›å»ºä¸€ä¸ª model3D è¾“å‡ºç»„ä»¶ã€‚è¾“å‡ºç»„ä»¶ä¹ŸæœŸæœ›ä¸€ä¸ªæ–‡ä»¶ä½œä¸º{str}æ–‡ä»¶è·¯å¾„ã€‚
  - `clear_color`ï¼šè¿™æ˜¯ 3D æ¨¡åž‹ç”»å¸ƒçš„èƒŒæ™¯é¢œè‰²ã€‚æœŸæœ› RGBa å€¼ã€‚
  - `label`ï¼šå‡ºçŽ°åœ¨ç»„ä»¶å·¦ä¸Šè§’çš„æ ‡ç­¾ã€‚
- `examples`ï¼š3D æ¨¡åž‹æ–‡ä»¶çš„åˆ—è¡¨ã€‚3D æ¨¡åž‹ç»„ä»¶å¯ä»¥æŽ¥å—*.obj*ï¼Œ*.glb*å’Œ*.gltf*æ–‡ä»¶ç±»åž‹ã€‚
- `cache_examples`ï¼šä¿å­˜ç¤ºä¾‹çš„é¢„æµ‹è¾“å‡ºï¼Œä»¥èŠ‚çœæŽ¨ç†æ—¶é—´ã€‚

## æŽ¢ç´¢æ›´å¤æ‚çš„ Model3D æ¼”ç¤º

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ DPT æ¨¡åž‹é¢„æµ‹å›¾åƒæ·±åº¦ï¼Œç„¶åŽä½¿ç”¨ 3D ç‚¹äº‘åˆ›å»º 3D å¯¹è±¡çš„æ¼”ç¤ºã€‚æŸ¥çœ‹[code.py](https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-obj/blob/main/app.py)æ–‡ä»¶ï¼Œäº†è§£ä»£ç å’Œæ¨¡åž‹é¢„æµ‹å‡½æ•°ã€‚
<gradio-app space="radames/dpt-depth-estimation-3d-obj"> </gradio-app>

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ PIFu æ¨¡åž‹å°†ç©¿ç€è¡£ç‰©çš„äººçš„å›¾åƒè½¬æ¢ä¸º 3D æ•°å­—åŒ–æ¨¡åž‹çš„æ¼”ç¤ºã€‚æŸ¥çœ‹[spaces.py](https://huggingface.co/spaces/radames/PIFu-Clothed-Human-Digitization/blob/main/PIFu/spaces.py)æ–‡ä»¶ï¼Œäº†è§£ä»£ç å’Œæ¨¡åž‹é¢„æµ‹å‡½æ•°ã€‚

<gradio-app space="radames/PIFu-Clothed-Human-Digitization"> </gradio-app>

---

æžå®šï¼è¿™å°±æ˜¯æž„å»º Model3D æ¨¡åž‹ç•Œé¢æ‰€éœ€çš„æ‰€æœ‰ä»£ç ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ‚¨å¯èƒ½ä¼šå‘çŽ°æœ‰ç”¨çš„å‚è€ƒèµ„æ–™ï¼š

- Gradio çš„[â€œå…¥é—¨æŒ‡å—â€](https://gradio.app/getting_started/)
- ç¬¬ä¸€ä¸ª[3D æ¨¡åž‹æ¼”ç¤º](https://huggingface.co/spaces/dawood/Model3D)å’Œ[å®Œæ•´ä»£ç ](https://huggingface.co/spaces/dawood/Model3D/tree/main)ï¼ˆåœ¨ Hugging Face Spaces ä¸Šï¼‰

---

<!-- Source: guides/cn/07_other-tutorials/named-entity-recognition.md -->
# å‘½åå®žä½“è¯†åˆ« ï¼ˆNamed-Entity Recognitionï¼‰

ç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/rajistics/biobert_ner_demoï¼Œhttps://huggingface.co/spaces/abidlabs/nerï¼Œhttps://huggingface.co/spaces/rajistics/Financial_Analyst_AI
æ ‡ç­¾ï¼šNERï¼ŒTEXTï¼ŒHIGHLIGHT

## ç®€ä»‹

å‘½åå®žä½“è¯†åˆ«ï¼ˆNERï¼‰åˆç§°ä¸ºæ ‡è®°åˆ†ç±»æˆ–æ–‡æœ¬æ ‡è®°ï¼Œå®ƒçš„ä»»åŠ¡æ˜¯å¯¹ä¸€ä¸ªå¥å­è¿›è¡Œåˆ†ç±»ï¼Œå°†æ¯ä¸ªå•è¯ï¼ˆæˆ– "token"ï¼‰å½’ä¸ºä¸åŒçš„ç±»åˆ«ï¼Œæ¯”å¦‚äººåã€åœ°åæˆ–è¯æ€§ç­‰ã€‚

ä¾‹å¦‚ï¼Œç»™å®šä»¥ä¸‹å¥å­ï¼š

> èŠåŠ å“¥æœ‰å·´åŸºæ–¯å¦é¤åŽ…å—ï¼Ÿ

å‘½åå®žä½“è¯†åˆ«ç®—æ³•å¯ä»¥è¯†åˆ«å‡ºï¼š

- "Chicago" as a **location**
- "Pakistani" as an **ethnicity**

ç­‰ç­‰ã€‚

ä½¿ç”¨ `gradio`ï¼ˆç‰¹åˆ«æ˜¯ `HighlightedText` ç»„ä»¶ï¼‰ï¼Œæ‚¨å¯ä»¥è½»æ¾æž„å»ºä¸€ä¸ª NER æ¨¡åž‹çš„ Web æ¼”ç¤ºå¹¶ä¸Žå›¢é˜Ÿåˆ†äº«ã€‚

è¿™æ˜¯æ‚¨å°†èƒ½å¤Ÿæž„å»ºçš„ä¸€ä¸ªæ¼”ç¤ºçš„ç¤ºä¾‹ï¼š

$demo_ner_pipeline

æœ¬æ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨é¢„è®­ç»ƒçš„ NER æ¨¡åž‹å¹¶ä½¿ç”¨ Gradio ç•Œé¢éƒ¨ç½²è¯¥æ¨¡åž‹ã€‚æˆ‘ä»¬å°†å±•ç¤ºä¸¤ç§ä¸åŒçš„ä½¿ç”¨ `HighlightedText` ç»„ä»¶çš„æ–¹æ³•--æ ¹æ®æ‚¨çš„ NER æ¨¡åž‹ï¼Œå¯ä»¥é€‰æ‹©å…¶ä¸­ä»»ä½•ä¸€ç§æ›´å®¹æ˜“å­¦ä¹ çš„æ–¹å¼ï¼

### çŽ¯å¢ƒè¦æ±‚

ç¡®ä¿æ‚¨å·²ç»[å®‰è£…](/getting_started)äº† `gradio` Python åŒ…ã€‚æ‚¨è¿˜éœ€è¦ä¸€ä¸ªé¢„è®­ç»ƒçš„å‘½åå®žä½“è¯†åˆ«æ¨¡åž‹ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `transformers` åº“ä¸­çš„ä¸€ä¸ªæ¨¡åž‹ã€‚

### æ–¹æ³•ä¸€ï¼šå®žä½“å­—å…¸åˆ—è¡¨

è®¸å¤šå‘½åå®žä½“è¯†åˆ«æ¨¡åž‹è¾“å‡ºçš„æ˜¯ä¸€ä¸ªå­—å…¸åˆ—è¡¨ã€‚æ¯ä¸ªå­—å…¸åŒ…å«ä¸€ä¸ª*å®žä½“*ï¼Œä¸€ä¸ª " èµ·å§‹ " ç´¢å¼•å’Œä¸€ä¸ª " ç»“æŸ " ç´¢å¼•ã€‚è¿™å°±æ˜¯ `transformers` åº“ä¸­çš„ NER æ¨¡åž‹çš„æ“ä½œæ–¹å¼ã€‚

```py
from transformers import pipeline
ner_pipeline = pipeline("ner")
ner_pipeline("èŠåŠ å“¥æœ‰å·´åŸºæ–¯å¦é¤åŽ…å—ï¼Ÿ")
```

è¾“å‡ºç»“æžœï¼š

```bash
[{'entity': 'I-LOC',
  'score': 0.9988978,
  'index': 2,
  'word': 'Chicago',
  'start': 5,
  'end': 12},
 {'entity': 'I-MISC',
  'score': 0.9958592,
  'index': 5,
  'word': 'Pakistani',
  'start': 22,
  'end': 31}]
```

å¦‚æžœæ‚¨æœ‰è¿™æ ·çš„æ¨¡åž‹ï¼Œå°†å…¶è¿žæŽ¥åˆ° Gradio çš„ `HighlightedText` ç»„ä»¶éžå¸¸ç®€å•ã€‚æ‚¨åªéœ€è¦å°†è¿™ä¸ª**å®žä½“åˆ—è¡¨**ä¸Ž**åŽŸå§‹æ–‡æœ¬**ä»¥å­—å…¸çš„å½¢å¼ä¼ é€’ç»™æ¨¡åž‹ï¼Œå…¶ä¸­é”®åˆ†åˆ«ä¸º `"entities"` å’Œ `"text"`ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼š

$code_ner_pipeline
$demo_ner_pipeline

### æ–¹æ³•äºŒï¼šå…ƒç»„åˆ—è¡¨

å°†æ•°æ®ä¼ é€’ç»™ `HighlightedText` ç»„ä»¶çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨å…ƒç»„åˆ—è¡¨ã€‚æ¯ä¸ªå…ƒç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ åº”è¯¥æ˜¯è¢«å½’ç±»ä¸ºç‰¹å®šå®žä½“çš„å•è¯æˆ–è¯ç»„ã€‚ç¬¬äºŒä¸ªå…ƒç´ åº”è¯¥æ˜¯å®žä½“æ ‡ç­¾ï¼ˆå¦‚æžœä¸éœ€è¦æ ‡ç­¾ï¼Œåˆ™ä¸º `None`ï¼‰ã€‚`HighlightedText` ç»„ä»¶ä¼šè‡ªåŠ¨ç»„åˆå•è¯å’Œæ ‡ç­¾æ¥æ˜¾ç¤ºå®žä½“ã€‚

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™æ¯”ç¬¬ä¸€ç§æ–¹æ³•æ›´ç®€å•ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ Spacy çš„è¯æ€§æ ‡æ³¨å™¨æ¼”ç¤ºæ­¤æ–¹æ³•çš„ç¤ºä¾‹ï¼š

$code_text_analysis
$demo_text_analysis

---

åˆ°æ­¤ä¸ºæ­¢ï¼æ‚¨å·²ç»äº†è§£äº†ä¸ºæ‚¨çš„ NER æ¨¡åž‹æž„å»ºåŸºäºŽ Web çš„å›¾å½¢ç”¨æˆ·ç•Œé¢æ‰€éœ€çš„å…¨éƒ¨å†…å®¹ã€‚

æœ‰è¶£çš„æç¤ºï¼šåªéœ€åœ¨ `launch()` ä¸­è®¾ç½® `share=True`ï¼Œå³å¯ç«‹å³ä¸Žå…¶ä»–äººåˆ†äº«æ‚¨çš„ NER æ¼”ç¤ºã€‚

---

<!-- Source: guides/cn/07_other-tutorials/real-time-speech-recognition.md -->
# å®žæ—¶è¯­éŸ³è¯†åˆ«

Related spaces: https://huggingface.co/spaces/abidlabs/streaming-asr-paused, https://huggingface.co/spaces/abidlabs/full-context-asr
Tags: ASR, SPEECH, STREAMING

## ä»‹ç»

è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ˜¯æœºå™¨å­¦ä¹ ä¸­éžå¸¸é‡è¦ä¸”è“¬å‹ƒå‘å±•çš„é¢†åŸŸï¼Œå®ƒå°†å£è¯­è½¬æ¢ä¸ºæ–‡æœ¬ã€‚ASR ç®—æ³•å‡ ä¹Žåœ¨æ¯éƒ¨æ™ºèƒ½æ‰‹æœºä¸Šéƒ½æœ‰è¿è¡Œï¼Œå¹¶è¶Šæ¥è¶Šå¤šåœ°åµŒå…¥åˆ°ä¸“ä¸šå·¥ä½œæµç¨‹ä¸­ï¼Œä¾‹å¦‚æŠ¤å£«å’ŒåŒ»ç”Ÿçš„æ•°å­—åŠ©æ‰‹ã€‚ç”±äºŽ ASR ç®—æ³•æ˜¯ç›´æŽ¥é¢å‘å®¢æˆ·å’Œæœ€ç»ˆç”¨æˆ·è®¾è®¡çš„ï¼Œå› æ­¤åœ¨é¢å¯¹å„ç§è¯­éŸ³æ¨¡å¼ï¼ˆä¸åŒçš„å£éŸ³ã€éŸ³è°ƒå’ŒèƒŒæ™¯éŸ³é¢‘æ¡ä»¶ï¼‰æ—¶ï¼ŒéªŒè¯å®ƒä»¬çš„è¡Œä¸ºæ˜¯å¦ç¬¦åˆé¢„æœŸéžå¸¸é‡è¦ã€‚

ä½¿ç”¨ `gradio`ï¼Œæ‚¨å¯ä»¥è½»æ¾æž„å»ºä¸€ä¸ª ASR æ¨¡åž‹çš„æ¼”ç¤ºï¼Œå¹¶ä¸Žæµ‹è¯•å›¢é˜Ÿå…±äº«ï¼Œæˆ–é€šè¿‡è®¾å¤‡ä¸Šçš„éº¦å…‹é£Žè¿›è¡Œè‡ªè¡Œæµ‹è¯•ã€‚

æœ¬æ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­éŸ³è¯†åˆ«æ¨¡åž‹å¹¶åœ¨ Gradio ç•Œé¢ä¸Šéƒ¨ç½²ã€‚æˆ‘ä»¬å°†ä»Žä¸€ä¸ª **full-context å…¨æ–‡**æ¨¡åž‹å¼€å§‹ï¼Œå…¶ä¸­ç”¨æˆ·åœ¨è¿›è¡Œé¢„æµ‹ä¹‹å‰è¦è¯´å®Œæ•´æ®µéŸ³é¢‘ã€‚ç„¶åŽï¼Œæˆ‘ä»¬å°†è°ƒæ•´æ¼”ç¤ºä»¥ä½¿å…¶å˜ä¸º **streaming æµå¼**ï¼Œè¿™æ„å‘³ç€éŸ³é¢‘æ¨¡åž‹å°†åœ¨æ‚¨è¯´è¯æ—¶å°†è¯­éŸ³è½¬æ¢ä¸ºæ–‡æœ¬ã€‚æˆ‘ä»¬åˆ›å»ºçš„æµå¼æ¼”ç¤ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼ˆåœ¨ä¸‹æ–¹å°è¯•æˆ–[åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€](https://huggingface.co/spaces/abidlabs/streaming-asr-paused)ï¼‰ï¼š

<iframe src="https://abidlabs-streaming-asr-paused.hf.space" frameBorder="0" height="350" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
å®žæ—¶ ASR æœ¬è´¨ä¸Šæ˜¯*æœ‰çŠ¶æ€çš„*ï¼Œå³æ¨¡åž‹çš„é¢„æµ‹ç»“æžœå–å†³äºŽç”¨æˆ·å…ˆå‰è¯´çš„å•è¯ã€‚å› æ­¤ï¼Œåœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¿˜å°†ä»‹ç»å¦‚ä½•åœ¨ Gradio æ¼”ç¤ºä¸­ä½¿ç”¨ **state**ã€‚

### å…ˆå†³æ¡ä»¶

ç¡®ä¿æ‚¨å·²ç»[å®‰è£…](/getting_started)äº† `gradio` Python åŒ…ã€‚æ‚¨è¿˜éœ€è¦ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­éŸ³è¯†åˆ«æ¨¡åž‹ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»Žä¸¤ä¸ª ASR åº“æž„å»ºæ¼”ç¤ºï¼š

- Transformersï¼ˆä¸ºæ­¤ï¼Œ`pip install transformers` å’Œ `pip install torch`ï¼‰\* DeepSpeechï¼ˆ`pip install deepspeech==0.8.2`ï¼‰

ç¡®ä¿æ‚¨è‡³å°‘å®‰è£…äº†å…¶ä¸­ä¹‹ä¸€ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥è·Ÿéšæœ¬æ•™ç¨‹æ“ä½œã€‚å¦‚æžœæ‚¨å°šæœªå®‰è£… `ffmpeg`ï¼Œè¯·åœ¨[ç³»ç»Ÿä¸Šä¸‹è½½å¹¶å®‰è£…](https://www.ffmpeg.org/download.html)ï¼Œä»¥ä¾¿ä»Žéº¦å…‹é£Žå¤„ç†æ–‡ä»¶ã€‚

ä¸‹é¢æ˜¯æž„å»ºå®žæ—¶è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åº”ç”¨ç¨‹åºçš„æ­¥éª¤ï¼š

1. [è®¾ç½® Transformers ASR æ¨¡åž‹](#1-set-up-the-transformers-asr-model)
2. [ä½¿ç”¨ Transformers åˆ›å»ºä¸€ä¸ªå…¨æ–‡ ASR æ¼”ç¤º]
   (#2-create-a-full-context-asr-demo-with-transformers)
3. [ä½¿ç”¨ Transformers åˆ›å»ºä¸€ä¸ªæµå¼ ASR æ¼”ç¤º](#3-create-a-streaming-asr-demo-with-transformers)
4. [ä½¿ç”¨ DeepSpeech åˆ›å»ºä¸€ä¸ªæµå¼ ASR æ¼”ç¤º](#4-create-a-streaming-asr-demo-with-deepspeech)

## 1. è®¾ç½® Transformers ASR æ¨¡åž‹

é¦–å…ˆï¼Œæ‚¨éœ€è¦æ‹¥æœ‰ä¸€ä¸ª ASR æ¨¡åž‹ï¼Œæ‚¨å¯ä»¥è‡ªå·±è®­ç»ƒï¼Œæˆ–è€…éœ€è¦ä¸‹è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡åž‹ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face æ¨¡åž‹çš„é¢„è®­ç»ƒ ASR æ¨¡åž‹ `Wav2Vec2`ã€‚

ä»¥ä¸‹æ˜¯ä»Ž Hugging Face çš„ `transformers` åŠ è½½ `Wav2Vec2` çš„ä»£ç ï¼š

```python
from transformers import pipeline
p = pipeline("automatic-speech-recognition")
```

å°±æ˜¯è¿™æ ·ï¼é»˜è®¤æƒ…å†µä¸‹ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡åž‹ç®¡é“ä¼šåŠ è½½ Facebook çš„ `facebook/wav2vec2-base-960h` æ¨¡åž‹ã€‚

## 2. ä½¿ç”¨ Transformers åˆ›å»ºä¸€ä¸ªå…¨æ–‡ ASR æ¼”ç¤º

æˆ‘ä»¬å°†é¦–å…ˆåˆ›å»ºä¸€ä¸ª*å…¨æ–‡*ASR æ¼”ç¤ºï¼Œå…¶ä¸­ç”¨æˆ·åœ¨ä½¿ç”¨ ASR æ¨¡åž‹è¿›è¡Œé¢„æµ‹ä¹‹å‰è¯´å®Œæ•´æ®µéŸ³é¢‘ã€‚ä½¿ç”¨ Gradio éžå¸¸ç®€å•ï¼Œæˆ‘ä»¬åªéœ€åœ¨ä¸Šé¢çš„ `pipeline` å¯¹è±¡å‘¨å›´åˆ›å»ºä¸€ä¸ªå‡½æ•°ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ `gradio` å†…ç½®çš„ `Audio` ç»„ä»¶ï¼Œé…ç½®ä»Žç”¨æˆ·çš„éº¦å…‹é£ŽæŽ¥æ”¶è¾“å…¥å¹¶è¿”å›žå½•åˆ¶éŸ³é¢‘çš„æ–‡ä»¶è·¯å¾„ã€‚è¾“å‡ºç»„ä»¶å°†æ˜¯ä¸€ä¸ªç®€å•çš„ `Textbox`ã€‚

```python
import gradio as gr

def transcribe(audio):
    text = p(audio)["text"]
    return text

gr.Interface(
    fn=transcribe,
    inputs=gr.Audio(sources=["microphone"], type="filepath"),
    outputs="text").launch()
```

é‚£ä¹ˆè¿™é‡Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ`transcribe` å‡½æ•°æŽ¥å—ä¸€ä¸ªå‚æ•° `audio`ï¼Œå®ƒæ˜¯ç”¨æˆ·å½•åˆ¶çš„éŸ³é¢‘æ–‡ä»¶çš„æ–‡ä»¶è·¯å¾„ã€‚`pipeline` å¯¹è±¡æœŸæœ›ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œç„¶åŽè¿”å›žåˆ°å‰ç«¯å¹¶åœ¨æ–‡æœ¬æ¡†ä¸­æ˜¾ç¤ºã€‚

è®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„æ•ˆæžœå§ï¼ï¼ˆå½•åˆ¶ä¸€æ®µçŸ­éŸ³é¢‘å¹¶ç‚¹å‡»æäº¤ï¼Œæˆ–[åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€](https://huggingface.co/spaces/abidlabs/full-context-asr)ï¼‰ï¼š

<iframe src="https://abidlabs-full-context-asr.hf.space" frameBorder="0" height="350" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>
## 3. ä½¿ç”¨ Transformers åˆ›å»ºä¸€ä¸ªæµå¼ ASR æ¼”ç¤º
å¤ªæ£’äº†ï¼æˆ‘ä»¬å·²ç»æž„å»ºäº†ä¸€ä¸ªå¯¹çŸ­éŸ³é¢‘å‰ªè¾‘æ•ˆæžœè‰¯å¥½çš„ ASR æ¨¡åž‹ã€‚ä½†æ˜¯ï¼Œå¦‚æžœæ‚¨æ­£åœ¨è®°å½•è¾ƒé•¿çš„éŸ³é¢‘å‰ªè¾‘ï¼Œåˆ™å¯èƒ½éœ€è¦ä¸€ä¸ª*æµå¼*ç•Œé¢ï¼Œå³åœ¨ç”¨æˆ·è¯´è¯æ—¶é€å¥è½¬å½•éŸ³é¢‘ï¼Œè€Œä¸ä»…ä»…åœ¨æœ€åŽä¸€æ¬¡å…¨éƒ¨è½¬å½•ã€‚

å¥½æ¶ˆæ¯æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°è°ƒæ•´åˆšåˆšåˆ›å»ºçš„æ¼”ç¤ºï¼Œä½¿å…¶æˆä¸ºæµå¼çš„ï¼Œä½¿ç”¨ç›¸åŒçš„ `Wav2Vec2` æ¨¡åž‹ã€‚

æœ€å¤§çš„å˜åŒ–æ˜¯æˆ‘ä»¬çŽ°åœ¨å¿…é¡»å¼•å…¥ä¸€ä¸ª `state` å‚æ•°ï¼Œå®ƒä¿å­˜åˆ°ç›®å‰ä¸ºæ­¢*è½¬å½•çš„éŸ³é¢‘*ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬åªéœ€å¤„ç†æœ€æ–°çš„éŸ³é¢‘å—ï¼Œå¹¶å°†å…¶ç®€å•åœ°è¿½åŠ åˆ°å…ˆå‰è½¬å½•çš„éŸ³é¢‘ä¸­ã€‚

åœ¨å‘ Gradio æ¼”ç¤ºæ·»åŠ çŠ¶æ€æ—¶ï¼Œæ‚¨éœ€è¦å®Œæˆ 3 ä»¶äº‹ï¼š

- åœ¨å‡½æ•°ä¸­æ·»åŠ  `state` å‚æ•°* åœ¨å‡½æ•°æœ«å°¾è¿”å›žæ›´æ–°åŽçš„ `state`* åœ¨ `Interface` çš„ `inputs` å’Œ `outputs` ä¸­æ·»åŠ  `"state"` ç»„ä»¶

ä»¥ä¸‹æ˜¯ä»£ç ç¤ºä¾‹ï¼š

```python
def transcribe(audio, state=""):
    text = p(audio)["text"]
    state += text + " "
    return state, state

# Set the starting state to an empty string
gr.Interface(
    fn=transcribe,
    inputs=[
        gr.Audio(sources=["microphone"], type="filepath", streaming=True),
        "state"
    ],
    outputs=[
        "textbox",
        "state"
    ],
    live=True).launch()
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å¦ä¸€ä¸ªæ›´æ”¹ï¼Œå³æˆ‘ä»¬è®¾ç½®äº† `live=True`ã€‚è¿™ä½¿å¾— Gradio æŽ¥å£ä¿æŒæŒç»­è¿è¡Œï¼Œå› æ­¤å®ƒå¯ä»¥è‡ªåŠ¨è½¬å½•éŸ³é¢‘ï¼Œè€Œæ— éœ€ç”¨æˆ·åå¤ç‚¹å‡»æäº¤æŒ‰é’®ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„æ•ˆæžœï¼ˆåœ¨ä¸‹æ–¹å°è¯•æˆ–[åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€](https://huggingface.co/spaces/abidlabs/streaming-asr)ï¼‰ï¼

<iframe src="https://abidlabs-streaming-asr.hf.space" frameBorder="0" height="350" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

ä½ å¯èƒ½æ³¨æ„åˆ°çš„ä¸€ä»¶äº‹æ˜¯ï¼Œç”±äºŽéŸ³é¢‘å—éžå¸¸å°ï¼Œæ‰€ä»¥è½¬å½•è´¨é‡ä¸‹é™äº†ï¼Œå®ƒä»¬ç¼ºä¹æ­£ç¡®è½¬å½•æ‰€éœ€çš„ä¸Šä¸‹æ–‡ã€‚æ­¤é—®é¢˜çš„â€œhackyâ€è§£å†³æ–¹æ³•æ˜¯ç®€å•åœ°å¢žåŠ  `transcribe()` å‡½æ•°çš„è¿è¡Œæ—¶é—´ï¼Œä»¥ä¾¿å¤„ç†æ›´é•¿çš„éŸ³é¢‘å—ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨å‡½æ•°ä¸­æ·»åŠ  `time.sleep()` æ¥å®žçŽ°è¿™ä¸€ç‚¹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼ˆæŽ¥ä¸‹æ¥æˆ‘ä»¬å°†çœ‹åˆ°ä¸€ä¸ªæ­£ç¡®çš„è§£å†³æ–¹æ³•ï¼‰

```python
from transformers import pipeline
import gradio as gr
import time

p = pipeline("automatic-speech-recognition")

def transcribe(audio, state=""):
    time.sleep(2)
    text = p(audio)["text"]
    state += text + " "
    return state, state

gr.Interface(
    fn=transcribe,
    inputs=[
        gr.Audio(sources=["microphone"], type="filepath", streaming=True),
        "state"
    ],
    outputs=[
        "textbox",
        "state"
    ],
    live=True).launch()
```

å°è¯•ä¸‹é¢çš„æ¼”ç¤ºï¼ŒæŸ¥çœ‹å·®å¼‚ï¼ˆæˆ–[åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€](https://huggingface.co/spaces/abidlabs/streaming-asr-paused)ï¼‰ï¼

<iframe src="https://abidlabs-streaming-asr-paused.hf.space" frameBorder="0" height="350" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

## 4. ä½¿ç”¨ DeepSpeech åˆ›å»ºæµå¼ ASR æ¼”ç¤º

æ‚¨ä¸ä»…é™äºŽä½¿ç”¨ `transformers` åº“ä¸­çš„ ASR æ¨¡åž‹ - æ‚¨å¯ä»¥ä½¿ç”¨è‡ªå·±çš„æ¨¡åž‹æˆ–å…¶ä»–åº“ä¸­çš„æ¨¡åž‹ã€‚`DeepSpeech` åº“åŒ…å«ä¸“é—¨ç”¨äºŽå¤„ç†æµå¼éŸ³é¢‘æ•°æ®çš„æ¨¡åž‹ã€‚è¿™äº›æ¨¡åž‹åœ¨å¤„ç†æµå¼æ•°æ®æ—¶è¡¨çŽ°éžå¸¸å¥½ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿè€ƒè™‘åˆ°å…ˆå‰çš„éŸ³é¢‘å—åœ¨è¿›è¡Œé¢„æµ‹æ—¶äº§ç”Ÿçš„å½±å“ã€‚

æ·±å…¥ç ”ç©¶ DeepSpeech åº“è¶…å‡ºäº†æœ¬æŒ‡å—çš„èŒƒå›´ï¼ˆå¯ä»¥åœ¨[æ­¤å¤„æŸ¥çœ‹å…¶ä¼˜ç§€çš„æ–‡æ¡£](https://deepspeech.readthedocs.io/en/r0.9/)ï¼‰ï¼Œä½†æ˜¯æ‚¨å¯ä»¥åƒä½¿ç”¨ Transformer ASR æ¨¡åž‹ä¸€æ ·ï¼Œä½¿ç”¨ DeepSpeech ASR æ¨¡åž‹ä½¿ç”¨ç±»ä¼¼çš„æ–¹æ³•ä½¿ç”¨ Gradioã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼ˆåœ¨ Linux ä¸Šï¼‰ï¼š

é¦–å…ˆé€šè¿‡ç»ˆç«¯å®‰è£… DeepSpeech åº“å¹¶ä¸‹è½½é¢„è®­ç»ƒæ¨¡åž‹ï¼š

```bash
wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.pbmm
wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.scorer
apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg
pip install deepspeech==0.8.2
```

ç„¶åŽï¼Œåˆ›å»ºä¸Žä¹‹å‰ç›¸ä¼¼çš„ `transcribe()` å‡½æ•°ï¼š

```python
from deepspeech import Model
import numpy as np

model_file_path = "deepspeech-0.8.2-models.pbmm"
lm_file_path = "deepspeech-0.8.2-models.scorer"
beam_width = 100
lm_alpha = 0.93
lm_beta = 1.18

model = Model(model_file_path)
model.enableExternalScorer(lm_file_path)
model.setScorerAlphaBeta(lm_alpha, lm_beta)
model.setBeamWidth(beam_width)


def reformat_freq(sr, y):
    if sr not in (
        48000,
        16000,
    ):  # Deepspeech only supports 16k, (we convert 48k -> 16k)
        raise ValueError("Unsupported rate", sr)
    if sr == 48000:
        y = (
            ((y / max(np.max(y), 1)) * 32767)
            .reshape((-1, 3))
            .mean(axis=1)
            .astype("int16")
        )
        sr = 16000
    return sr, y


def transcribe(speech, stream):
    _, y = reformat_freq(*speech)
    if stream is None:
        stream = model.createStream()
    stream.feedAudioContent(y)
    text = stream.intermediateDecode()
    return text, stream

```

ç„¶åŽï¼Œå¦‚å‰æ‰€è¿°åˆ›å»ºä¸€ä¸ª Gradio æŽ¥å£ï¼ˆå”¯ä¸€çš„åŒºåˆ«æ˜¯è¿”å›žç±»åž‹åº”è¯¥æ˜¯ `numpy` è€Œä¸æ˜¯ `filepath` ä»¥ä¸Ž DeepSpeech æ¨¡åž‹å…¼å®¹ï¼‰

```python
import gradio as gr

gr.Interface(
    fn=transcribe,
    inputs=[
        gr.Audio(sources=["microphone"], type="numpy"),
        "state"
    ],
    outputs= [
        "text",
        "state"
    ],
    live=True).launch()
```

è¿è¡Œæ‰€æœ‰è¿™äº›åº”è¯¥å…è®¸æ‚¨ä½¿ç”¨ä¸€ä¸ªæ¼‚äº®çš„ GUI éƒ¨ç½²å®žæ—¶ ASR æ¨¡åž‹ã€‚å°è¯•ä¸€ä¸‹ï¼Œçœ‹å®ƒåœ¨æ‚¨é‚£é‡Œè¿è¡Œå¾—æœ‰å¤šå¥½ã€‚

---

ä½ å·²ç»å®Œæˆäº†ï¼è¿™å°±æ˜¯æž„å»ºç”¨äºŽ ASR æ¨¡åž‹çš„åŸºäºŽ Web çš„ GUI æ‰€éœ€çš„æ‰€æœ‰ä»£ç ã€‚

æœ‰è¶£çš„æç¤ºï¼šæ‚¨åªéœ€åœ¨ `launch()` ä¸­è®¾ç½® `share=True`ï¼Œå³å¯å³æ—¶ä¸Žä»–äººå…±äº« ASR æ¨¡åž‹ã€‚

---

<!-- Source: guides/cn/07_other-tutorials/running-background-tasks.md -->
# è¿è¡ŒåŽå°ä»»åŠ¡

Related spaces: https://huggingface.co/spaces/freddyaboulton/gradio-google-forms
Tags: TASKS, SCHEDULED, TABULAR, DATA

## ç®€ä»‹

æœ¬æŒ‡å—ä»‹ç»äº†å¦‚ä½•ä»Ž gradio åº”ç”¨ç¨‹åºä¸­è¿è¡ŒåŽå°ä»»åŠ¡ã€‚
åŽå°ä»»åŠ¡æ˜¯åœ¨æ‚¨çš„åº”ç”¨ç¨‹åºçš„è¯·æ±‚-å“åº”ç”Ÿå‘½å‘¨æœŸä¹‹å¤–æ‰§è¡Œçš„æ“ä½œï¼Œå¯ä»¥æ˜¯ä¸€æ¬¡æ€§çš„æˆ–å®šæœŸçš„ã€‚
åŽå°ä»»åŠ¡çš„ç¤ºä¾‹åŒ…æ‹¬å®šæœŸå°†æ•°æ®ä¸Žå¤–éƒ¨æ•°æ®åº“åŒæ­¥æˆ–é€šè¿‡ç”µå­é‚®ä»¶å‘é€æ¨¡åž‹é¢„æµ‹æŠ¥å‘Šã€‚

## æ¦‚è¿°

æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªç®€å•çš„â€œGoogle Formsâ€é£Žæ ¼çš„åº”ç”¨ç¨‹åºï¼Œç”¨äºŽæ”¶é›† gradio åº“çš„ç”¨æˆ·åé¦ˆã€‚
æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªæœ¬åœ° sqlite æ•°æ®åº“æ¥å­˜å‚¨æ•°æ®ï¼Œä½†æˆ‘ä»¬å°†å®šæœŸå°†æ•°æ®åº“çš„çŠ¶æ€ä¸Ž[HuggingFace Dataset](https://huggingface.co/datasets)åŒæ­¥ï¼Œä»¥ä¾¿å§‹ç»ˆå¤‡ä»½æˆ‘ä»¬çš„ç”¨æˆ·è¯„è®ºã€‚
åŒæ­¥å°†åœ¨æ¯ 60 ç§’è¿è¡Œçš„åŽå°ä»»åŠ¡ä¸­è¿›è¡Œã€‚

åœ¨æ¼”ç¤ºç»“æŸæ—¶ï¼Œæ‚¨å°†æ‹¥æœ‰ä¸€ä¸ªå®Œå…¨å¯å·¥ä½œçš„åº”ç”¨ç¨‹åºï¼Œç±»ä¼¼äºŽä»¥ä¸‹åº”ç”¨ç¨‹åº :

<gradio-app space="freddyaboulton/gradio-google-forms"> </gradio-app>

## ç¬¬ä¸€æ­¥ - ç¼–å†™æ•°æ®åº“é€»è¾‘ ðŸ’¾

æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºå°†å­˜å‚¨è¯„è®ºè€…çš„å§“åï¼Œä»–ä»¬å¯¹ gradio ç»™å‡ºçš„è¯„åˆ†ï¼ˆ1 åˆ° 5 çš„èŒƒå›´ï¼‰ï¼Œä»¥åŠä»–ä»¬æƒ³è¦åˆ†äº«çš„å…³äºŽè¯¥åº“çš„ä»»ä½•è¯„è®ºã€‚è®©æˆ‘ä»¬ç¼–å†™ä¸€äº›ä»£ç ï¼Œåˆ›å»ºä¸€ä¸ªæ•°æ®åº“è¡¨æ¥å­˜å‚¨è¿™äº›æ•°æ®ã€‚æˆ‘ä»¬è¿˜å°†ç¼–å†™ä¸€äº›å‡½æ•°ï¼Œä»¥å°†è¯„è®ºæ’å…¥è¯¥è¡¨ä¸­å¹¶èŽ·å–æœ€æ–°çš„ 10 æ¡è¯„è®ºã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ `sqlite3` åº“æ¥è¿žæŽ¥æˆ‘ä»¬çš„ sqlite æ•°æ®åº“ï¼Œä½† gradio å¯ä»¥ä¸Žä»»ä½•åº“ä¸€èµ·ä½¿ç”¨ã€‚

ä»£ç å¦‚ä¸‹ :

```python
DB_FILE = "./reviews.db"
db = sqlite3.connect(DB_FILE)

# Create table if it doesn't already exist
try:
    db.execute("SELECT * FROM reviews").fetchall()
    db.close()
except sqlite3.OperationalError:
    db.execute(
        '''
        CREATE TABLE reviews (id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
                              created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP NOT NULL,
                              name TEXT, review INTEGER, comments TEXT)
        ''')
    db.commit()
    db.close()

def get_latest_reviews(db: sqlite3.Connection):
    reviews = db.execute("SELECT * FROM reviews ORDER BY id DESC limit 10").fetchall()
    total_reviews = db.execute("Select COUNT(id) from reviews").fetchone()[0]
    reviews = pd.DataFrame(reviews, columns=["id", "date_created", "name", "review", "comments"])
    return reviews, total_reviews


def add_review(name: str, review: int, comments: str):
    db = sqlite3.connect(DB_FILE)
    cursor = db.cursor()
    cursor.execute("INSERT INTO reviews(name, review, comments) VALUES(?,?,?)", [name, review, comments])
    db.commit()
    reviews, total_reviews = get_latest_reviews(db)
    db.close()
    return reviews, total_reviews
```

è®©æˆ‘ä»¬è¿˜å†™ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨ gradio åº”ç”¨ç¨‹åºåŠ è½½æ—¶åŠ è½½æœ€æ–°çš„è¯„è®º :

```python
def load_data():
    db = sqlite3.connect(DB_FILE)
    reviews, total_reviews = get_latest_reviews(db)
    db.close()
    return reviews, total_reviews
```

## ç¬¬äºŒæ­¥ - åˆ›å»º gradio åº”ç”¨ âš¡

çŽ°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†æ•°æ®åº“é€»è¾‘ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ gradio åˆ›å»ºä¸€ä¸ªåŠ¨æ€çš„ç½‘é¡µæ¥è¯¢é—®ç”¨æˆ·çš„åé¦ˆæ„è§ï¼

ä½¿ç”¨ä»¥ä¸‹ä»£ç æ®µ :

```python
with gr.Blocks() as demo:
    with gr.Row():
        with gr.Column():
            name = gr.Textbox(label="Name", placeholder="What is your name?")
            review = gr.Radio(label="How satisfied are you with using gradio?", choices=[1, 2, 3, 4, 5])
            comments = gr.Textbox(label="Comments", lines=10, placeholder="Do you have any feedback on gradio?")
            submit = gr.Button(value="Submit Feedback")
        with gr.Column():
            data = gr.Dataframe(label="Most recently created 10 rows")
            count = gr.Number(label="Total number of reviews")
    submit.click(add_review, [name, review, comments], [data, count])
    demo.load(load_data, None, [data, count])
```

## ç¬¬ä¸‰æ­¥ - ä¸Ž HuggingFace æ•°æ®é›†åŒæ­¥ ðŸ¤—

åœ¨ç¬¬ 2 æ­¥åŽæˆ‘ä»¬å¯ä»¥è°ƒç”¨ `demo.launch()` æ¥è¿è¡Œä¸€ä¸ªå®Œæ•´åŠŸèƒ½çš„åº”ç”¨ç¨‹åºã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ•°æ®å°†å­˜å‚¨åœ¨æœ¬åœ°æœºå™¨ä¸Šã€‚å¦‚æžœ sqlite æ–‡ä»¶æ„å¤–åˆ é™¤ï¼Œæˆ‘ä»¬å°†ä¸¢å¤±æ‰€æœ‰è¯„è®ºï¼è®©æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ•°æ®å¤‡ä»½åˆ° HuggingFace hub çš„æ•°æ®é›†ä¸­ã€‚

åœ¨ç»§ç»­ä¹‹å‰ï¼Œè¯·åœ¨[æ­¤å¤„](https://huggingface.co/datasets)åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ã€‚

çŽ°åœ¨ï¼Œåœ¨æˆ‘ä»¬è„šæœ¬çš„**é¡¶éƒ¨**ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[huggingface hub å®¢æˆ·ç«¯åº“](https://huggingface.co/docs/huggingface_hub/index)è¿žæŽ¥åˆ°æˆ‘ä»¬çš„æ•°æ®é›†å¹¶èŽ·å–æœ€æ–°çš„å¤‡ä»½ã€‚

```python
TOKEN = os.environ.get('HUB_TOKEN')
repo = huggingface_hub.Repository(
    local_dir="data",
    repo_type="dataset",
    clone_from="<name-of-your-dataset>",
    use_auth_token=TOKEN
)
repo.git_pull()

shutil.copyfile("./data/reviews.db", DB_FILE)
```

è¯·æ³¨æ„ï¼Œæ‚¨éœ€è¦ä»Ž HuggingFace çš„â€œè®¾ç½®â€é€‰é¡¹å¡ä¸­èŽ·å–è®¿é—®ä»¤ç‰Œï¼Œä»¥ä¸Šä»£ç æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚åœ¨è„šæœ¬ä¸­ï¼Œé€šè¿‡çŽ¯å¢ƒå˜é‡å®‰å…¨è®¿é—®ä»¤ç‰Œã€‚

![access_token](/assets/guides/access_token.png)

çŽ°åœ¨ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåŽå°ä»»åŠ¡ï¼Œæ¯ 60 ç§’å°†æˆ‘ä»¬çš„æœ¬åœ°æ•°æ®åº“ä¸Žæ•°æ®é›†ä¸­çš„æ•°æ®åŒæ­¥ä¸€æ¬¡ã€‚
æˆ‘ä»¬å°†ä½¿ç”¨[AdvancedPythonScheduler](https://apscheduler.readthedocs.io/en/3.x/)æ¥å¤„ç†è°ƒåº¦ã€‚
ç„¶è€Œï¼Œè¿™å¹¶ä¸æ˜¯å”¯ä¸€å¯ç”¨çš„ä»»åŠ¡è°ƒåº¦åº“ã€‚è¯·éšæ„ä½¿ç”¨æ‚¨ç†Ÿæ‚‰çš„ä»»ä½•åº“ã€‚

å¤‡ä»½æ•°æ®çš„å‡½æ•°å¦‚ä¸‹ :

```python
from apscheduler.schedulers.background import BackgroundScheduler

def backup_db():
    shutil.copyfile(DB_FILE, "./data/reviews.db")
    db = sqlite3.connect(DB_FILE)
    reviews = db.execute("SELECT * FROM reviews").fetchall()
    pd.DataFrame(reviews).to_csv("./data/reviews.csv", index=False)
    print("updating db")
    repo.push_to_hub(blocking=False, commit_message=f"Updating data at {datetime.datetime.now()}")


scheduler = BackgroundScheduler()
scheduler.add_job(func=backup_db, trigger="interval", seconds=60)
scheduler.start()
```

## ç¬¬å››æ­¥ï¼ˆé™„åŠ ï¼‰- éƒ¨ç½²åˆ° HuggingFace Spaces

æ‚¨å¯ä»¥ä½¿ç”¨ HuggingFace [Spaces](https://huggingface.co/spaces) å¹³å°å…è´¹éƒ¨ç½²è¿™ä¸ªåº”ç”¨ç¨‹åº âœ¨

å¦‚æžœæ‚¨ä¹‹å‰æ²¡æœ‰ä½¿ç”¨è¿‡ Spacesï¼Œè¯·æŸ¥çœ‹[æ­¤å¤„](/using_hugging_face_integrations)çš„å…ˆå‰æŒ‡å—ã€‚
æ‚¨å°†éœ€è¦å°† `HUB_TOKEN` çŽ¯å¢ƒå˜é‡ä½œä¸ºæŒ‡å—ä¸­çš„ä¸€ä¸ªç§˜å¯†ä½¿ç”¨ã€‚

## ç»“è®º

æ­å–œï¼æ‚¨çŸ¥é“å¦‚ä½•åœ¨æ‚¨çš„ gradio åº”ç”¨ç¨‹åºä¸­æŒ‰è®¡åˆ’è¿è¡ŒåŽå°ä»»åŠ¡â²ï¸ã€‚

åœ¨ Spaces ä¸Šè¿è¡Œçš„åº”ç”¨ç¨‹åºå¯åœ¨[æ­¤å¤„](https://huggingface.co/spaces/freddyaboulton/gradio-google-forms)æŸ¥çœ‹ã€‚
å®Œæ•´çš„ä»£ç åœ¨[æ­¤å¤„](https://huggingface.co/spaces/freddyaboulton/gradio-google-forms/blob/main/app.py)ã€‚

---

<!-- Source: guides/cn/07_other-tutorials/running-gradio-on-your-web-server-with-nginx.md -->
# åœ¨ Web æœåŠ¡å™¨ä¸Šä½¿ç”¨ Nginx è¿è¡Œ Gradio åº”ç”¨

æ ‡ç­¾ï¼šéƒ¨ç½²ï¼ŒWeb æœåŠ¡å™¨ï¼ŒNginx

## ä»‹ç»

Gradio æ˜¯ä¸€ä¸ª Python åº“ï¼Œå…è®¸æ‚¨å¿«é€Ÿåˆ›å»ºå¯å®šåˆ¶çš„ Web åº”ç”¨ç¨‹åºï¼Œç”¨äºŽæœºå™¨å­¦ä¹ æ¨¡åž‹å’Œæ•°æ®å¤„ç†æµæ°´çº¿ã€‚Gradio åº”ç”¨å¯ä»¥å…è´¹éƒ¨ç½²åœ¨[Hugging Face Spaces](https://hf.space)ä¸Šã€‚

ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨è‡ªå·±çš„ Web æœåŠ¡å™¨ä¸Šéƒ¨ç½² Gradio åº”ç”¨ã€‚æ‚¨å¯èƒ½å·²ç»åœ¨ä½¿ç”¨[Nginx](https://www.nginx.com/)ä½œä¸ºé«˜æ€§èƒ½çš„ Web æœåŠ¡å™¨æ¥æä¾›æ‚¨çš„ç½‘ç«™ï¼ˆä¾‹å¦‚ `https://www.example.com`ï¼‰ï¼Œå¹¶ä¸”æ‚¨å¸Œæœ›å°† Gradio é™„åŠ åˆ°ç½‘ç«™çš„ç‰¹å®šå­è·¯å¾„ä¸Šï¼ˆä¾‹å¦‚ `https://www.example.com/gradio-demo`ï¼‰ã€‚

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†æŒ‡å¯¼æ‚¨åœ¨è‡ªå·±çš„ Web æœåŠ¡å™¨ä¸Šçš„ Nginx åŽé¢è¿è¡Œ Gradio åº”ç”¨çš„è¿‡ç¨‹ï¼Œä»¥å®žçŽ°æ­¤ç›®çš„ã€‚

**å…ˆå†³æ¡ä»¶**

1. å®‰è£…äº† [Nginx çš„ Linux Web æœåŠ¡å™¨](https://www.nginx.com/blog/setting-up-nginx/) å’Œ [Gradio](/quickstart) åº“

2. åœ¨ Web æœåŠ¡å™¨ä¸Šå°† Gradio åº”ç”¨ä¿å­˜ä¸º Python æ–‡ä»¶

## ç¼–è¾‘ Nginx é…ç½®æ–‡ä»¶

1. é¦–å…ˆç¼–è¾‘ Web æœåŠ¡å™¨ä¸Šçš„ Nginx é…ç½®æ–‡ä»¶ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ–‡ä»¶ä½äºŽï¼š`/etc/nginx/nginx.conf`

åœ¨ `http` å—ä¸­ï¼Œæ·»åŠ ä»¥ä¸‹è¡Œä»¥ä»Žå•ç‹¬çš„æ–‡ä»¶åŒ…å«æœåŠ¡å™¨å—é…ç½®ï¼š

```bash
include /etc/nginx/sites-enabled/*;
```

2. åœ¨ `/etc/nginx/sites-available` ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶ï¼ˆå¦‚æžœç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰ï¼Œæ–‡ä»¶åè¡¨ç¤ºæ‚¨çš„åº”ç”¨ï¼Œä¾‹å¦‚ï¼š`sudo nano /etc/nginx/sites-available/my_gradio_app`

3. å°†ä»¥ä¸‹å†…å®¹ç²˜è´´åˆ°æ–‡ä»¶ç¼–è¾‘å™¨ä¸­ï¼š

```bash
server {
    listen 80;
    server_name example.com www.example.com;  # å°†æ­¤é¡¹æ›´æ”¹ä¸ºæ‚¨çš„åŸŸå

    location /gradio-demo/ {  # å¦‚æžœè¦åœ¨ä¸åŒè·¯å¾„ä¸Šæä¾›Gradioåº”ç”¨ï¼Œè¯·æ›´æ”¹æ­¤é¡¹
        proxy_pass http://127.0.0.1:7860/; # å¦‚æžœæ‚¨çš„Gradioåº”ç”¨å°†åœ¨ä¸åŒç«¯å£ä¸Šè¿è¡Œï¼Œè¯·æ›´æ”¹æ­¤é¡¹
        proxy_redirect off;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
    }
}
```

## åœ¨ Web æœåŠ¡å™¨ä¸Šè¿è¡Œ Gradio åº”ç”¨

1. åœ¨å¯åŠ¨ Gradio åº”ç”¨ä¹‹å‰ï¼Œæ‚¨éœ€è¦å°† `root_path` è®¾ç½®ä¸ºä¸Ž Nginx é…ç½®ä¸­æŒ‡å®šçš„å­è·¯å¾„ç›¸åŒã€‚è¿™å¯¹äºŽ Gradio åœ¨é™¤åŸŸçš„æ ¹è·¯å¾„ä¹‹å¤–çš„ä»»ä½•å­è·¯å¾„ä¸Šè¿è¡Œæ˜¯å¿…è¦çš„ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå…·æœ‰è‡ªå®šä¹‰ `root_path` çš„ç®€å•ç¤ºä¾‹ Gradio åº”ç”¨ï¼š

```python
import gradio as gr
import time

def test(x):
    time.sleep(4)
    return x

gr.Interface(test, "textbox", "textbox").queue().launch(root_path="/gradio-demo")
```

2. é€šè¿‡é”®å…¥ `tmux` å¹¶æŒ‰å›žè½¦é”®ï¼ˆå¯é€‰ï¼‰å¯åŠ¨ `tmux` ä¼šè¯

æŽ¨èåœ¨ `tmux` ä¼šè¯ä¸­è¿è¡Œ Gradio åº”ç”¨ï¼Œä»¥ä¾¿å¯ä»¥è½»æ¾åœ°åœ¨åŽå°è¿è¡Œå®ƒ

3. ç„¶åŽï¼Œå¯åŠ¨æ‚¨çš„ Gradio åº”ç”¨ã€‚åªéœ€è¾“å…¥ `python`ï¼ŒåŽè·Ÿæ‚¨çš„ Gradio Python æ–‡ä»¶çš„åç§°ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œåº”ç”¨å°†åœ¨ `localhost:7860` ä¸Šè¿è¡Œï¼Œä½†å¦‚æžœå®ƒåœ¨å…¶ä»–ç«¯å£ä¸Šå¯åŠ¨ï¼Œæ‚¨éœ€è¦æ›´æ–°ä¸Šé¢çš„ Nginx é…ç½®æ–‡ä»¶ã€‚

## é‡æ–°å¯åŠ¨ Nginx

1. å¦‚æžœæ‚¨åœ¨ tmux ä¼šè¯ä¸­ï¼Œè¯·é€šè¿‡é”®å…¥ CTRL + Bï¼ˆæˆ– CMD + Bï¼‰ï¼Œç„¶åŽæŒ‰ä¸‹ "D" é”®æ¥é€€å‡ºã€‚

2. æœ€åŽï¼Œé€šè¿‡è¿è¡Œ `sudo systemctl restart nginx` é‡æ–°å¯åŠ¨ nginxã€‚

å°±æ˜¯è¿™æ ·ï¼å¦‚æžœæ‚¨åœ¨æµè§ˆå™¨ä¸­è®¿é—® `https://example.com/gradio-demo`ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿçœ‹åˆ°æ‚¨çš„ Gradio åº”ç”¨åœ¨é‚£é‡Œè¿è¡Œã€‚

---

<!-- Source: guides/cn/07_other-tutorials/theming-guide.md -->
# ä¸»é¢˜ Theming

Tags: THEMES

## ä»‹ç»

Gradio å…·æœ‰å†…ç½®çš„ä¸»é¢˜å¼•æ“Žï¼Œå¯è®©æ‚¨è‡ªå®šä¹‰åº”ç”¨çš„å¤–è§‚å’Œæ„Ÿè§‰ã€‚æ‚¨å¯ä»¥é€‰æ‹©å„ç§ä¸»é¢˜ï¼Œæˆ–è€…åˆ›å»ºè‡ªå·±çš„ä¸»é¢˜ã€‚è¦è¿™æ ·åšï¼Œè¯·å°† `theme=` kwarg ä¼ é€’ç»™ `Blocks` æˆ– `Interface` æž„é€ å‡½æ•°ã€‚ä¾‹å¦‚ï¼š

```python
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    ...
```

<div class="wrapper">
<iframe
	src="https://gradio-theme-soft.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

Gradio å¸¦æœ‰ä¸€ç»„é¢„æž„å»ºçš„ä¸»é¢˜ï¼Œæ‚¨å¯ä»¥ä»Ž `gr.themes.*` ä¸­åŠ è½½è¿™äº›ä¸»é¢˜ã€‚è¿™äº›ä¸»é¢˜åŒ…æ‹¬ï¼š

- `gr.themes.Base()`
- `gr.themes.Default()`
- `gr.themes.Glass()`
- `gr.themes.Monochrome()`
- `gr.themes.Soft()`

è¿™äº›ä¸»é¢˜ä¸ºæ•°ç™¾ä¸ª CSS å˜é‡è®¾ç½®äº†å€¼ã€‚æ‚¨å¯ä»¥ä½¿ç”¨é¢„æž„å»ºçš„ä¸»é¢˜ä½œä¸ºè‡ªå®šä¹‰ä¸»é¢˜çš„èµ·ç‚¹ï¼Œä¹Ÿå¯ä»¥ä»Žå¤´å¼€å§‹åˆ›å»ºè‡ªå·±çš„ä¸»é¢˜ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æ¯ç§æ–¹æ³•ã€‚

## ä½¿ç”¨ä¸»é¢˜æž„å»ºå™¨

ä½¿ç”¨ä¸»é¢˜æž„å»ºå™¨æž„å»ºä¸»é¢˜æœ€ç®€å•ã€‚è¦åœ¨æœ¬åœ°å¯åŠ¨ä¸»é¢˜æž„å»ºå™¨ï¼Œè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```python
import gradio as gr

gr.themes.builder()
```

$demo_theme_builder

æ‚¨å¯ä»¥ä½¿ç”¨ä¸Šé¢çš„ Spaces ä¸Šè¿è¡Œçš„ Theme Builderï¼Œä½†é€šè¿‡ `gr.themes.builder()` åœ¨æœ¬åœ°å¯åŠ¨æ—¶è¿è¡Œé€Ÿåº¦æ›´å¿«ã€‚

åœ¨ Theme Builder ä¸­ç¼–è¾‘å€¼æ—¶ï¼Œåº”ç”¨ç¨‹åºå°†å®žæ—¶é¢„è§ˆæ›´æ–°ã€‚æ‚¨å¯ä»¥ä¸‹è½½ç”Ÿæˆçš„ä¸»é¢˜ä»£ç ï¼Œä»¥ä¾¿åœ¨ä»»ä½• Gradio åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨å®ƒã€‚

åœ¨æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä»¥ç¼–ç¨‹æ–¹å¼æž„å»ºä¸»é¢˜ã€‚

## é€šè¿‡æž„é€ å‡½æ•°æ‰©å±•ä¸»é¢˜

å°½ç®¡æ¯ä¸ªä¸»é¢˜éƒ½æœ‰æ•°ç™¾ä¸ª CSS å˜é‡ï¼Œä½†å¤§å¤šæ•°è¿™äº›å˜é‡çš„å€¼éƒ½æ˜¯ä»Ž 8 ä¸ªæ ¸å¿ƒå˜é‡ä¸­èŽ·å–çš„ï¼Œå¯ä»¥é€šè¿‡æ¯ä¸ªé¢„æž„å»ºä¸»é¢˜çš„æž„é€ å‡½æ•°è®¾ç½®è¿™äº›å˜é‡ã€‚é€šè¿‡ä¿®æ”¹è¿™ 8 ä¸ªå‚æ•°çš„å€¼ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿæ›´æ”¹åº”ç”¨ç¨‹åºçš„å¤–è§‚å’Œæ„Ÿè§‰ã€‚

### æ ¸å¿ƒé¢œè‰²

å‰ 3 ä¸ªæž„é€ å‡½æ•°å‚æ•°è®¾ç½®ä¸»é¢˜çš„é¢œè‰²ï¼Œå¹¶ä¸”æ˜¯ `gradio.themes.Color` å¯¹è±¡ã€‚åœ¨å†…éƒ¨ï¼Œè¿™äº› Color å¯¹è±¡åŒ…å«å•ä¸ªè‰²è°ƒçš„è°ƒè‰²æ¿çš„äº®åº¦å€¼ï¼ŒèŒƒå›´ä»Ž 50ï¼Œ100ï¼Œ200...ï¼Œ800ï¼Œ900ï¼Œ950ã€‚å…¶ä»– CSS å˜é‡æ˜¯ä»Žè¿™ 3 ç§é¢œè‰²æ´¾ç”Ÿçš„ã€‚

3 ä¸ªé¢œè‰²æž„é€ å‡½æ•°å‚æ•°æ˜¯ï¼š

- `primary_hue`ï¼šè¿™æ˜¯ä¸»é¢˜ä¸­çš„ä¸»è‰²ã€‚åœ¨é»˜è®¤ä¸»é¢˜ä¸­ï¼Œæ­¤å€¼è®¾ç½®ä¸º `gradio.themes.colors.orange`ã€‚
- `secondary_hue`ï¼šè¿™æ˜¯ä¸»é¢˜ä¸­ç”¨äºŽè¾…åŠ©å…ƒç´ çš„é¢œè‰²ã€‚åœ¨é»˜è®¤ä¸»é¢˜ä¸­ï¼Œæ­¤å€¼è®¾ç½®ä¸º `gradio.themes.colors.blue`ã€‚
- `neutral_hue`ï¼šè¿™æ˜¯ä¸»é¢˜ä¸­ç”¨äºŽæ–‡æœ¬å’Œå…¶ä»–ä¸­æ€§å…ƒç´ çš„é¢œè‰²ã€‚åœ¨é»˜è®¤ä¸»é¢˜ä¸­ï¼Œæ­¤å€¼è®¾ç½®ä¸º `gradio.themes.colors.gray`ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨å­—ç¬¦ä¸²å¿«æ·æ–¹å¼ä¿®æ”¹è¿™äº›å€¼ï¼Œä¾‹å¦‚

```python
with gr.Blocks(theme=gr.themes.Default(primary_hue="red", secondary_hue="pink")) as demo:
    ...
```

æˆ–è€…ç›´æŽ¥ä½¿ç”¨ `Color` å¯¹è±¡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
with gr.Blocks(theme=gr.themes.Default(primary_hue=gr.themes.colors.red, secondary_hue=gr.themes.colors.pink)) as demo:
    ...
```

<div class="wrapper">
<iframe
	src="https://gradio-theme-extended-step-1.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

é¢„å®šä¹‰çš„é¢œè‰²åŒ…æ‹¬ï¼š

- `slate`
- `gray`
- `zinc`
- `neutral`
- `stone`
- `red`
- `orange`
- `amber`
- `yellow`
- `lime`
- `green`
- `emerald`
- `teal`
- `cyan`
- `sky`
- `blue`
- `indigo`
- `violet`
- `purple`
- `fuchsia`
- `pink`
- `rose`

æ‚¨è¿˜å¯ä»¥åˆ›å»ºè‡ªå·±çš„è‡ªå®šä¹‰ `Color` å¯¹è±¡å¹¶ä¼ é€’å®ƒä»¬ã€‚

### æ ¸å¿ƒå¤§å° ï¼ˆCore Sizingï¼‰

æŽ¥ä¸‹æ¥çš„ 3 ä¸ªæž„é€ å‡½æ•°å‚æ•°è®¾ç½®ä¸»é¢˜çš„å¤§å°ï¼Œå¹¶ä¸”æ˜¯ `gradio.themes.Size` å¯¹è±¡ã€‚åœ¨å†…éƒ¨ï¼Œè¿™äº› Size å¯¹è±¡åŒ…å«ä»Ž `xxs` åˆ° `xxl` çš„åƒç´ å¤§å°å€¼ã€‚å…¶ä»– CSS å˜é‡æ˜¯ä»Žè¿™ 3 ä¸ªå¤§å°æ´¾ç”Ÿçš„ã€‚

- `spacing_size`ï¼šæ­¤è®¾ç½®äº†å…ƒç´ å†…éƒ¨çš„å¡«å……å’Œå…ƒç´ ä¹‹é—´çš„é—´è·ã€‚åœ¨é»˜è®¤ä¸»é¢˜ä¸­ï¼Œæ­¤å€¼è®¾ç½®ä¸º `gradio.themes.sizes.spacing_md`ã€‚
- `radius_size`ï¼šæ­¤è®¾ç½®äº†å…ƒç´ çš„åœ†è§’å¼§åº¦ã€‚åœ¨é»˜è®¤ä¸»é¢˜ä¸­ï¼Œæ­¤å€¼è®¾ç½®ä¸º `gradio.themes.sizes.radius_md`ã€‚
- `text_size`ï¼šæ­¤è®¾ç½®äº†æ–‡æœ¬çš„å­—ä½“å¤§å°ã€‚åœ¨é»˜è®¤ä¸»é¢˜ä¸­ï¼Œæ­¤å€¼è®¾ç½®ä¸º `gradio.themes.sizes.text_md`ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨å­—ç¬¦ä¸²å¿«æ·æ–¹å¼ä¿®æ”¹è¿™äº›å€¼ï¼Œä¾‹å¦‚

```python
with gr.Blocks(theme=gr.themes.Default(spacing_size="sm", radius_size="none")) as demo:
    ...
```

æˆ–è€…ç›´æŽ¥ä½¿ç”¨ `Size` å¯¹è±¡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
with gr.Blocks(theme=gr.themes.Default(spacing_size=gr.themes.sizes.spacing_sm, radius_size=gr.themes.sizes.radius_none)) as demo:
    ...
```

<div class="wrapper">
<iframe
	src="https://gradio-theme-extended-step-2.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

é¢„å®šä¹‰çš„å¤§å°å¯¹è±¡åŒ…æ‹¬ï¼š

- `radius_none`
- `radius_sm`
- `radius_md`
- `radius_lg`
- `spacing_sm`
- `spacing_md`
- `spacing_lg`
- `text_sm`
- `text_md`
- `text_lg`

æ‚¨è¿˜å¯ä»¥åˆ›å»ºè‡ªå·±çš„è‡ªå®šä¹‰ `Size` å¯¹è±¡å¹¶ä¼ é€’å®ƒä»¬ã€‚

### æ ¸å¿ƒå­—ä½“ï¼ˆCore Fontsï¼‰

æœ€åŽçš„ 2 ä¸ªæž„é€ å‡½æ•°å‚æ•°è®¾ç½®ä¸»é¢˜çš„å­—ä½“ã€‚æ‚¨å¯ä»¥å°†ä¸€ç³»åˆ—å­—ä½“ä¼ é€’ç»™è¿™äº›å‚æ•°ï¼Œä»¥æŒ‡å®šå›žé€€å­—ä½“ã€‚å¦‚æžœæä¾›äº†å­—ç¬¦ä¸²ï¼Œå®ƒå°†è¢«åŠ è½½ä¸ºç³»ç»Ÿå­—ä½“ã€‚å¦‚æžœæä¾›äº† `gradio.themes.GoogleFont`ï¼Œåˆ™å°†ä»Ž Google Fonts åŠ è½½è¯¥å­—ä½“ã€‚

- `font`ï¼šæ­¤è®¾ç½®ä¸»é¢˜çš„ä¸»è¦å­—ä½“ã€‚åœ¨é»˜è®¤ä¸»é¢˜ä¸­ï¼Œæ­¤å€¼è®¾ç½®ä¸º `gradio.themes.GoogleFont("IBM Plex Sans")`ã€‚
- `font_mono`ï¼šæ­¤è®¾ç½®ä¸»é¢˜çš„ç­‰å®½å­—ä½“ã€‚åœ¨é»˜è®¤ä¸»é¢˜ä¸­ï¼Œæ­¤å€¼è®¾ç½®ä¸º `gradio.themes.GoogleFont("IBM Plex Mono")`ã€‚

æ‚¨å¯ä»¥ä¿®æ”¹è¿™äº›å€¼ï¼Œä¾‹å¦‚ä»¥ä¸‹æ–¹å¼ï¼š

```python
with gr.Blocks(theme=gr.themes.Default(font=[gr.themes.GoogleFont("Inconsolata"), "Arial", "sans-serif"])) as demo:
    ...
```

<div class="wrapper">
<iframe
	src="https://gradio-theme-extended-step-3.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

## é€šè¿‡ `.set()` æ‰©å±•ä¸»é¢˜

ä¸»é¢˜åŠ è½½åŽï¼Œæ‚¨è¿˜å¯ä»¥ä¿®æ”¹ CSS å˜é‡çš„å€¼ã€‚ä¸ºæ­¤ï¼Œè¯·ä½¿ç”¨ä¸»é¢˜å¯¹è±¡çš„ `.set()` æ–¹æ³•æ¥è®¿é—® CSS å˜é‡ã€‚ä¾‹å¦‚ï¼š

```python
theme = gr.themes.Default(primary_hue="blue").set(    loader_color="#FF0000",    slider_color="#FF0000",)
ä½¿ç”¨`gr.Blocks(theme=theme)`åˆ›å»ºæ¼”ç¤ºå—    ...
```

åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°† `loader_color` å’Œ `slider_color` å˜é‡è®¾ç½®ä¸º`#FF0000`ï¼Œå°½ç®¡æ•´ä½“ `primary_color` ä½¿ç”¨è“è‰²è°ƒè‰²æ¿ã€‚æ‚¨å¯ä»¥ä»¥è¿™ç§æ–¹å¼è®¾ç½®ä¸»é¢˜ä¸­å®šä¹‰çš„ä»»ä½• CSS å˜é‡ã€‚
æ‚¨çš„ IDE ç±»åž‹æç¤ºåº”è¯¥å¸®åŠ©æ‚¨å¯¼èˆªè¿™äº›å˜é‡ã€‚ç”±äºŽæœ‰å¾ˆå¤š CSS å˜é‡ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¿™äº›å˜é‡çš„å‘½åå’Œç»„ç»‡æ–¹å¼ã€‚

### CSS å˜é‡å‘½åè§„èŒƒ

CSS å˜é‡åå¯èƒ½ä¼šå˜å¾—å¾ˆé•¿ï¼Œä¾‹å¦‚ `button_primary_background_fill_hover_dark`ï¼ä½†æ˜¯å®ƒä»¬éµå¾ªä¸€ç§å¸¸è§çš„å‘½åçº¦å®šï¼Œä½¿å¾—ç†è§£å˜é‡åŠŸèƒ½å’ŒæŸ¥æ‰¾æ‚¨è¦æŸ¥æ‰¾çš„å˜é‡å˜å¾—å®¹æ˜“ã€‚å˜é‡åç”±ä¸‹åˆ’çº¿åˆ†éš”ï¼Œç”±ä»¥ä¸‹ç»„æˆï¼š

1. ç›®æ ‡å…ƒç´ ï¼Œä¾‹å¦‚ `button`ã€`slider` æˆ– `block`ã€‚2. ç›®æ ‡å…ƒç´ ç±»åž‹æˆ–å­å…ƒç´ ï¼Œä¾‹å¦‚ `button_primary` æˆ– `block_label`ã€‚3. å±žæ€§ï¼Œä¾‹å¦‚ `button_primary_background_fill` æˆ– `block_label_border_width`ã€‚4. ä»»ä½•ç›¸å…³çŠ¶æ€ï¼Œä¾‹å¦‚ `button_primary_background_fill_hover`ã€‚5. å¦‚æžœåœ¨æš—æ¨¡å¼ä¸­å€¼ä¸åŒï¼Œåˆ™ä½¿ç”¨åŽç¼€ `_dark`ã€‚ä¾‹å¦‚ï¼Œ`input_border_color_focus_dark`ã€‚
   å½“ç„¶ï¼Œè®¸å¤š CSS å˜é‡åéƒ½æ¯”è¿™ä¸ªçŸ­ï¼Œä¾‹å¦‚ `table_border_color` æˆ– `input_shadow`ã€‚

### CSS å˜é‡ç»„ç»‡

è™½ç„¶æœ‰æ•°ç™¾ä¸ª CSS å˜é‡ï¼Œä½†å¹¶ä¸éœ€è¦ä¸ºæ¯ä¸ªå˜é‡éƒ½æŒ‡å®šå•ç‹¬çš„å€¼ã€‚å®ƒä»¬é€šè¿‡å¼•ç”¨ä¸€ç»„æ ¸å¿ƒå˜é‡å’Œå½¼æ­¤å¼•ç”¨æ¥èŽ·å–å€¼ã€‚è¿™æ ·åšå¯ä»¥ä»…ä¿®æ”¹å°‘é‡å˜é‡ä»¥æ”¹å˜æ•´ä¸ªä¸»é¢˜çš„å¤–è§‚å’Œæ„Ÿè§‰ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥æ›´ç²¾ç»†åœ°æŽ§åˆ¶æˆ‘ä»¬å¯èƒ½æƒ³è¦ä¿®æ”¹çš„ä¸ªåˆ«å…ƒç´ ã€‚

#### å¼•ç”¨æ ¸å¿ƒå˜é‡

è¦å¼•ç”¨å…¶ä¸­ä¸€ä¸ªæ ¸å¿ƒæž„é€ å‡½æ•°å˜é‡ï¼Œè¯·åœ¨å˜é‡åå‰åŠ ä¸Šæ˜Ÿå·ã€‚è¦å¼•ç”¨æ ¸å¿ƒé¢œè‰²ï¼Œè¯·ä½¿ç”¨`*primary_`ã€`*secondary_` æˆ–`*neutral_` å‰ç¼€ï¼ŒåŽè·Ÿäº®åº¦å€¼ã€‚ä¾‹å¦‚ï¼š

```python
theme = gr.themes.Default(primary_hue="blue").set(
    button_primary_background_fill="*primary_200",
    button_primary_background_fill_hover="*primary_300",
)
```

åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°† `button_primary_background_fill` å’Œ `button_primary_background_fill_hover` å˜é‡åˆ†åˆ«è®¾ç½®ä¸º`*primary_200` å’Œ`*primary_300`ã€‚è¿™äº›å˜é‡å°†åˆ†åˆ«è®¾ç½®ä¸ºè“è‰²ä¸»è‰²è°ƒè°ƒè‰²æ¿çš„ 200 å’Œ 300 äº®åº¦å€¼ã€‚
åŒæ ·åœ°ï¼Œè¦å¼•ç”¨æ ¸å¿ƒå¤§å°ï¼Œè¯·ä½¿ç”¨`*spacing_`ã€`*radius_` æˆ–`*text_` å‰ç¼€ï¼ŒåŽè·Ÿå¤§å°å€¼ã€‚ä¾‹å¦‚ï¼š

```python
theme = gr.themes.Default(radius_size="md").set(
    button_primary_border_radius="*radius_xl",
)
```

åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°† `button_primary_border_radius` å˜é‡è®¾ç½®ä¸º`*radius_xl`ã€‚æ­¤å˜é‡å°†è®¾ç½®ä¸ºä¸­ç­‰åŠå¾„å¤§å°èŒƒå›´çš„ `xl` è®¾ç½®ã€‚

#### å¼•ç”¨å…¶ä»–å˜é‡

å˜é‡ä¹Ÿå¯ä»¥å¼•ç”¨å½¼æ­¤ã€‚ä¾‹å¦‚ï¼Œè¯·çœ‹ä¸‹é¢çš„ç¤ºä¾‹ï¼š

```python
theme = gr.themes.Default().set(
    button_primary_background_fill="#FF0000",
    button_primary_background_fill_hover="#FF0000",
    button_primary_border="#FF0000",
)
```

å°†è¿™äº›å€¼è®¾ç½®ä¸ºç›¸åŒçš„é¢œè‰²æœ‰ç‚¹ç¹çã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ `button_primary_background_fill_hover` å’Œ `button_primary_border` å˜é‡ä¸­ä½¿ç”¨`*` å‰ç¼€å¼•ç”¨ `button_primary_background_fill` å˜é‡ã€‚

```python
theme = gr.themes.Default().set(
    button_primary_background_fill="#FF0000",
    button_primary_background_fill_hover="*button_primary_background_fill",
    button_primary_border="*button_primary_background_fill",
)
```

çŽ°åœ¨ï¼Œå¦‚æžœæˆ‘ä»¬æ›´æ”¹ `button_primary_background_fill` å˜é‡ï¼Œ`button_primary_background_fill_hover` å’Œ `button_primary_border` å˜é‡å°†è‡ªåŠ¨æ›´æ–°ã€‚
å¦‚æžœæ‚¨æ‰“ç®—å…±äº«ä¸»é¢˜ï¼Œè¿™å°†éžå¸¸æœ‰ç”¨- å®ƒä½¿å¾—ä¿®æ”¹ä¸»é¢˜å˜å¾—å®¹æ˜“ï¼Œè€Œæ— éœ€æ›´æ”¹æ¯ä¸ªå˜é‡ã€‚
è¯·æ³¨æ„ï¼Œæš—æ¨¡å¼å˜é‡è‡ªåŠ¨ç›¸äº’å¼•ç”¨ã€‚ä¾‹å¦‚ï¼š

```python
theme = gr.themes.Default().set(
    button_primary_background_fill="#FF0000",
    button_primary_background_fill_dark="#AAAAAA",
    button_primary_border="*button_primary_background_fill",
    button_primary_border_dark="*button_primary_background_fill_dark",
)
```

`button_primary_border_dark` å°†ä»Ž `button_primary_background_fill_dark` èŽ·å–å…¶å€¼ï¼Œå› ä¸ºæš—æ¨¡å¼æ€»æ˜¯ä½¿ç”¨å˜é‡çš„æš—ç‰ˆæœ¬ã€‚

## åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„ä¸»é¢˜

å‡è®¾æ‚¨æƒ³ä»Žå¤´å¼€å§‹åˆ›å»ºä¸€ä¸ªä¸»é¢˜ï¼æˆ‘ä»¬å°†é€æ­¥è¿›è¡Œ - æ‚¨è¿˜å¯ä»¥å‚è€ƒ gradio æºä»£ç åº“ä¸­é¢„æž„å»ºä¸»é¢˜çš„æºä»£ç ï¼Œè¯·çœ‹è¿™é‡Œçš„ç¤ºä¾‹ï¼š[Monochrome theme çš„æºä»£ç ](https://github.com/gradio-app/gradio/blob/main/gradio/themes/monochrome.py)
æˆ‘ä»¬çš„æ–°ä¸»é¢˜ç±»å°†ç»§æ‰¿è‡ª `gradio.themes.Base`ï¼Œè¿™æ˜¯ä¸€ä¸ªè®¾ç½®äº†è®¸å¤šæ–¹ä¾¿é»˜è®¤å€¼çš„ä¸»é¢˜ã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåä¸º Seafoam çš„ç®€å•æ¼”ç¤ºï¼Œä»¥åŠä½¿ç”¨å®ƒçš„ç®€å•åº”ç”¨ç¨‹åºã€‚
$code_theme_new_step_1

<div class="wrapper">
<iframe
	src="https://gradio-theme-new-step-1.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

Base ä¸»é¢˜éžå¸¸ç®€æ´ï¼Œä½¿ç”¨ `gr.themes.Blue` ä½œä¸ºå…¶ä¸»è¦é¢œè‰²-ç”±äºŽæ­¤åŽŸå› ï¼Œä¸»æŒ‰é’®å’ŒåŠ è½½åŠ¨ç”»éƒ½æ˜¯è“è‰²çš„ã€‚è®©æˆ‘ä»¬æ”¹å˜åº”ç”¨ç¨‹åºçš„é»˜è®¤æ ¸å¿ƒå‚æ•°ã€‚æˆ‘ä»¬å°†è¦†ç›–æž„é€ å‡½æ•°å¹¶ä¼ é€’æ–°çš„é»˜è®¤å€¼ç»™æ ¸å¿ƒæž„é€ å‡½æ•°å‚æ•°ã€‚
æˆ‘ä»¬å°†ä½¿ç”¨ `gr.themes.Emerald` ä½œä¸ºæˆ‘ä»¬çš„ä¸»è¦é¢œè‰²ï¼Œå¹¶å°†æ¬¡è¦å’Œä¸­æ€§è‰²è°ƒè®¾ç½®ä¸º `gr.themes.Blue`ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `text_lg` ä½¿æ–‡æœ¬æ›´å¤§ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `Quicksand` ä½œä¸ºæˆ‘ä»¬çš„é»˜è®¤å­—ä½“ï¼Œä»Ž Google Fonts åŠ è½½ã€‚
$code_theme_new_step_2

<div class="wrapper">
<iframe
	src="https://gradio-theme-new-step-2.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

æ³¨æ„åˆ°ä¸»æŒ‰é’®å’ŒåŠ è½½åŠ¨ç”»çŽ°åœ¨æ˜¯ç»¿è‰²çš„äº†å—ï¼Ÿè¿™äº› CSS å˜é‡ä¸Ž `primary_hue` ç›¸å…³è”ã€‚
æˆ‘ä»¬æ¥ç›´æŽ¥ä¿®æ”¹ä¸»é¢˜ã€‚æˆ‘ä»¬å°†è°ƒç”¨ `set()` æ–¹æ³•æ¥æ˜Žç¡®è¦†ç›– CSS å˜é‡å€¼ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½• CSS é€»è¾‘ï¼Œå¹¶ä½¿ç”¨`*` å‰ç¼€å¼•ç”¨æˆ‘ä»¬çš„æ ¸å¿ƒæž„é€ å‡½æ•°çš„å‚æ•°ã€‚

$code_theme_new_step_3

<div class="wrapper">
<iframe
	src="https://gradio-theme-new-step-3.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

çœ‹çœ‹æˆ‘ä»¬çš„ä¸»é¢˜çŽ°åœ¨å¤šä¹ˆæœ‰è¶£ï¼ä»…é€šè¿‡å‡ ä¸ªå˜é‡çš„æ›´æ”¹ï¼Œæˆ‘ä»¬çš„ä¸»é¢˜å®Œå…¨æ”¹å˜äº†ã€‚

æ‚¨å¯èƒ½ä¼šå‘çŽ°æŽ¢ç´¢[å…¶ä»–é¢„å»ºä¸»é¢˜çš„æºä»£ç ](https://github.com/gradio-app/gradio/blob/main/gradio/themes)ä¼šå¾ˆæœ‰å¸®åŠ©ï¼Œä»¥äº†è§£ä»–ä»¬å¦‚ä½•ä¿®æ”¹åŸºæœ¬ä¸»é¢˜ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨æµè§ˆå™¨çš„æ£€æŸ¥å·¥å…·ï¼Œé€‰æ‹© UI ä¸­çš„å…ƒç´ å¹¶æŸ¥çœ‹åœ¨æ ·å¼é¢æ¿ä¸­ä½¿ç”¨çš„ CSS å˜é‡ã€‚

## åˆ†äº«ä¸»é¢˜

åœ¨åˆ›å»ºä¸»é¢˜åŽï¼Œæ‚¨å¯ä»¥å°†å…¶ä¸Šä¼ åˆ° HuggingFace Hubï¼Œè®©å…¶ä»–äººæŸ¥çœ‹ã€ä½¿ç”¨å’Œæž„å»ºä¸»é¢˜ï¼

### ä¸Šä¼ ä¸»é¢˜

æœ‰ä¸¤ç§ä¸Šä¼ ä¸»é¢˜çš„æ–¹å¼ï¼Œé€šè¿‡ä¸»é¢˜ç±»å®žä¾‹æˆ–å‘½ä»¤è¡Œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„â€œseafoamâ€ä¸»é¢˜æ¥ä»‹ç»è¿™ä¸¤ç§æ–¹å¼ã€‚

- é€šè¿‡ç±»å®žä¾‹

æ¯ä¸ªä¸»é¢˜å®žä¾‹éƒ½æœ‰ä¸€ä¸ªåä¸ºâ€œpush_to_hubâ€çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥å°†ä¸»é¢˜ä¸Šä¼ åˆ° HuggingFace Hubã€‚

```python
seafoam.push_to_hub(repo_name="seafoam",
                    version="0.0.1",
					hf_token="<token>")
```

- é€šè¿‡å‘½ä»¤è¡Œ

é¦–å…ˆå°†ä¸»é¢˜ä¿å­˜åˆ°ç£ç›˜

```python
seafoam.dump(filename="seafoam.json")
```

ç„¶åŽä½¿ç”¨â€œupload_themeâ€å‘½ä»¤ï¼š

```bash
upload_theme\
"seafoam.json"\
"seafoam"\
--version "0.0.1"\
--hf_token "<token>"
```

è¦ä¸Šä¼ ä¸»é¢˜ï¼Œæ‚¨å¿…é¡»æ‹¥æœ‰ä¸€ä¸ª HuggingFace è´¦æˆ·ï¼Œå¹¶é€šè¿‡ `hf_token` å‚æ•°ä¼ é€’æ‚¨çš„[è®¿é—®ä»¤ç‰Œ](https://huggingface.co/docs/huggingface_hub/quick-start#login)ã€‚
ä½†æ˜¯ï¼Œå¦‚æžœæ‚¨é€šè¿‡[HuggingFace å‘½ä»¤è¡Œ](https://huggingface.co/docs/huggingface_hub/quick-start#login)ç™»å½•ï¼ˆä¸Ž `gradio` ä¸€èµ·å®‰è£…ï¼‰ï¼Œ
é‚£ä¹ˆæ‚¨å¯ä»¥çœç•¥ `hf_token` å‚æ•°ã€‚

`version` å‚æ•°å…è®¸æ‚¨ä¸ºä¸»é¢˜æŒ‡å®šä¸€ä¸ªæœ‰æ•ˆçš„[è¯­ä¹‰ç‰ˆæœ¬](https://www.geeksforgeeks.org/introduction-semantic-versioning/)å­—ç¬¦ä¸²ã€‚
è¿™æ ·ï¼Œæ‚¨çš„ç”¨æˆ·å°±å¯ä»¥åœ¨ä»–ä»¬çš„åº”ç”¨ç¨‹åºä¸­æŒ‡å®šè¦ä½¿ç”¨çš„ä¸»é¢˜ç‰ˆæœ¬ã€‚è¿™è¿˜å…è®¸æ‚¨å‘å¸ƒä¸»é¢˜æ›´æ–°è€Œä¸å¿…æ‹…å¿ƒ
ä»¥å‰åˆ›å»ºçš„åº”ç”¨ç¨‹åºçš„å¤–è§‚å¦‚ä½•æ›´æ”¹ã€‚`version` å‚æ•°æ˜¯å¯é€‰çš„ã€‚å¦‚æžœçœç•¥ï¼Œä¸‹ä¸€ä¸ªä¿®è®¢ç‰ˆæœ¬å°†è‡ªåŠ¨åº”ç”¨ã€‚

### ä¸»é¢˜é¢„è§ˆ

é€šè¿‡è°ƒç”¨ `push_to_hub` æˆ– `upload_theme`ï¼Œä¸»é¢˜èµ„æºå°†å­˜å‚¨åœ¨[HuggingFace ç©ºé—´](https://huggingface.co/docs/hub/spaces-overview)ä¸­ã€‚

æˆ‘ä»¬çš„ seafoam ä¸»é¢˜çš„é¢„è§ˆåœ¨è¿™é‡Œï¼š[seafoam é¢„è§ˆ](https://huggingface.co/spaces/gradio/seafoam)ã€‚

<div class="wrapper">
<iframe
	src="https://gradio-seafoam.hf.space?__theme=light"
	frameborder="0"
></iframe>
</div>

### å‘çŽ°ä¸»é¢˜

[ä¸»é¢˜åº“](https://huggingface.co/spaces/gradio/theme-gallery)æ˜¾ç¤ºäº†æ‰€æœ‰å…¬å¼€çš„ gradio ä¸»é¢˜ã€‚åœ¨å‘å¸ƒä¸»é¢˜ä¹‹åŽï¼Œ
å®ƒå°†åœ¨å‡ åˆ†é’ŸåŽè‡ªåŠ¨æ˜¾ç¤ºåœ¨ä¸»é¢˜åº“ä¸­ã€‚

æ‚¨å¯ä»¥æŒ‰ç…§ç©ºé—´ä¸Šç‚¹èµžçš„æ•°é‡ä»¥åŠæŒ‰åˆ›å»ºæ—¶é—´ä»Žæœ€è¿‘åˆ°æœ€è¿‘å¯¹ä¸»é¢˜è¿›è¡ŒæŽ’åºï¼Œä¹Ÿå¯ä»¥åœ¨æµ…è‰²å’Œæ·±è‰²æ¨¡å¼ä¹‹é—´åˆ‡æ¢ä¸»é¢˜ã€‚

<div class="wrapper">
<iframe
	src="https://gradio-theme-gallery.hf.space"
	frameborder="0"
></iframe>
</div>

### ä¸‹è½½

è¦ä½¿ç”¨ Hub ä¸­çš„ä¸»é¢˜ï¼Œè¯·åœ¨ `ThemeClass` ä¸Šä½¿ç”¨ `from_hub` æ–¹æ³•ï¼Œç„¶åŽå°†å…¶ä¼ é€’ç»™æ‚¨çš„åº”ç”¨ç¨‹åºï¼š

```python
my_theme = gr.Theme.from_hub("gradio/seafoam")

with gr.Blocks(theme=my_theme) as demo:
    ....
```

æ‚¨ä¹Ÿå¯ä»¥ç›´æŽ¥å°†ä¸»é¢˜å­—ç¬¦ä¸²ä¼ é€’ç»™ `Blocks` æˆ– `Interface`ï¼ˆ`gr.Blocks(theme="gradio/seafoam")`ï¼‰

æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨è¯­ä¹‰ç‰ˆæœ¬è¡¨è¾¾å¼å°†æ‚¨çš„åº”ç”¨ç¨‹åºå›ºå®šåˆ°ä¸Šæ¸¸ä¸»é¢˜ç‰ˆæœ¬ã€‚

ä¾‹å¦‚ï¼Œä»¥ä¸‹å†…å®¹å°†ç¡®ä¿æˆ‘ä»¬ä»Žâ€œseafoamâ€ä»“åº“ä¸­åŠ è½½çš„ä¸»é¢˜ä½äºŽ `0.0.1` å’Œ `0.1.0` ç‰ˆæœ¬ä¹‹é—´ï¼š

```python
with gr.Blocks(theme="gradio/seafoam@>=0.0.1,<0.1.0") as demo:
    ....
```

äº«å—åˆ›å»ºè‡ªå·±çš„ä¸»é¢˜å§ï¼å¦‚æžœæ‚¨åˆ¶ä½œäº†ä¸€ä¸ªè‡ªè±ªçš„ä¸»é¢˜ï¼Œè¯·å°†å…¶ä¸Šä¼ åˆ° Hub ä¸Žä¸–ç•Œåˆ†äº«ï¼
å¦‚æžœåœ¨[Twitter](https://twitter.com/gradio)ä¸Šæ ‡è®°æˆ‘ä»¬ï¼Œæˆ‘ä»¬å¯ä»¥ç»™æ‚¨çš„ä¸»é¢˜ä¸€ä¸ªå®£ä¼ ï¼

<style>
.wrapper {
    position: relative;
    padding-bottom: 56.25%;
    padding-top: 25px;
    height: 0;
}
.wrapper iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}
</style>

---

<!-- Source: guides/cn/07_other-tutorials/using-flagging.md -->
# ä½¿ç”¨æ ‡è®°

ç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic
æ ‡ç­¾ï¼šæ ‡è®°ï¼Œæ•°æ®

## ç®€ä»‹

å½“æ‚¨æ¼”ç¤ºä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡åž‹æ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æ”¶é›†è¯•ç”¨æ¨¡åž‹çš„ç”¨æˆ·çš„æ•°æ®ï¼Œç‰¹åˆ«æ˜¯æ¨¡åž‹è¡Œä¸ºä¸å¦‚é¢„æœŸçš„æ•°æ®ç‚¹ã€‚æ•èŽ·è¿™äº›â€œå›°éš¾â€æ•°æ®ç‚¹æ˜¯æœ‰ä»·å€¼çš„ï¼Œå› ä¸ºå®ƒå…è®¸æ‚¨æ”¹è¿›æœºå™¨å­¦ä¹ æ¨¡åž‹å¹¶ä½¿å…¶æ›´å¯é å’Œç¨³å¥ã€‚

Gradio é€šè¿‡åœ¨æ¯ä¸ªâ€œç•Œé¢â€ä¸­åŒ…å«ä¸€ä¸ª**æ ‡è®°**æŒ‰é’®æ¥ç®€åŒ–è¿™äº›æ•°æ®çš„æ”¶é›†ã€‚è¿™ä½¿å¾—ç”¨æˆ·æˆ–æµ‹è¯•äººå‘˜å¯ä»¥è½»æ¾åœ°å°†æ•°æ®å‘é€å›žè¿è¡Œæ¼”ç¤ºçš„æœºå™¨ã€‚æ ·æœ¬ä¼šä¿å­˜åœ¨ä¸€ä¸ª CSV æ—¥å¿—æ–‡ä»¶ä¸­ï¼ˆé»˜è®¤æƒ…å†µä¸‹ï¼‰ã€‚å¦‚æžœæ¼”ç¤ºæ¶‰åŠå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘æˆ–å…¶ä»–ç±»åž‹çš„æ–‡ä»¶ï¼Œåˆ™è¿™äº›æ–‡ä»¶ä¼šå•ç‹¬ä¿å­˜åœ¨ä¸€ä¸ªå¹¶è¡Œç›®å½•ä¸­ï¼Œå¹¶ä¸”è¿™äº›æ–‡ä»¶çš„è·¯å¾„ä¼šä¿å­˜åœ¨ CSV æ–‡ä»¶ä¸­ã€‚

## åœ¨ `gradio.Interface` ä¸­ä½¿ç”¨**æ ‡è®°**æŒ‰é’®

ä½¿ç”¨ Gradio çš„ `Interface` è¿›è¡Œæ ‡è®°ç‰¹åˆ«ç®€å•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨è¾“å‡ºç»„ä»¶ä¸‹æ–¹æœ‰ä¸€ä¸ªæ ‡è®°ä¸º**æ ‡è®°**çš„æŒ‰é’®ã€‚å½“ç”¨æˆ·æµ‹è¯•æ‚¨çš„æ¨¡åž‹æ—¶ï¼Œå¦‚æžœçœ‹åˆ°æœ‰è¶£çš„è¾“å‡ºï¼Œä»–ä»¬å¯ä»¥ç‚¹å‡»æ ‡è®°æŒ‰é’®å°†è¾“å…¥å’Œè¾“å‡ºæ•°æ®å‘é€å›žè¿è¡Œæ¼”ç¤ºçš„æœºå™¨ã€‚æ ·æœ¬ä¼šä¿å­˜åœ¨ä¸€ä¸ª CSV æ—¥å¿—æ–‡ä»¶ä¸­ï¼ˆé»˜è®¤æƒ…å†µä¸‹ï¼‰ã€‚å¦‚æžœæ¼”ç¤ºæ¶‰åŠå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘æˆ–å…¶ä»–ç±»åž‹çš„æ–‡ä»¶ï¼Œåˆ™è¿™äº›æ–‡ä»¶ä¼šå•ç‹¬ä¿å­˜åœ¨ä¸€ä¸ªå¹¶è¡Œç›®å½•ä¸­ï¼Œå¹¶ä¸”è¿™äº›æ–‡ä»¶çš„è·¯å¾„ä¼šä¿å­˜åœ¨ CSV æ–‡ä»¶ä¸­ã€‚

åœ¨ `gradio.Interface` ä¸­æœ‰[å››ä¸ªå‚æ•°](https://gradio.app/docs/interface#initialization)æŽ§åˆ¶æ ‡è®°çš„å·¥ä½œæ–¹å¼ã€‚æˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»å®ƒä»¬ã€‚

- `allow_flagging`ï¼šæ­¤å‚æ•°å¯ä»¥è®¾ç½®ä¸º `"manual"`ï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œ`"auto"` æˆ– `"never"`ã€‚
  - `manual`ï¼šç”¨æˆ·å°†çœ‹åˆ°ä¸€ä¸ªæ ‡è®°æŒ‰é’®ï¼Œåªæœ‰åœ¨ç‚¹å‡»æŒ‰é’®æ—¶æ ·æœ¬æ‰ä¼šè¢«æ ‡è®°ã€‚
  - `auto`ï¼šç”¨æˆ·å°†ä¸ä¼šçœ‹åˆ°ä¸€ä¸ªæ ‡è®°æŒ‰é’®ï¼Œä½†æ¯ä¸ªæ ·æœ¬éƒ½ä¼šè‡ªåŠ¨è¢«æ ‡è®°ã€‚
  - `never`ï¼šç”¨æˆ·å°†ä¸ä¼šçœ‹åˆ°ä¸€ä¸ªæ ‡è®°æŒ‰é’®ï¼Œå¹¶ä¸”ä¸ä¼šæ ‡è®°ä»»ä½•æ ·æœ¬ã€‚
- `flagging_options`ï¼šæ­¤å‚æ•°å¯ä»¥æ˜¯ `None`ï¼ˆé»˜è®¤å€¼ï¼‰æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
  - å¦‚æžœæ˜¯ `None`ï¼Œåˆ™ç”¨æˆ·åªéœ€ç‚¹å‡»**æ ‡è®°**æŒ‰é’®ï¼Œä¸ä¼šæ˜¾ç¤ºå…¶ä»–é€‰é¡¹ã€‚
  - å¦‚æžœæä¾›äº†ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œåˆ™ç”¨æˆ·ä¼šçœ‹åˆ°å¤šä¸ªæŒ‰é’®ï¼Œå¯¹åº”äºŽæä¾›çš„æ¯ä¸ªå­—ç¬¦ä¸²ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ­¤å‚æ•°çš„å€¼ä¸º`[" é”™è¯¯ ", " æ¨¡ç³Š "]`ï¼Œåˆ™ä¼šæ˜¾ç¤ºæ ‡è®°ä¸º**æ ‡è®°ä¸ºé”™è¯¯**å’Œ**æ ‡è®°ä¸ºæ¨¡ç³Š**çš„æŒ‰é’®ã€‚è¿™ä»…é€‚ç”¨äºŽ `allow_flagging` ä¸º `"manual"` çš„æƒ…å†µã€‚
  - æ‰€é€‰é€‰é¡¹å°†ä¸Žè¾“å…¥å’Œè¾“å‡ºä¸€èµ·è®°å½•ã€‚
- `flagging_dir`ï¼šæ­¤å‚æ•°æŽ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
  - å®ƒè¡¨ç¤ºæ ‡è®°æ•°æ®å­˜å‚¨çš„ç›®å½•åç§°ã€‚
- `flagging_callback`ï¼šæ­¤å‚æ•°æŽ¥å— `FlaggingCallback` ç±»çš„å­ç±»çš„å®žä¾‹
  - ä½¿ç”¨æ­¤å‚æ•°å…è®¸æ‚¨ç¼–å†™åœ¨ç‚¹å‡»æ ‡è®°æŒ‰é’®æ—¶è¿è¡Œçš„è‡ªå®šä¹‰ä»£ç 
  - é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒè®¾ç½®ä¸º `gr.CSVLogger` çš„ä¸€ä¸ªå®žä¾‹
  - ä¸€ä¸ªç¤ºä¾‹æ˜¯å°†å…¶è®¾ç½®ä¸º `gr.HuggingFaceDatasetSaver` çš„ä¸€ä¸ªå®žä¾‹ï¼Œè¿™æ ·æ‚¨å¯ä»¥å°†ä»»ä½•æ ‡è®°çš„æ•°æ®å¯¼å…¥åˆ° HuggingFace æ•°æ®é›†ä¸­ï¼ˆå‚è§ä¸‹æ–‡ï¼‰ã€‚

## æ ‡è®°çš„æ•°æ®ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

åœ¨ `flagging_dir` å‚æ•°æä¾›çš„ç›®å½•ä¸­ï¼Œå°†è®°å½•æ ‡è®°çš„æ•°æ®çš„ CSV æ–‡ä»¶ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼šä¸‹é¢çš„ä»£ç åˆ›å»ºäº†åµŒå…¥å…¶ä¸­çš„è®¡ç®—å™¨ç•Œé¢ï¼š

```python
import gradio as gr


def calculator(num1, operation, num2):
    if operation == "add":
        return num1 + num2
    elif operation == "subtract":
        return num1 - num2
    elif operation == "multiply":
        return num1 * num2
    elif operation == "divide":
        return num1 / num2


iface = gr.Interface(
    calculator,
    ["number", gr.Radio(["add", "subtract", "multiply", "divide"]), "number"],
    "number",
    allow_flagging="manual"
)

iface.launch()
```

<gradio-app space="gradio/calculator-flag-basic/"></gradio-app>

å½“æ‚¨ç‚¹å‡»ä¸Šé¢çš„æ ‡è®°æŒ‰é’®æ—¶ï¼Œå¯åŠ¨ç•Œé¢çš„ç›®å½•å°†åŒ…æ‹¬ä¸€ä¸ªæ–°çš„æ ‡è®°å­æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ª CSV æ–‡ä»¶ã€‚è¯¥ CSV æ–‡ä»¶åŒ…æ‹¬æ‰€æœ‰è¢«æ ‡è®°çš„æ•°æ®ã€‚

```directory
+-- flagged/
|   +-- logs.csv
```

_flagged/logs.csv_

```csv
num1,operation,num2,Output,timestamp
5,add,7,12,2022-01-31 11:40:51.093412
6,subtract,1.5,4.5,2022-01-31 03:25:32.023542
```

å¦‚æžœç•Œé¢æ¶‰åŠæ–‡ä»¶æ•°æ®ï¼Œä¾‹å¦‚å›¾åƒå’ŒéŸ³é¢‘ç»„ä»¶ï¼Œè¿˜å°†åˆ›å»ºæ–‡ä»¶å¤¹æ¥å­˜å‚¨è¿™äº›æ ‡è®°çš„æ•°æ®ã€‚ä¾‹å¦‚ï¼Œå°† `image` è¾“å…¥åˆ° `image` è¾“å‡ºç•Œé¢å°†åˆ›å»ºä»¥ä¸‹ç»“æž„ã€‚

```directory
+-- flagged/
|   +-- logs.csv
|   +-- image/
|   |   +-- 0.png
|   |   +-- 1.png
|   +-- Output/
|   |   +-- 0.png
|   |   +-- 1.png
```

_flagged/logs.csv_

```csv
im,Output timestamp
im/0.png,Output/0.png,2022-02-04 19:49:58.026963
im/1.png,Output/1.png,2022-02-02 10:40:51.093412
```

å¦‚æžœæ‚¨å¸Œæœ›ç”¨æˆ·ä¸ºæ ‡è®°æä¾›ä¸€ä¸ªåŽŸå› ï¼Œæ‚¨å¯ä»¥å°†å­—ç¬¦ä¸²åˆ—è¡¨ä¼ é€’ç»™ Interface çš„ `flagging_options` å‚æ•°ã€‚ç”¨æˆ·åœ¨æ ‡è®°æ—¶å¿…é¡»é€‰æ‹©å…¶ä¸­ä¸€é¡¹ï¼Œé€‰é¡¹å°†ä½œä¸ºé™„åŠ åˆ—ä¿å­˜åœ¨ CSV æ–‡ä»¶ä¸­ã€‚

å¦‚æžœæˆ‘ä»¬å›žåˆ°è®¡ç®—å™¨ç¤ºä¾‹ï¼Œä¸‹é¢çš„ä»£ç å°†åˆ›å»ºåµŒå…¥å…¶ä¸­çš„ç•Œé¢ã€‚

```python
iface = gr.Interface(
    calculator,
    ["number", gr.Radio(["add", "subtract", "multiply", "divide"]), "number"],
    "number",
    allow_flagging="manual",
    flagging_options=["wrong sign", "off by one", "other"]
)

iface.launch()
```

<gradio-app space="gradio/calculator-flagging-options/"></gradio-app>

å½“ç”¨æˆ·ç‚¹å‡»æ ‡è®°æŒ‰é’®æ—¶ï¼ŒCSV æ–‡ä»¶çŽ°åœ¨å°†åŒ…æ‹¬æŒ‡ç¤ºæ‰€é€‰é€‰é¡¹çš„åˆ—ã€‚

_flagged/logs.csv_

```csv
num1,operation,num2,Output,flag,timestamp
5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412
6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512
```

## HuggingFaceDatasetSaver å›žè°ƒ

æœ‰æ—¶ï¼Œå°†æ•°æ®ä¿å­˜åˆ°æœ¬åœ° CSV æ–‡ä»¶æ˜¯ä¸åˆç†çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨ Hugging Face Spaces ä¸Š
ï¼Œå¼€å‘è€…é€šå¸¸æ— æ³•è®¿é—®æ‰˜ç®¡ Gradio æ¼”ç¤ºçš„åº•å±‚ä¸´æ—¶æœºå™¨ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨ Hugging Face Space ä¸­å…³é—­æ ‡è®°çš„åŽŸå› ã€‚ç„¶è€Œï¼Œ
æ‚¨å¯èƒ½å¸Œæœ›å¯¹æ ‡è®°çš„æ•°æ®åšå…¶ä»–å¤„ç†ã€‚
you may want to do something else with the flagged data.

é€šè¿‡ `flagging_callback` å‚æ•°ï¼Œæˆ‘ä»¬ä½¿è¿™å˜å¾—éžå¸¸ç®€å•ã€‚

ä¾‹å¦‚ï¼Œä¸‹é¢æˆ‘ä»¬å°†ä¼šå°†æ ‡è®°çš„æ•°æ®ä»Žæˆ‘ä»¬çš„è®¡ç®—å™¨ç¤ºä¾‹å¯¼å…¥åˆ° Hugging Face æ•°æ®é›†ä¸­ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥æž„å»ºä¸€ä¸ªâ€œä¼—åŒ…â€æ•°æ®é›†ï¼š

```python
import os

HF_TOKEN = os.getenv('HF_TOKEN')
hf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, "crowdsourced-calculator-demo")

iface = gr.Interface(
    calculator,
    ["number", gr.Radio(["add", "subtract", "multiply", "divide"]), "number"],
    "number",
    description="Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)",
    allow_flagging="manual",
    flagging_options=["wrong sign", "off by one", "other"],
    flagging_callback=hf_writer
)

iface.launch()
```

æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„ Hugging Face ä»¤ç‰Œå’Œ
è¦ä¿å­˜æ ·æœ¬çš„æ•°æ®é›†çš„åç§°ï¼Œå®šä¹‰äº†æˆ‘ä»¬è‡ªå·±çš„
`gradio.HuggingFaceDatasetSaver` çš„å®žä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°† `allow_flagging="manual"` è®¾ç½®ä¸ºäº†
ï¼Œå› ä¸ºåœ¨ Hugging Face Spaces ä¸­ï¼Œ`allow_flagging` é»˜è®¤è®¾ç½®ä¸º `"never"`ã€‚è¿™æ˜¯æˆ‘ä»¬çš„æ¼”ç¤ºï¼š

<gradio-app space="gradio/calculator-flagging-crowdsourced/"></gradio-app>

æ‚¨çŽ°åœ¨å¯ä»¥åœ¨è¿™ä¸ª[å…¬å…±çš„ Hugging Face æ•°æ®é›†](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)ä¸­çœ‹åˆ°ä¸Šé¢æ ‡è®°çš„æ‰€æœ‰ç¤ºä¾‹ã€‚

![flagging callback hf](/assets/guides/flagging-callback-hf.png)

æˆ‘ä»¬åˆ›å»ºäº† `gradio.HuggingFaceDatasetSaver` ç±»ï¼Œä½†åªè¦å®ƒç»§æ‰¿è‡ª[æ­¤æ–‡ä»¶](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py)ä¸­å®šä¹‰çš„ `FlaggingCallback`ï¼Œæ‚¨å¯ä»¥ä¼ é€’è‡ªå·±çš„è‡ªå®šä¹‰ç±»ã€‚å¦‚æžœæ‚¨åˆ›å»ºäº†ä¸€ä¸ªå¾ˆæ£’çš„å›žè°ƒï¼Œè¯·å°†å…¶è´¡çŒ®ç»™è¯¥å­˜å‚¨åº“ï¼

## ä½¿ç”¨ Blocks è¿›è¡Œæ ‡è®°

å¦‚æžœæ‚¨æ­£åœ¨ä½¿ç”¨ `gradio.Blocks`ï¼Œåˆè¯¥æ€Žä¹ˆåŠžå‘¢ï¼Ÿä¸€æ–¹é¢ï¼Œä½¿ç”¨ Blocks æ‚¨æ‹¥æœ‰æ›´å¤šçš„çµæ´»æ€§
--æ‚¨å¯ä»¥ç¼–å†™ä»»ä½•æ‚¨æƒ³åœ¨æŒ‰é’®è¢«ç‚¹å‡»æ—¶è¿è¡Œçš„ Python ä»£ç ï¼Œ
å¹¶ä½¿ç”¨ Blocks ä¸­çš„å†…ç½®äº‹ä»¶åˆ†é…å®ƒã€‚

åŒæ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›ä½¿ç”¨çŽ°æœ‰çš„ `FlaggingCallback` æ¥é¿å…ç¼–å†™é¢å¤–çš„ä»£ç ã€‚
è¿™éœ€è¦ä¸¤ä¸ªæ­¥éª¤ï¼š

1. æ‚¨å¿…é¡»åœ¨ä»£ç ä¸­çš„æŸä¸ªä½ç½®è¿è¡Œæ‚¨çš„å›žè°ƒçš„ `.setup()` æ–¹æ³•
   åœ¨ç¬¬ä¸€æ¬¡æ ‡è®°æ•°æ®ä¹‹å‰
2. å½“ç‚¹å‡»æ ‡è®°æŒ‰é’®æ—¶ï¼Œæ‚¨è§¦å‘å›žè°ƒçš„ `.flag()` æ–¹æ³•ï¼Œ
   ç¡®ä¿æ­£ç¡®æ”¶é›†å‚æ•°å¹¶ç¦ç”¨é€šå¸¸çš„é¢„å¤„ç†ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨é»˜è®¤çš„ `CSVLogger` æ ‡è®°å›¾åƒæ€€æ—§æ»¤é•œ Blocks æ¼”ç¤ºçš„ç¤ºä¾‹ï¼š
data using the default `CSVLogger`:

$code_blocks_flag
$demo_blocks_flag

## éšç§

é‡è¦æç¤ºï¼šè¯·ç¡®ä¿ç”¨æˆ·äº†è§£ä»–ä»¬æäº¤çš„æ•°æ®ä½•æ—¶è¢«ä¿å­˜ä»¥åŠæ‚¨è®¡åˆ’å¦‚ä½•å¤„ç†å®ƒã€‚å½“æ‚¨ä½¿ç”¨ `allow_flagging=auto`ï¼ˆå½“é€šè¿‡æ¼”ç¤ºæäº¤çš„æ‰€æœ‰æ•°æ®éƒ½è¢«æ ‡è®°æ—¶ï¼‰ï¼Œè¿™ä¸€ç‚¹å°¤ä¸ºé‡è¦

### è¿™å°±æ˜¯å…¨éƒ¨ï¼ç¥æ‚¨å»ºè®¾æ„‰å¿« :)

---

<!-- Source: guides/cn/CONTRIBUTING.md -->
# Contributing a Guide

Want to help teach Gradio? Consider contributing a Guide! ðŸ¤—

Broadly speaking, there are two types of guides:

- **Use cases**: guides that cover step-by-step how to build a particular type of machine learning demo or app using Gradio. Here's an example: [_Creating a Chatbot_](https://github.com/gradio-app/gradio/blob/master/guides/creating_a_chatbot.md)
- **Feature explanation**: guides that describe in detail a particular feature of Gradio. Here's an example: [_Using Flagging_](https://github.com/gradio-app/gradio/blob/master/guides/using_flagging.md)

We encourage you to submit either type of Guide! (Looking for ideas? We may also have open [issues](https://github.com/gradio-app/gradio/issues?q=is%3Aopen+is%3Aissue+label%3Aguides) where users have asked for guides on particular topics)

## Guide Structure

As you can see with the previous examples, Guides are standard markdown documents. They usually:

- start with an Introduction section describing the topic
- include subheadings to make articles easy to navigate
- include real code snippets that make it easy to follow along and implement the Guide
- include embedded Gradio demos to make them more interactive and provide immediate demonstrations of the topic being discussed. These Gradio demos are hosted on [Hugging Face Spaces](https://huggingface.co/spaces) and are embedded using the standard \<iframe\> tag.

## How to Contribute a Guide

1. Clone or fork this `gradio` repo
2. Add a new markdown document with a descriptive title to the `/guides` folder
3. Write your Guide in standard markdown! Embed Gradio demos wherever helpful
4. Add a list of `related_spaces` at the top of the markdown document (see the previously linked Guides for how to do this)
5. Add 3 `tags` at the top of the markdown document to help users find your guide (again, see the previously linked Guides for how to do this)
6. Open a PR to have your guide reviewed

That's it! We're looking forward to reading your Guide ðŸ¥³

---

<!-- Source: guides/CONTRIBUTING.md -->
# Contributing a Guide

Want to help teach Gradio? Consider contributing a Guide! ðŸ¤—

Broadly speaking, there are two types of guides:

- **Use cases**: guides that cover step-by-step how to build a particular type of machine learning demo or app using Gradio. Here's an example: [_Creating a Chatbot_](https://github.com/gradio-app/gradio/blob/master/guides/creating_a_chatbot.md)
- **Feature explanation**: guides that describe in detail a particular feature of Gradio. Here's an example: [_Using Flagging_](https://github.com/gradio-app/gradio/blob/master/guides/using_flagging.md)

We encourage you to submit either type of Guide! (Looking for ideas? We may also have open [issues](https://github.com/gradio-app/gradio/issues?q=is%3Aopen+is%3Aissue+label%3Aguides) where users have asked for guides on particular topics)

## Guide Structure

As you can see with the previous examples, Guides are standard markdown documents. They usually:

- start with an Introduction section describing the topic
- include subheadings to make articles easy to navigate
- include real code snippets that make it easy to follow along and implement the Guide
- include embedded Gradio demos to make them more interactive and provide immediate demonstrations of the topic being discussed. These Gradio demos are hosted on [Hugging Face Spaces](https://huggingface.co/spaces) and are embedded using the standard \<iframe\> tag.

## How to Contribute a Guide

1. Clone or fork this `gradio` repo
2. Add a new markdown document with a descriptive title to the `/guides` folder
3. Write your Guide in standard markdown! Embed Gradio demos wherever helpful
4. Add a list of `related_spaces` at the top of the markdown document (see the previously linked Guides for how to do this)
5. Add 3 `tags` at the top of the markdown document to help users find your guide (again, see the previously linked Guides for how to do this)
6. Open a PR to have your guide reviewed

That's it! We're looking forward to reading your Guide ðŸ¥³
